{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8048,
     "status": "ok",
     "timestamp": 1595880019461,
     "user": {
      "displayName": "yaniel barbosa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjeEwoPVbOEW9Pwf_Z5Q-LyxKX0GvQrJ-S-pfKh=s64",
      "userId": "13057870575473273651"
     },
     "user_tz": -60
    },
    "id": "eR2oejU7JhSE",
    "outputId": "b803b1cb-a567-4b08-84de-09862d595617"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 12:16:17.361400: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-30 12:16:17.391459: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-30 12:16:17.867773: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 12:16:18.512107: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-30 12:16:18.541466: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-30 12:16:18.544244: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re #regex\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Dense, LayerNormalization, Dropout,  LSTM, Dense, Embedding, Bidirectional,BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import io\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "import random\n",
    "\n",
    "from rouge import Rouge #https://towardsdatascience.com/introduction-to-text-summarization-with-rouge-scores-84140c64b471\n",
    "\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras_tuner as kt\n",
    "\n",
    "\n",
    "print(\"Num GPUs Available: \", (tf.config.experimental.list_physical_devices()))\n",
    "\n",
    "ROUGE = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/n/anaconda3/envs/AI_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#!pip install keras-nlp\n",
    "import keras_nlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8007,
     "status": "ok",
     "timestamp": 1595880019463,
     "user": {
      "displayName": "yaniel barbosa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjeEwoPVbOEW9Pwf_Z5Q-LyxKX0GvQrJ-S-pfKh=s64",
      "userId": "13057870575473273651"
     },
     "user_tz": -60
    },
    "id": "CBl11TrADX6-"
   },
   "outputs": [],
   "source": [
    "buffer_size = 10000\n",
    "batch_size= 16\n",
    "window_size= 40 \n",
    "epochs= 150\n",
    "embedding_dim = 200\n",
    "\n",
    "TRAIN_AND_SAVE = True\n",
    "TRAIN_AND_SAVE_TRANFORMER = True\n",
    "\n",
    "TRAIN_TEXT= True\n",
    "SEED=42\n",
    "TRAIN_TOKEN = True\n",
    "TUNNING = True\n",
    "GENERATE_TEXT= True\n",
    "\n",
    "\n",
    "translationTable = str.maketrans(\"áéíóúàèìòùâêîôûãõç\", \"aeiouaeiouaeiouaoc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow.data as tf_data\n",
    "# import tensorflow.strings as tf_strings\n",
    "\n",
    "# keras.utils.get_file(\n",
    "#     origin=\"https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip\",\n",
    "#     extract=True,\n",
    "# )\n",
    "# dir = os.path.expanduser(\"~/.keras/datasets/simplebooks/\")\n",
    "\n",
    "# # Load simplebooks-92 train set and filter out short lines.\n",
    "# raw_train_ds = (\n",
    "#     tf_data.TextLineDataset(dir + \"simplebooks-92-raw/train.txt\")\n",
    "#     .filter(lambda x: tf_strings.length(x) > 512)\n",
    "#     .batch(batch_size)\n",
    "#     .shuffle(buffer_size=256)\n",
    "# )\n",
    "# for x in raw_train_ds.take(1):\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_CHECKPOINTS_DIR = './checkpoints'\n",
    "tfrecord_filename = \"train_tmp.tfrecord\"\n",
    "train_tmp_record_path = f'{DATA_CHECKPOINTS_DIR}/{tfrecord_filename}'\n",
    "\n",
    "!mkdir -p {DATA_CHECKPOINTS_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 1266856\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a casa que vieram habitar em lisboa , no outon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apesar deste fresco nome de vivenda campestre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>longos anos o ramalhete permanecera desabitad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>em monsenhor bucarini , nuncio de sao santida...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>este inutil pardieiro como lhe chamava vilaca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10205</th>\n",
       "      <td>ega , ao lado , ajuntava , ofegante , atirand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10206</th>\n",
       "      <td>a lanterna vermelha do americano , ao longe ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10207</th>\n",
       "      <td>e foi em carlos e em joao da ega uma esperanc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10208</th>\n",
       "      <td>entao , para apanhar o americano , os dois am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10209</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10210 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentences\n",
       "0      a casa que vieram habitar em lisboa , no outon...\n",
       "1       apesar deste fresco nome de vivenda campestre...\n",
       "2       longos anos o ramalhete permanecera desabitad...\n",
       "3       em monsenhor bucarini , nuncio de sao santida...\n",
       "4       este inutil pardieiro como lhe chamava vilaca...\n",
       "...                                                  ...\n",
       "10205   ega , ao lado , ajuntava , ofegante , atirand...\n",
       "10206   a lanterna vermelha do americano , ao longe ,...\n",
       "10207   e foi em carlos e em joao da ega uma esperanc...\n",
       "10208   entao , para apanhar o americano , os dois am...\n",
       "10209                                                   \n",
       "\n",
       "[10210 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_data(path_to_file ='./nietzsche.txt'):\n",
    "    path_to_file = \"./eca_de_queiros_os_maias.txt\"\n",
    "    \n",
    "    with io.open(path_to_file, encoding='utf-8') as f:\n",
    "        text = f.read().lower()\n",
    "\n",
    "    print('corpus length:', len(text))\n",
    "    text = text.translate(translationTable)\n",
    "\n",
    "    string_check= re.compile('[^a-zA-Z.?!,:\\'<>]')\n",
    "\n",
    "    text = re.sub(string_check, ' ', (text)\n",
    "                        .replace(\"-\",\" \")\n",
    "                        .replace(\"´\",\"'\")\n",
    "                        .replace(\"`\",\"'\")\n",
    "                        .replace(\",\",\" , \")\n",
    "                        .replace(\"s.\",\" sao \")\n",
    "                        .replace(\"d.\",\"don\")\n",
    "                        .replace(\"v.\",\"v\")\n",
    "                        .replace(\"sr.\", \"senhor\")\n",
    "                        .replace(\"sra.\", \"senhora\")\n",
    "                        .replace(\"exmo.\", \"exmo\")\n",
    "                        .replace(\"exma.\", \"exma\")\n",
    "                        .replace(\"x.\", \"x\")\n",
    "    )\n",
    "\n",
    "    text = re.sub(' +', ' ',text)\n",
    "    text = (re.sub('\\.+', \" . \\n\", text).replace(\"<br />\",\" \"))\n",
    "\n",
    "    lines_list = list()\n",
    "    lines_list = text.split(\"\\n\")\n",
    "\n",
    "    cols = ['sentences']\n",
    "    df_tmp= pd.DataFrame(columns=cols)\n",
    "    df_tmp[\"sentences\"] = lines_list\n",
    "\n",
    "    return df_tmp\n",
    "complete_text= prepare_data()\n",
    "complete_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 1266856\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a casa que vieram habitar em lisboa , no outon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apesar deste fresco nome de vivenda campestre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>longos anos o ramalhete permanecera desabitad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>em monsenhor bucarini , nuncio de sao santida...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>este inutil pardieiro como lhe chamava vilaca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10205</th>\n",
       "      <td>ega , ao lado , ajuntava , ofegante , atirand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10206</th>\n",
       "      <td>a lanterna vermelha do americano , ao longe ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10207</th>\n",
       "      <td>e foi em carlos e em joao da ega uma esperanc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10208</th>\n",
       "      <td>entao , para apanhar o americano , os dois am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10209</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10210 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentences\n",
       "0      a casa que vieram habitar em lisboa , no outon...\n",
       "1       apesar deste fresco nome de vivenda campestre...\n",
       "2       longos anos o ramalhete permanecera desabitad...\n",
       "3       em monsenhor bucarini , nuncio de sao santida...\n",
       "4       este inutil pardieiro como lhe chamava vilaca...\n",
       "...                                                  ...\n",
       "10205   ega , ao lado , ajuntava , ofegante , atirand...\n",
       "10206   a lanterna vermelha do americano , ao longe ,...\n",
       "10207   e foi em carlos e em joao da ega uma esperanc...\n",
       "10208   entao , para apanhar o americano , os dois am...\n",
       "10209                                                   \n",
       "\n",
       "[10210 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "22255"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'serra',\n",
       " 'regeladas',\n",
       " 'tocar',\n",
       " 'faces',\n",
       " 'frementes',\n",
       " 'bilis',\n",
       " 'capela',\n",
       " 'monforte!',\n",
       " 'caricia:',\n",
       " 'desordem!',\n",
       " 'arruinados',\n",
       " 'hotel',\n",
       " 'transluziam',\n",
       " 'calva!',\n",
       " 'indulgencia',\n",
       " 'romantica!',\n",
       " 'voluptuosamente',\n",
       " 'queen',\n",
       " 'chaile',\n",
       " 'voltou',\n",
       " 'tarimbeiro',\n",
       " 'boulogne',\n",
       " 'subida',\n",
       " 'realizava',\n",
       " 'sofrendo',\n",
       " 'piaram',\n",
       " 'desdiga!',\n",
       " 'serenata',\n",
       " 'trespassados',\n",
       " 'parecer',\n",
       " 'descrevia',\n",
       " 'passeios',\n",
       " 'cavalgadura',\n",
       " 'praguejava',\n",
       " 'meticulosa',\n",
       " 'velho!',\n",
       " 'terraco',\n",
       " 'seguida',\n",
       " 'permanecia',\n",
       " 'divino',\n",
       " 'prazeres',\n",
       " 'caldo',\n",
       " 'facto',\n",
       " 'traste!',\n",
       " 'mediocridade',\n",
       " 'decrepito',\n",
       " 'viajara',\n",
       " 'alfandega:',\n",
       " 'remar!',\n",
       " 'sossego!',\n",
       " 'impura',\n",
       " 'ensopava',\n",
       " 'extravagancias',\n",
       " 'absorvido',\n",
       " 'recusara',\n",
       " 'entristecia',\n",
       " 'consulta!',\n",
       " 'israel',\n",
       " 'limite',\n",
       " 'cobrindo',\n",
       " 'incrivel!',\n",
       " 'desaparelhar',\n",
       " 'assistiu',\n",
       " 'obrigava',\n",
       " 'aflicao',\n",
       " 'negar:',\n",
       " 'cerimonia?',\n",
       " 'engolidos',\n",
       " 'mimosa',\n",
       " 'respeito',\n",
       " 'coletes',\n",
       " 'pago',\n",
       " 'bem:',\n",
       " 'aterrador',\n",
       " 'camaradagem',\n",
       " 'torneada',\n",
       " 'groseile',\n",
       " 'pratico',\n",
       " 'bernarda',\n",
       " 'terror',\n",
       " 'telegramas:',\n",
       " 'musculatura',\n",
       " 'cre',\n",
       " 'balseira',\n",
       " 'gremio!',\n",
       " 'flores',\n",
       " 'estavamos',\n",
       " 'comover',\n",
       " 'esvoacando',\n",
       " 'grande!',\n",
       " 'bebado',\n",
       " 'lancarem',\n",
       " 'borroes',\n",
       " 'retrocedera',\n",
       " 'juntas',\n",
       " 'mandou',\n",
       " 'estrofe:',\n",
       " 'vimo',\n",
       " 'coberta',\n",
       " 'ensinava',\n",
       " 'esbocou',\n",
       " 'dezassete',\n",
       " 'curiosos',\n",
       " 'jantar?',\n",
       " 'palreira',\n",
       " 'satisfez',\n",
       " 'entoam',\n",
       " 'trabalhadas',\n",
       " 'baralho',\n",
       " 'velasquez',\n",
       " 'refulgiu',\n",
       " 'somem',\n",
       " 'acordaria',\n",
       " 'grotesco',\n",
       " 'merendare',\n",
       " 'catolica',\n",
       " 'acontecido',\n",
       " 'meter',\n",
       " 'partilhar',\n",
       " 'adora',\n",
       " 'crepusculo',\n",
       " 'supunha',\n",
       " 'avestruz:',\n",
       " 'atiradas',\n",
       " 'albe',\n",
       " 'acotovela',\n",
       " 'cacete!',\n",
       " 'critico',\n",
       " 'chega',\n",
       " 'batia:',\n",
       " 'regar',\n",
       " 'habita',\n",
       " 'cortesia',\n",
       " 'lindissimo',\n",
       " 'flores?',\n",
       " 'ensaiando',\n",
       " 'risca',\n",
       " 'd:',\n",
       " 'urgentemente',\n",
       " 'nocoes',\n",
       " 'horrivelmente',\n",
       " 'contas:',\n",
       " 'filialmente',\n",
       " 'anonima!',\n",
       " 'satisfeitas',\n",
       " 'murchavam',\n",
       " 'deixe',\n",
       " 'apostolos',\n",
       " 'alentejo',\n",
       " 'cristo?',\n",
       " 'temporal',\n",
       " 'apoderava',\n",
       " 'iluminada',\n",
       " 'arranque',\n",
       " 'indignaram',\n",
       " 'raquesinha!',\n",
       " 'sucedia',\n",
       " 'dominio:',\n",
       " 'indignavaa!',\n",
       " 'interessante',\n",
       " 'dancam',\n",
       " 'tenro',\n",
       " 'cruzada',\n",
       " 'viveu',\n",
       " 'inacessivel',\n",
       " 'dinheiro?',\n",
       " 'acetinado',\n",
       " 'abalada!',\n",
       " 'senti',\n",
       " 'fagulha',\n",
       " 'agradeca!',\n",
       " 'cohen:',\n",
       " 'tradicionalmente',\n",
       " 'percebia',\n",
       " 'solidao',\n",
       " 'rebu',\n",
       " 'i',\n",
       " 'trataram',\n",
       " 'tentou',\n",
       " 'eufrates',\n",
       " 'sozinha',\n",
       " 'animava',\n",
       " 'sancho',\n",
       " 'navio',\n",
       " 'arrombo',\n",
       " 'esquecesse',\n",
       " 'embelezar',\n",
       " 'autobiografia',\n",
       " 'casticais',\n",
       " 'reprimir',\n",
       " 'pacata',\n",
       " 'abandona?',\n",
       " 'ciumes',\n",
       " 'murmurar:',\n",
       " 'consumido',\n",
       " 'caracter',\n",
       " 'entroncamento',\n",
       " 'ilha',\n",
       " 'traca',\n",
       " 'encantadora!',\n",
       " 'arrebatado',\n",
       " 'prosperar',\n",
       " 'exilar',\n",
       " 'comoda:',\n",
       " 'bondade:',\n",
       " 'familiares',\n",
       " 'ouviu',\n",
       " 'lembravam',\n",
       " 'aroma!',\n",
       " 'aceitariam',\n",
       " 'acelerou',\n",
       " 'perfidas',\n",
       " 'tipoia!',\n",
       " 'arejar',\n",
       " 'fincados',\n",
       " 'olharzinho',\n",
       " 'lazareto',\n",
       " 'entendo',\n",
       " 'apoderando',\n",
       " 'beijaram',\n",
       " 'desconfiada',\n",
       " 'complicadamente',\n",
       " 'vem',\n",
       " 'amando',\n",
       " 'ai?',\n",
       " 'atenuar',\n",
       " 'ocultado',\n",
       " 'considerara',\n",
       " 'danada!',\n",
       " 'tepida',\n",
       " 'ia',\n",
       " 'invadido',\n",
       " 'carvalhosa',\n",
       " 'trespassava',\n",
       " 'macgren',\n",
       " 'ideais',\n",
       " 'iii',\n",
       " 'descomposto',\n",
       " 'caluniador',\n",
       " 'beaudelaire',\n",
       " 'fiacre',\n",
       " 'mouriscos',\n",
       " 'outono?',\n",
       " 'andei',\n",
       " 'nascente',\n",
       " 'aparelho',\n",
       " 'esbugalhados',\n",
       " 'ardiam',\n",
       " 'rosnara',\n",
       " 'reticencia',\n",
       " 'nocturnos',\n",
       " 'inercia',\n",
       " 'prendas:',\n",
       " 'desdobrado',\n",
       " 'detido',\n",
       " 'descrenca!',\n",
       " 'empenhara',\n",
       " 'imobilizou',\n",
       " 'nedia',\n",
       " 'calica',\n",
       " 'abaixo',\n",
       " 'prido',\n",
       " 'sentiu',\n",
       " 'tumultuosa',\n",
       " 'aparecesse',\n",
       " 'chic',\n",
       " 'cactos',\n",
       " 'desconsolado',\n",
       " 'humilhado!',\n",
       " 'origem',\n",
       " 'repetiu',\n",
       " 'langorosa',\n",
       " 'voltariam',\n",
       " 'varanda',\n",
       " 'emigrados',\n",
       " 'actriz',\n",
       " 'aterrada',\n",
       " 'amigavelmente',\n",
       " 'solido',\n",
       " 'estomago:',\n",
       " 'contesse!',\n",
       " 'amo!',\n",
       " 'prazere',\n",
       " 'ferem',\n",
       " 'concluiu',\n",
       " 'luzinha',\n",
       " 'morticos',\n",
       " 'mach!',\n",
       " 'canoro',\n",
       " 'bochechinha',\n",
       " 'suficiente',\n",
       " 'usando',\n",
       " 'matriculou',\n",
       " 'ceava',\n",
       " 'jogador',\n",
       " 'embaciaram',\n",
       " 'bilhete',\n",
       " 'mendelsshon',\n",
       " 'brilhando',\n",
       " 'teima',\n",
       " 'suspirosinho',\n",
       " 'apertar',\n",
       " 'cobertas',\n",
       " 'patio:',\n",
       " 'desenxabida:',\n",
       " 'resultado:',\n",
       " 'afundados',\n",
       " 'armarios',\n",
       " 'trovoada',\n",
       " 'extaseou',\n",
       " 'pesadamente',\n",
       " 'baetao',\n",
       " 'inferiores',\n",
       " 'batia',\n",
       " 'erguiam',\n",
       " 'demitido',\n",
       " 'finorios',\n",
       " 'sentia',\n",
       " 'respirando',\n",
       " 'alheios!',\n",
       " 'chegue!',\n",
       " 'aproximacao',\n",
       " 'redoma',\n",
       " 'comeco',\n",
       " 'perfumando',\n",
       " 'perguntar:',\n",
       " 'convivencia',\n",
       " 'tortulho',\n",
       " 'silvar',\n",
       " 'parcela',\n",
       " 'extinto',\n",
       " 'insipida',\n",
       " 'saciedade',\n",
       " 'lindas',\n",
       " 'voltaretesinho',\n",
       " 'corridas',\n",
       " 'clemencia',\n",
       " 'coices',\n",
       " 'seguido',\n",
       " 'acacias',\n",
       " 'jantavamos',\n",
       " 'parecem',\n",
       " 'popa',\n",
       " 'positivista',\n",
       " 'impressao!',\n",
       " 'joaninha',\n",
       " 'maozinha',\n",
       " 'caetano',\n",
       " 'concebia',\n",
       " 'ostentando',\n",
       " 'debrucava',\n",
       " 'azeda',\n",
       " 'consenso',\n",
       " 'lua!',\n",
       " 'apito',\n",
       " 'contasse',\n",
       " 'impaciencia',\n",
       " 'nunca',\n",
       " 'lareira',\n",
       " 'esplendor',\n",
       " 'voile',\n",
       " 'andassem',\n",
       " 'enternecera',\n",
       " 'libres',\n",
       " 'batuta',\n",
       " 'viuvo',\n",
       " 'pasmando',\n",
       " 'cativar',\n",
       " 'escondido',\n",
       " 'levasse',\n",
       " 'aparecem',\n",
       " 'corrigiu',\n",
       " 'riscadinho',\n",
       " 'ladrou',\n",
       " 'pratas',\n",
       " 'choramigar',\n",
       " 'edificacao',\n",
       " 'ilhas',\n",
       " 'esmagado',\n",
       " 'escuridao',\n",
       " 'inocente:',\n",
       " 'calhou',\n",
       " 'estupidas',\n",
       " 'realcava',\n",
       " 'majestade!',\n",
       " 'fossem',\n",
       " 'lampejando',\n",
       " 'poetisava',\n",
       " 'exageradas',\n",
       " 'objectara',\n",
       " 'martires',\n",
       " 'suja',\n",
       " 'ustedon',\n",
       " 'saudades',\n",
       " 'esgrouviada',\n",
       " 'craben',\n",
       " 'passaram',\n",
       " 'despegavam',\n",
       " 'uns',\n",
       " 'mecha',\n",
       " 'braganca',\n",
       " 'comungando',\n",
       " 'pustula',\n",
       " 'idiotas',\n",
       " 'enlacar',\n",
       " 'culpa?',\n",
       " 'covas',\n",
       " 'acompanho',\n",
       " 'barracoes',\n",
       " 'recusar',\n",
       " 'triplicado',\n",
       " 'legisladores',\n",
       " 'diletantismo',\n",
       " 'faltou',\n",
       " 'competia',\n",
       " 'repercorreu',\n",
       " 'tardaram',\n",
       " 'beauvais',\n",
       " 'museu?',\n",
       " 'poucos',\n",
       " 'namorou',\n",
       " 'marteloulhe',\n",
       " 'clara?',\n",
       " 'acavalam',\n",
       " 'episodios',\n",
       " 'na',\n",
       " 'afirmando',\n",
       " 'baca',\n",
       " 'spartacus',\n",
       " 'precipitar',\n",
       " 'silvestre',\n",
       " 'monotonia',\n",
       " 'garantir',\n",
       " 'provocando',\n",
       " 'marechal',\n",
       " 'obrigacoes!',\n",
       " 'golo',\n",
       " 'pisam',\n",
       " 'encovado',\n",
       " 'mistura',\n",
       " 'doenca:',\n",
       " 'agradara',\n",
       " 'cachenez',\n",
       " 'adelia',\n",
       " 'enrugada',\n",
       " 'brutamente',\n",
       " 'acalmara',\n",
       " 'preferiam',\n",
       " 'estupidez!',\n",
       " 'patuscada',\n",
       " 'espanto:',\n",
       " 'literata',\n",
       " 'venus',\n",
       " 'gauche',\n",
       " 'angustiado',\n",
       " 'viajantes',\n",
       " 'deputado?',\n",
       " 'efeitos',\n",
       " 'savedra!',\n",
       " 'cigarro:',\n",
       " 'chapeleira',\n",
       " 'conversarmos',\n",
       " 'limao:',\n",
       " 'viscondessa!',\n",
       " 'eira',\n",
       " 'rebocada',\n",
       " 'amiga?',\n",
       " 'terres?',\n",
       " 'deusa',\n",
       " 'pensoes',\n",
       " 'remar',\n",
       " 'avancando',\n",
       " 'nu?',\n",
       " 'sumamente',\n",
       " 'cavalheiro!',\n",
       " 'trincando',\n",
       " 'enleado',\n",
       " 'esfarrapada',\n",
       " 'orgulhar',\n",
       " 'fundara',\n",
       " 'jeito',\n",
       " 'submissa',\n",
       " 'tepidamente',\n",
       " 'colocalos',\n",
       " 'escangalhasse',\n",
       " 'fezemos',\n",
       " 'retratos',\n",
       " 'fruto',\n",
       " 'rufino',\n",
       " 'asilo',\n",
       " 'estacionavam',\n",
       " 'galho',\n",
       " 'relaxacao',\n",
       " 'bucela',\n",
       " 'secular',\n",
       " 'tentativa',\n",
       " 'imaginando',\n",
       " 'celebracao',\n",
       " 'beicinho',\n",
       " 'paiosinho!',\n",
       " 'arranjou',\n",
       " 'triste!',\n",
       " 'solteiroe',\n",
       " 'rapariga',\n",
       " 'falei',\n",
       " 'bebidas!',\n",
       " 'dormiam',\n",
       " 'macar',\n",
       " 'espirito',\n",
       " 'transportar',\n",
       " 'comodidade:',\n",
       " 'escutarem',\n",
       " 'fantasia',\n",
       " 'preguicas',\n",
       " 'veja',\n",
       " 'subiu',\n",
       " 'correntes',\n",
       " 'arrombadas',\n",
       " 'amoroso',\n",
       " 'inferioridade',\n",
       " 'disenteria',\n",
       " 'prodigio!',\n",
       " 'pronto?',\n",
       " 'levam',\n",
       " 'antiga',\n",
       " 'lazzaroni',\n",
       " 'adquire',\n",
       " 'credo',\n",
       " 'vos',\n",
       " 'estreitamente',\n",
       " 'queres',\n",
       " 'principe:',\n",
       " 'tunante',\n",
       " 'emilion',\n",
       " 'reflexoes',\n",
       " 'oleoso',\n",
       " 'pousava',\n",
       " 'bolsinho',\n",
       " 'rebentava',\n",
       " 'pneumonia',\n",
       " 'desabado',\n",
       " 'consternava',\n",
       " 'instincto',\n",
       " 'contentar',\n",
       " 'atabalhoara',\n",
       " 'porque?',\n",
       " 'ressurgido',\n",
       " 'fugas',\n",
       " 'acotovelavam',\n",
       " 'sublinhando:',\n",
       " 'arcadas',\n",
       " 'bibelot',\n",
       " 'destruiram',\n",
       " 'boca',\n",
       " 'salientes',\n",
       " 'estabelecer',\n",
       " 'tristeza',\n",
       " 'sumida',\n",
       " 'empreender',\n",
       " 'quadrados',\n",
       " 'diletante',\n",
       " 'maravilhada',\n",
       " 'rastejante!',\n",
       " 'jura',\n",
       " 'mosteiro',\n",
       " 'consagrado',\n",
       " 'apanhamo',\n",
       " 'silencioso',\n",
       " 'aluvioes',\n",
       " 'levando',\n",
       " 'evitar',\n",
       " 'humedeciam',\n",
       " 'causava',\n",
       " 'respirar',\n",
       " 'emulacao',\n",
       " 'servicos?',\n",
       " 'chaplain?',\n",
       " 'byron',\n",
       " 'batina',\n",
       " 'banhando',\n",
       " 'operario',\n",
       " 'chame',\n",
       " 'asco',\n",
       " 'tirado',\n",
       " 'levailant',\n",
       " 'fulgores',\n",
       " 'encolhendo',\n",
       " 'maestro',\n",
       " 'vi',\n",
       " 'lia?',\n",
       " 'altar',\n",
       " 'atrevido',\n",
       " 'macao',\n",
       " 'ja',\n",
       " 'cranios',\n",
       " 'scribe',\n",
       " 'caldeiras',\n",
       " 'viu',\n",
       " 'estamos',\n",
       " 'trabalhara',\n",
       " 'arquitectural',\n",
       " 'roldao',\n",
       " 'taco',\n",
       " 'fatia',\n",
       " 'devotamente',\n",
       " 'vencidos',\n",
       " 'retrocedeu:',\n",
       " 'jornal!',\n",
       " 'venerar',\n",
       " 'nivel',\n",
       " 'craveiro?',\n",
       " 'mimo',\n",
       " 'terminando',\n",
       " 'vexada',\n",
       " 'cantante',\n",
       " 'invasao',\n",
       " 'penteado',\n",
       " 'onda',\n",
       " 'perdoasse',\n",
       " 'recuperara',\n",
       " 'lagrimas!',\n",
       " 'afeicoes',\n",
       " 'grata',\n",
       " 'encarnicados!',\n",
       " 'quinze',\n",
       " 'fronteira',\n",
       " 'repetindo',\n",
       " 'inevitavel:',\n",
       " 'estava:',\n",
       " 'inofensivo!',\n",
       " 'voando',\n",
       " 'conservador',\n",
       " 'antigos',\n",
       " 'repente!',\n",
       " 'singular',\n",
       " 'querera',\n",
       " 'artagnan?',\n",
       " 'chafurdamos',\n",
       " 'meretriz',\n",
       " 'embrulhos',\n",
       " 'troca!',\n",
       " 'exclamava:',\n",
       " 'enlacada',\n",
       " 'aceitaram',\n",
       " 'monge',\n",
       " 'eduardus',\n",
       " 'cheques',\n",
       " 'divertir',\n",
       " 'nice',\n",
       " 'gorgear',\n",
       " 'nevoa',\n",
       " 'desespero!',\n",
       " 'devassa',\n",
       " 'pressagio',\n",
       " 'mergulhou',\n",
       " 'ameacas!',\n",
       " 'adoro!',\n",
       " 'barrotes',\n",
       " 'episodio',\n",
       " 'aterro!',\n",
       " 'azeite',\n",
       " 'senhas',\n",
       " 'dentro',\n",
       " 'supusera',\n",
       " 'matam',\n",
       " 'tamisa',\n",
       " 'livreco',\n",
       " 'avenida!',\n",
       " 'devorava',\n",
       " 'levantamento',\n",
       " 'calados',\n",
       " 'surgira',\n",
       " 'esvaziara',\n",
       " 'escritorio',\n",
       " 'confortavel',\n",
       " 'reveladora',\n",
       " 'incuravel',\n",
       " 'tinhas',\n",
       " 'modo!',\n",
       " 'agarrava',\n",
       " 'volto',\n",
       " 'certos',\n",
       " 'subalterna',\n",
       " 'derramando',\n",
       " 'olhos',\n",
       " 'lustre',\n",
       " 'jacobino',\n",
       " 'retira',\n",
       " 'finalmente',\n",
       " 'capelo',\n",
       " 'saloias',\n",
       " 'desvairado:',\n",
       " 'acharam',\n",
       " 'destacando',\n",
       " 'semblante',\n",
       " 'alegraram',\n",
       " 'real',\n",
       " 'augusto',\n",
       " 'cercava',\n",
       " 'recomendado',\n",
       " 'olho:',\n",
       " 'comunicavam',\n",
       " 'terminaram',\n",
       " 'richmond',\n",
       " 'contentara',\n",
       " 'alarido:',\n",
       " 'mao?',\n",
       " 'estalavam',\n",
       " 'alagada',\n",
       " 'imprudente!',\n",
       " 'periodo',\n",
       " 'perninhas',\n",
       " 'desforra',\n",
       " 'pedestal',\n",
       " 'lactea',\n",
       " 'travando',\n",
       " 'quintarolas',\n",
       " 'obrigatorio',\n",
       " 'recatadas',\n",
       " 'loura:',\n",
       " 'apesar',\n",
       " 'instantinho',\n",
       " 'guitarradas',\n",
       " 'odes',\n",
       " 'enlameados',\n",
       " 'flauta',\n",
       " 'grisalha',\n",
       " 'surgiu',\n",
       " 'deserta',\n",
       " 'hirtos',\n",
       " 'tactica',\n",
       " 'roidos',\n",
       " 'charmant!',\n",
       " 'suor',\n",
       " 'abrigar',\n",
       " 'fatigada',\n",
       " 'colo!',\n",
       " 'marrasquino',\n",
       " 'fingia',\n",
       " 'teodosio',\n",
       " 'aconselhava',\n",
       " 'planta',\n",
       " 'guita:',\n",
       " 'recrescia',\n",
       " 'casualmente',\n",
       " 'fatais',\n",
       " 'dormira',\n",
       " 'estrangeiro',\n",
       " 'enferrujadote',\n",
       " 'queixas',\n",
       " 'vintista',\n",
       " 'tomado:',\n",
       " 'bastou',\n",
       " 'mastreacoes',\n",
       " 'carcassa',\n",
       " 'abalamos!',\n",
       " 'grade',\n",
       " 'arrojara',\n",
       " 'embrulhar',\n",
       " 'vermelho:',\n",
       " 'armadura',\n",
       " 'prateleiras',\n",
       " 'cachet!',\n",
       " 'etes',\n",
       " 'bofetadas',\n",
       " 'rolou',\n",
       " 'encarnacao',\n",
       " 'rebuscava',\n",
       " 'ilustracoe',\n",
       " 'despedido',\n",
       " 'mauricio',\n",
       " 'exercito!',\n",
       " 'tardou',\n",
       " 'retinia',\n",
       " 'pior!',\n",
       " 'bochecha',\n",
       " 'deixara',\n",
       " 'feito?',\n",
       " 'cidades',\n",
       " 'voltassem',\n",
       " 'fatal',\n",
       " 'opalas',\n",
       " 'lentidao',\n",
       " 'colocar',\n",
       " 'errar',\n",
       " 'milhao',\n",
       " 'ampla',\n",
       " 'descobri',\n",
       " 'veludilho',\n",
       " 'gargalhada!',\n",
       " 'confessou',\n",
       " 'lavrado',\n",
       " 'criado',\n",
       " 'madalena',\n",
       " 'embarcadico:',\n",
       " 'plantadores',\n",
       " 'aplicacoes',\n",
       " 'diferentes',\n",
       " 'excluem',\n",
       " 'bagatela',\n",
       " 'desuniao',\n",
       " 'rustico',\n",
       " 'mascar',\n",
       " 'travadas',\n",
       " 'forca',\n",
       " 'santo',\n",
       " 'pescoco',\n",
       " 'marcelina',\n",
       " 'amanha!',\n",
       " 'rubrica',\n",
       " 'cavacos',\n",
       " 'limitava',\n",
       " 'stuart',\n",
       " 'rangiam',\n",
       " 'modo?',\n",
       " 'flirtar',\n",
       " 'ladrone',\n",
       " 'consumar',\n",
       " 'esganicava:',\n",
       " 'linhagem',\n",
       " 'cobras',\n",
       " 'tapecaria',\n",
       " 'central',\n",
       " 'fera!',\n",
       " 'satisfacoes',\n",
       " 'absorvendo',\n",
       " 'toleravel',\n",
       " 'lado',\n",
       " 'vale',\n",
       " 'latitude',\n",
       " 'sebento',\n",
       " 'exclama',\n",
       " 'minuto',\n",
       " 'gestosinho',\n",
       " 'descoberta',\n",
       " 'carteira',\n",
       " 'resumia',\n",
       " 'suspiros',\n",
       " 'pertence?',\n",
       " 'mae',\n",
       " 'dissimulado',\n",
       " 'vitalismo',\n",
       " 'vibrar',\n",
       " 'grilos',\n",
       " 'footbal',\n",
       " 'tardinha',\n",
       " 'dei',\n",
       " 'prenunciava',\n",
       " 'extinguia',\n",
       " 'conspirara',\n",
       " 'ouvira',\n",
       " 'scott',\n",
       " 'ajuda',\n",
       " 'atravessando',\n",
       " 'palmas',\n",
       " 'laboratorios',\n",
       " 'debrucando',\n",
       " 'fundas',\n",
       " 'solavancos',\n",
       " 'salao',\n",
       " 'sorriso',\n",
       " 'instiga',\n",
       " 'mademoisele',\n",
       " 'admiravelmente',\n",
       " 'gastar',\n",
       " 'forte!',\n",
       " 'vivenda',\n",
       " 'varou',\n",
       " 'mantos',\n",
       " 'tolice',\n",
       " 'desavergonhadas',\n",
       " 'mordendo',\n",
       " 'brincar',\n",
       " 'maravilhasinha',\n",
       " 'ega:',\n",
       " 'arrebanhados',\n",
       " 'dever!',\n",
       " 'lugares',\n",
       " 'deleitar',\n",
       " 'solidade',\n",
       " 'irritante',\n",
       " 'emocoes',\n",
       " 'niniche!',\n",
       " 'lutador:',\n",
       " 'versos',\n",
       " 'necessidade',\n",
       " 'raspo',\n",
       " 'infortunio',\n",
       " 'movel',\n",
       " 'vizinho',\n",
       " 'apreciava',\n",
       " 'adiantou',\n",
       " 'disselhe',\n",
       " 'vitrines',\n",
       " 'faziam',\n",
       " 'alugado',\n",
       " 'enlevadamente',\n",
       " 'finorio',\n",
       " 'perseguida',\n",
       " 'joelho',\n",
       " 'honesta',\n",
       " 'dolente:',\n",
       " 'capote',\n",
       " 'fluente',\n",
       " 'tremulos',\n",
       " 'posicao',\n",
       " 'lapi',\n",
       " 'bruscamente:',\n",
       " 'postico!',\n",
       " 'despachar',\n",
       " 'mobiladas',\n",
       " 'japonesas',\n",
       " 'edificante',\n",
       " 'embebendo',\n",
       " 'havaneza',\n",
       " 'petrificado',\n",
       " 'falaremo',\n",
       " 'digeria',\n",
       " 'lolita',\n",
       " 'tigela',\n",
       " 'trabalhadores',\n",
       " 'dilacerar',\n",
       " 'divina',\n",
       " 'irritou',\n",
       " 'janto',\n",
       " 'ferozmente',\n",
       " 'prevencao',\n",
       " 'repassados',\n",
       " 'beneficio',\n",
       " 'revestira',\n",
       " 'necessitara',\n",
       " 'assado',\n",
       " 'desesperada:',\n",
       " 'banal',\n",
       " 'interessantes',\n",
       " 'cercaria',\n",
       " 'meninos',\n",
       " 'nascentes',\n",
       " 'preceito',\n",
       " 'soma',\n",
       " 'curei',\n",
       " 'doido?',\n",
       " 'amarrotado',\n",
       " 'como',\n",
       " 'frequente!',\n",
       " 'candelabros',\n",
       " 'transformismo',\n",
       " 'chuviscava',\n",
       " 'amant',\n",
       " 'enganos',\n",
       " 'declamamos',\n",
       " 'regelava',\n",
       " 'aniquilassemos!',\n",
       " 'reluziu',\n",
       " 'necessariamente',\n",
       " 'estirava',\n",
       " 'costumada',\n",
       " 'espessas',\n",
       " 'dormir',\n",
       " 'indiscreta',\n",
       " 'conventos',\n",
       " 'questao!',\n",
       " 'romanticamente',\n",
       " 'marie?',\n",
       " 'usarem',\n",
       " 'relanceara',\n",
       " 'empalidecer',\n",
       " 'idoso',\n",
       " 'maconaria',\n",
       " 'ficara',\n",
       " 'negreira',\n",
       " 'completava',\n",
       " 'entrelinha',\n",
       " 'campeao',\n",
       " 'regimentos',\n",
       " 'naturalmente',\n",
       " 'pouco!',\n",
       " 'coitada!',\n",
       " 'bristol!',\n",
       " 'dobrar',\n",
       " 'darque',\n",
       " 'portugal',\n",
       " 'estragou',\n",
       " 'perceptor',\n",
       " 'aflicoe',\n",
       " 'pulgas',\n",
       " 'suburbio',\n",
       " 'veus',\n",
       " 'atarracadas',\n",
       " 'similitudes',\n",
       " 'vale!',\n",
       " 'pareceme',\n",
       " 'traves',\n",
       " 'excelencia?',\n",
       " 'antiquado',\n",
       " 'pessimamente!',\n",
       " 'escancarado',\n",
       " 'enlouquecera',\n",
       " 'tremoco',\n",
       " ...]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df= prepare_data()\n",
    "\n",
    "complete_text = ' '.join(df.sentences.values.tolist())\n",
    "complete_unique_words_list = list(set(complete_text.split(' ')))\n",
    "vocab_len = len(complete_unique_words_list)\n",
    "display(df)\n",
    "display(vocab_len)\n",
    "display(complete_unique_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>sent_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a casa que vieram habitar em lisboa , no outon...</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apesar deste fresco nome de vivenda campestre...</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>em monsenhor bucarini , nuncio de sao santida...</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>este inutil pardieiro como lhe chamava vilaca...</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nessa ocasiao vendera se outra propriedade d ...</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10178</th>\n",
       "      <td>mas carlos queria realmente saber se , no fun...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10188</th>\n",
       "      <td>tudo aceitar , o que vem e o que foge , com a...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10192</th>\n",
       "      <td>se me dissessem que ali em baixo estava uma f...</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10195</th>\n",
       "      <td>e ambos retardaram o passo , descendo para a ...</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10196</th>\n",
       "      <td>ja avistavam o aterro , a sua longa fila de l...</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1781 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentences  sent_length\n",
       "0      a casa que vieram habitar em lisboa , no outon...           42\n",
       "1       apesar deste fresco nome de vivenda campestre...          129\n",
       "3       em monsenhor bucarini , nuncio de sao santida...          210\n",
       "4       este inutil pardieiro como lhe chamava vilaca...           64\n",
       "5       nessa ocasiao vendera se outra propriedade d ...           53\n",
       "...                                                  ...          ...\n",
       "10178   mas carlos queria realmente saber se , no fun...           49\n",
       "10188   tudo aceitar , o que vem e o que foge , com a...           61\n",
       "10192   se me dissessem que ali em baixo estava uma f...           41\n",
       "10195   e ambos retardaram o passo , descendo para a ...           47\n",
       "10196   ja avistavam o aterro , a sua longa fila de l...           58\n",
       "\n",
       "[1781 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"sent_length\"] = df.sentences.apply(lambda x: len(x.split()))\n",
    "\n",
    "plt.figure()\n",
    "pd.Series(df.sent_length).hist()\n",
    "plt.title(\"Sentence Size for training dataset\")\n",
    "df[df.sent_length>window_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>sent_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a casa que vieram habitar em lisboa , no outon...</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apesar deste fresco nome de vivenda campestre...</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>longos anos o ramalhete permanecera desabitad...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>em monsenhor bucarini , nuncio de sao santida...</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>este inutil pardieiro como lhe chamava vilaca...</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10204</th>\n",
       "      <td>com efeito , nao vale a pena fazer um esforco...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10205</th>\n",
       "      <td>ega , ao lado , ajuntava , ofegante , atirand...</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10206</th>\n",
       "      <td>a lanterna vermelha do americano , ao longe ,...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10207</th>\n",
       "      <td>e foi em carlos e em joao da ega uma esperanc...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10208</th>\n",
       "      <td>entao , para apanhar o americano , os dois am...</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9687 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentences  sent_length\n",
       "0      a casa que vieram habitar em lisboa , no outon...           42\n",
       "1       apesar deste fresco nome de vivenda campestre...          129\n",
       "2       longos anos o ramalhete permanecera desabitad...           25\n",
       "3       em monsenhor bucarini , nuncio de sao santida...          210\n",
       "4       este inutil pardieiro como lhe chamava vilaca...           64\n",
       "...                                                  ...          ...\n",
       "10204   com efeito , nao vale a pena fazer um esforco...           18\n",
       "10205   ega , ao lado , ajuntava , ofegante , atirand...           33\n",
       "10206   a lanterna vermelha do americano , ao longe ,...           14\n",
       "10207   e foi em carlos e em joao da ega uma esperanc...           29\n",
       "10208   entao , para apanhar o americano , os dois am...           31\n",
       "\n",
       "[9687 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.sent_length>=5]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'data scientist tasks': 0.5333333283333335,\n",
       "  'data scientist jobs': 0.5333333283333335,\n",
       "  'data scientist job': 0.4666666616666668,\n",
       "  'data scientist, and': 0.3333333283333334,\n",
       "  'data engineer tasks': 0.4666666616666667,\n",
       "  'data engineer jobs': 0.4666666616666667},\n",
       " 'data scientist jobs',\n",
       " 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_list=['data scientist tasks', 'data scientist jobs','data scientist job','data scientist, and','data engineer tasks','data engineer jobs']\n",
    "\n",
    "def Minimum_Bayes_Risk(list_of_candidates, field = \"rouge-l\", candidate_reference = None):\n",
    "    candidate_score = {}\n",
    "    max_score = 0\n",
    "    best_score_candidate= ''\n",
    "    best_candidate_index = -1\n",
    "    if candidate_reference == None :\n",
    "        for candidate_ref in list_of_candidates:\n",
    "            \n",
    "            rest_of_candidates = copy.deepcopy(list_of_candidates)\n",
    "            if candidate_ref in rest_of_candidates : rest_of_candidates.remove(candidate_ref)\n",
    "            score = 0\n",
    "\n",
    "            for candidate in rest_of_candidates:\n",
    "                score += ROUGE.get_scores(candidate, candidate_ref)[0][field][\"f\"]\n",
    "            \n",
    "            score = score/len(rest_of_candidates)\n",
    "\n",
    "            candidate_score[candidate_ref] = score\n",
    "\n",
    "            if score >= max_score:\n",
    "                best_score_candidate =candidate_ref\n",
    "                best_candidate_index = list_of_candidates.index(candidate_ref)\n",
    "                max_score = score\n",
    "\n",
    "    else:\n",
    "            \n",
    "            for candidate in list_of_candidates:\n",
    "                score = 0\n",
    "                score = ROUGE.get_scores(candidate, candidate_reference)[0][field][\"f\"]\n",
    "                if score >= max_score:\n",
    "                    best_score_candidate =candidate_ref\n",
    "\n",
    "\n",
    "    return candidate_score, best_score_candidate, best_candidate_index\n",
    "\n",
    "candidate_score, best_score_candidate, best_candidate_index = Minimum_Bayes_Risk(candidate_list)\n",
    "candidate_score, best_score_candidate , best_candidate_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting vocabs and tokenizing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>sent_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5113</th>\n",
       "      <td>para que servia entao o grande movimento natu...</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7606</th>\n",
       "      <td>o ministro da finlandia abriu os bracos para ...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9375</th>\n",
       "      <td>e se nao partes para santa olavia , eu vou pa...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1332</th>\n",
       "      <td>que perguntavas tu , filho? disse enfim afons...</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9956</th>\n",
       "      <td>o que , o damaso , coitado .</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9027</th>\n",
       "      <td>o avo deve saber! afonso da maia , que um tre...</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2585</th>\n",
       "      <td>tudo isto o assustava .</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5314</th>\n",
       "      <td>comecava eu entao a minha carreira publica .</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9374</th>\n",
       "      <td>recebi uma carta de minha mae .</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5287</th>\n",
       "      <td>carlos encolheu os ombro sao como podia ela a...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7749 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentences  sent_length\n",
       "5113   para que servia entao o grande movimento natu...           36\n",
       "7606   o ministro da finlandia abriu os bracos para ...           25\n",
       "9375   e se nao partes para santa olavia , eu vou pa...           13\n",
       "1332   que perguntavas tu , filho? disse enfim afons...           47\n",
       "9956                      o que , o damaso , coitado .             8\n",
       "...                                                 ...          ...\n",
       "9027   o avo deve saber! afonso da maia , que um tre...           33\n",
       "2585                           tudo isto o assustava .             5\n",
       "5314      comecava eu entao a minha carreira publica .             8\n",
       "9374                   recebi uma carta de minha mae .             7\n",
       "5287   carlos encolheu os ombro sao como podia ela a...           21\n",
       "\n",
       "[7749 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>sent_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4919</th>\n",
       "      <td>tinha diante de si os tres meses em que ela e...</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5034</th>\n",
       "      <td>pareceu lhe que trazia o ar escorracado , e s...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1612</th>\n",
       "      <td>isto so a pontape .</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9806</th>\n",
       "      <td>olha este horror! a ciencia para tudo acha um...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>foi um tio , um negociante de calcuta ou de a...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9561</th>\n",
       "      <td>e , sobre a rua deserta , cerrou se finalment...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10192</th>\n",
       "      <td>se me dissessem que ali em baixo estava uma f...</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2978</th>\n",
       "      <td>vamos indo primeiro a lawrence .</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7245</th>\n",
       "      <td>mas o domingos servia o anana sao e o ega pro...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6792</th>\n",
       "      <td>carlos parara , comovido .</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1938 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentences  sent_length\n",
       "4919    tinha diante de si os tres meses em que ela e...           31\n",
       "5034    pareceu lhe que trazia o ar escorracado , e s...           15\n",
       "1612                                isto so a pontape .             5\n",
       "9806    olha este horror! a ciencia para tudo acha um...           24\n",
       "1262    foi um tio , um negociante de calcuta ou de a...           21\n",
       "...                                                  ...          ...\n",
       "9561    e , sobre a rua deserta , cerrou se finalment...           19\n",
       "10192   se me dissessem que ali em baixo estava uma f...           41\n",
       "2978                   vamos indo primeiro a lawrence .             6\n",
       "7245    mas o domingos servia o anana sao e o ega pro...           18\n",
       "6792                         carlos parara , comovido .             5\n",
       "\n",
       "[1938 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_val_test = train_test_split(df,test_size=0.2,train_size=0.8)\n",
    "\n",
    "display(df_train)\n",
    "display(df_val_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>sent_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9453</th>\n",
       "      <td>carlos beijou a mao fria que pendia .</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8871</th>\n",
       "      <td>que diabo , senhor , e necessario ter topete!...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8476</th>\n",
       "      <td>por fim carlos atirou o contra a porta duma c...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8711</th>\n",
       "      <td>acendeu a sua palmatoria entreabriu o reposte...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1673</th>\n",
       "      <td>ouco por ali lamenta la .</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3496</th>\n",
       "      <td>carlos entao zangou se .</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>nao temos nada capaz de dar a um rapaz um boc...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2398</th>\n",
       "      <td>eu ca vou de selvagem .</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1890</th>\n",
       "      <td>alencar ja tinha carlos estreitado ao peito ,...</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1559</th>\n",
       "      <td>ao findar o atomo exclamava , com a vasta sol...</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1453 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentences  sent_length\n",
       "9453             carlos beijou a mao fria que pendia .             8\n",
       "8871   que diabo , senhor , e necessario ter topete!...           30\n",
       "8476   por fim carlos atirou o contra a porta duma c...           11\n",
       "8711   acendeu a sua palmatoria entreabriu o reposte...           24\n",
       "1673                         ouco por ali lamenta la .             6\n",
       "...                                                 ...          ...\n",
       "3496                          carlos entao zangou se .             5\n",
       "3996   nao temos nada capaz de dar a um rapaz um boc...           14\n",
       "2398                           eu ca vou de selvagem .             6\n",
       "1890   alencar ja tinha carlos estreitado ao peito ,...           90\n",
       "1559   ao findar o atomo exclamava , com a vasta sol...           51\n",
       "\n",
       "[1453 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>sent_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>matriculou se realmente com entusiasmo .</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10009</th>\n",
       "      <td>isto e fantastico , ega! ega esfregava as mao...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6745</th>\n",
       "      <td>ia a descer a vidraca que faltava a correia q...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>maria sentia lhes por cima as risada sao as v...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2730</th>\n",
       "      <td>dai a pouco cruges , que devorava , exclamou ...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237</th>\n",
       "      <td>por isso ninguem agora lograva ter os seus sa...</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7753</th>\n",
       "      <td>e ele , bondoso como era , prometia , dizia: ...</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5703</th>\n",
       "      <td>nao se tinham visto desde as corridas , o poe...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9439</th>\n",
       "      <td>os criados em redor olharam , aterrado sao e ...</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2685</th>\n",
       "      <td>e nao digas nada la dentro .</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>485 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentences  sent_length\n",
       "1032           matriculou se realmente com entusiasmo .             6\n",
       "10009   isto e fantastico , ega! ega esfregava as mao...           26\n",
       "6745    ia a descer a vidraca que faltava a correia q...           20\n",
       "335     maria sentia lhes por cima as risada sao as v...           14\n",
       "2730    dai a pouco cruges , que devorava , exclamou ...           26\n",
       "...                                                  ...          ...\n",
       "5237    por isso ninguem agora lograva ter os seus sa...           38\n",
       "7753    e ele , bondoso como era , prometia , dizia: ...           55\n",
       "5703    nao se tinham visto desde as corridas , o poe...           25\n",
       "9439    os criados em redor olharam , aterrado sao e ...           52\n",
       "2685                       e nao digas nada la dentro .             7\n",
       "\n",
       "[485 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "df_test, df_val = train_test_split(df_val_test, test_size = 0.25,train_size =0.75)\n",
    "display(df_test)\n",
    "display(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b' aqui e alem esvoacava um lenco branco . '\n",
      " b' carlos , entusiasmado com a letra dela , quasi comparavel a lendaria letra do damaso , ocupava a agora incessantemente como copista , sentindo mais amor por um trabalho a que ela se associava . '\n",
      " b' quer voce fazer uma partida de bilhar , o marques? va la , homem . '\n",
      " b' a mama nao saia do catre , doente , sucumbida , chorando . '\n",
      " b' e o ega , o grande ega? . '\n",
      " b' e limpando negligentemente o monoculo ao lenco de seda branca: uma judia . '\n",
      " b' depois abria se um intermedio panteista: rompiam coros de flores , coros de astros , cantando na linguagem da luz , ou na eloquencia dos perfumes , a beleza , a graca , a pureza , a alma celeste de ester e de rachel . '\n",
      " b' nos tons agudos todo ele se ia alcando sobre a ponta dos pes , como levado no compasso vivo despegava entao a mao do peito , alargava um gesto , as belas joias dos seus aneis faiscavam . '\n",
      " b' nao ha outro , e unico! o bom deus fe lo num dia de grande verve , e depois quebrou a forma . '\n",
      " b' entao , disse carlos sorrindo , essa respeitavel fidalga . '\n",
      " b' era ja proximo da quinta , na volta de estrada , onde o muro fazia um recanto sob uma faia , defronte de sebes de piteiras resguardando campos de olivedo . '\n",
      " b' estou hoje duma estupidez! vilaca considerou o um momento , com malicia . '\n",
      " b' que frases essas , menino! murmurou ega . '\n",
      " b' uma canalha! exclamou o marques com um dos seus resumos brutais que varriam tudo . '\n",
      " b' ja entao os meus principios me nao permitiam entrar nesses covis religiosos: mas enfim fui acompanhar a mae . '\n",
      " b' de certo o ferira a ideia de maria , longe , num quarto alheio , agasalhando se lhe no leito do adulterio entre os bracos do outro . '], shape=(16,), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 12:16:19.494292: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-30 12:16:19.498322: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-30 12:16:19.501298: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-30 12:16:19.631122: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-30 12:16:19.632779: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-30 12:16:19.634312: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-30 12:16:19.635849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21208 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2024-07-30 12:16:19.864183: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "train_tfds = tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            df_train.sentences.tolist()\n",
    "        )\n",
    "    ).batch(batch_size).shuffle(buffer_size=len(df_train)).prefetch(tf.data.AUTOTUNE)\n",
    "for x in train_tfds.take(1):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b' matriculou se realmente com entusiasmo . '\n",
      " b' isto e fantastico , ega! ega esfregava as mao sao sim , mas precioso! porque essa simples forma de botas explicava todo o portugal contemporaneo . '\n",
      " b' ia a descer a vidraca que faltava a correia quando a tipoia parou de repente , na estrada solitaria . '\n",
      " b' maria sentia lhes por cima as risada sao as vezes tocava se viola . '\n",
      " b' dai a pouco cruges , que devorava , exclamou com a boca cheia: o reno tambem deve ser magnifico! carlos olhou o espantado e rindo . '\n",
      " b' alem disso , ega nao saberia tudo , mais tarde ou mais cedo , pela tagarelice alheia? antes lho dissesse ele , fraternalmente . '\n",
      " b' olha este horror! a ciencia para tudo acha um remedio , menos para a calva! transformam se as civilizacoes , a calva fica! . '\n",
      " b' e ela? oh , ela dormira dum sono so . ' b' ega ja pensara nisso . '\n",
      " b' foi um tio , um negociante de calcuta ou de australia , um nababo , que lhe deixou a fortuna . '\n",
      " b' mas o pior nao foi isso . '\n",
      " b' a arlesiana , essa , tambem a cada momento aparecia la a levar toalhas de rendas , um acucareiro que ninguem reclamara , ou algum vaso com flores para alegrar a alcova . '\n",
      " b' mas , ao mesmo tempo , fazia de cavalheiro , falava de dar a desforra , ficar ali , sendo necessario , ate de madrugada . '\n",
      " b' e o derradeiro , esvaido eco da carta do damaso foi , na vespera do sarau da trindade , um paragrafo da propria tarde onde ela fora publicada , nestas amaveis palavras: o nosso amigo e distinto sportman damaso salcede parte brevemente para uma viagem de recreio a italia . '\n",
      " b' que tolice! o avo tinha quasi oitenta anos , e uma doenca de coracao . '\n",
      " b' muito , e ha muitos anos , meu senhor! exclamou o velho procurador , tao comovido que mal podia erguer o calice na mao . '], shape=(16,), dtype=string)\n",
      "tf.Tensor(\n",
      "[b' a porta do bufete voltou se ainda , ergueu o chapeu . '\n",
      " b' os poetas da academia fizeram lhe versos em que encarnacion foi chamada lirio de israel , pomba da arca , e nuvem da manha . '\n",
      " b' tao desagradavel para mim . '\n",
      " b' apenas carlos se sentou ao pe dela , don maria perguntou lhe logo por esse aventureiro do ega . '\n",
      " b' mas depois , ao regressar da quinta , vinha ja mais calmo . '\n",
      " b' ao saltar um barranco , a espingarda disparase lhe , e a carga , zas , vai cravar se no napolitano! nao era possivel fazer curativos na tojeira , e voltaram logo a lisboa . '\n",
      " b' e o senhor afonso da maia , exclamou vilaca , a mudar de habitos , nessa idade! o que e ser avo , meu senhor! tolice! nao e isso . '\n",
      " b' entao ega teve so um desejo , o desesperado desejo de findar . '\n",
      " b' gambeta nunca o via , que nao lhe gritasse de longe , em espanhol: hombre , compadre! e ele tambem , logo: compadre , caramba! dai ficara a alcunha , e gambeta ria . '\n",
      " b' nao o querendo receber , ali , em mangas de camisa , mandou o entrar para o gabinete escarlate e preto . '\n",
      " b' que beleza de noite! pararam a porta do teatro da trindade no momento em que , duma tipoia de praca , se apeava um sujeito de barbas de apostolo , todo de luto , com um chapeu de largas abas recurvas a moda de  . '\n",
      " b' e a senhor don maria , acrescentou ele , devia ir! . '\n",
      " b' carlos acompanhou os ate ao reservado , num outro wagon que se estivera metendo de novo para sao exc . '\n",
      " b' fizeram se todas as pesquisa sao  . '\n",
      " b' sintra era entao um ninho de amores , e sob as suas romanticas ramagens as fidalgas abandonavam se aos bracos dos poeta sao elas eram elviras , eles eram anthony sao o dinheiro abundava a corte era alegre a regeneracao literata e galante ia engrandecer o pais , belo jardim da europa os bachareis chegavam de coimbra , frementes de eloquencia os ministros da coroa recitavam ao piano o mesmo sopro lirico inchava as odes e os projectos de lei . '\n",
      " b' fez se a paz , o cerco acabou . '], shape=(16,), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 12:16:19.877125: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-07-30 12:16:19.887974: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "dev_tfds = tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            df_val.sentences.tolist()\n",
    "        )\n",
    "    ).batch(batch_size).shuffle(buffer_size=len(df_val)).prefetch(tf.data.AUTOTUNE)\n",
    "for x in dev_tfds.take(1):\n",
    "    print(x)\n",
    "\n",
    "test_tfds = tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            df_test.sentences.tolist()\n",
    "        )\n",
    "    ).batch(batch_size).shuffle(buffer_size=len(df_test)).prefetch(tf.data.AUTOTUNE)\n",
    "for x in test_tfds.take(1):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 12:16:20.403609: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "## Train tokenizer vocabulary on training data only. Words that occur only on the test data will be unknown, and this is the expected behaviour when considering real word environment.\n",
    "\n",
    "vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
    "    train_tfds,\n",
    "    vocabulary_size=vocab_len,\n",
    "    lowercase=True,\n",
    "    reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\"]#, \"[EOS]\"],\n",
    ")\n",
    "\n",
    "#WordPieceTokenizer is an efficient implementation of the WordPiece algorithm used by BERT and other models. \n",
    "tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=vocab,\n",
    "    sequence_length=window_size,\n",
    "    lowercase=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set ...\n",
      "input :  tf.Tensor(\n",
      "[   2   44   38    8  887   81 1295    8   42 1250  140    8   37   34\n",
      "  222 1298  274  407    5    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0], shape=(40,), dtype=int32)\n",
      "output :  tf.Tensor(\n",
      "[  44   38    8  887   81 1295    8   42 1250  140    8   37   34  222\n",
      " 1298  274  407    5    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0], shape=(40,), dtype=int32)\n",
      "Detokenized input:  tf.Tensor(b'[BOS] nao se abandonar a uma esperanca nem a um desapontamento . [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', shape=(), dtype=string)\n",
      "Detokenized output:  tf.Tensor(b'nao se abandonar a uma esperanca nem a um desapontamento . [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', shape=(), dtype=string)\n",
      "Dev set ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 12:16:33.068844: E tensorflow/core/util/util.cc:131] oneDNN supports DT_INT64 only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.\n",
      "2024-07-30 12:16:33.113053: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-07-30 12:16:33.248788: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  tf.Tensor(\n",
      "[   2 2523   38   70  100    4   47  176  642 1789   43    5    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0], shape=(40,), dtype=int32)\n",
      "output :  tf.Tensor(\n",
      "[2523   38   70  100    4   47  176  642 1789   43    5    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0], shape=(40,), dtype=int32)\n",
      "Detokenized input:  tf.Tensor(b'[BOS] passa se la bem , as tercas feira sao . [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', shape=(), dtype=string)\n",
      "Detokenized output:  tf.Tensor(b'passa se la bem , as tercas feira sao . [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', shape=(), dtype=string)\n",
      "Testing set ...\n",
      "input :  tf.Tensor(\n",
      "[   2   12  171  316   37  111   24 2233   60  488    5    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0], shape=(40,), dtype=int32)\n",
      "output :  tf.Tensor(\n",
      "[  12  171  316   37  111   24 2233   60  488    5    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0], shape=(40,), dtype=int32)\n",
      "Detokenized input:  tf.Tensor(b'[BOS] e ser talvez um grande quimico . [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', shape=(), dtype=string)\n",
      "Detokenized output:  tf.Tensor(b'e ser talvez um grande quimico . [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', shape=(), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 12:16:33.383765: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# packer adds a start token\n",
    "start_packer = keras_nlp.layers.StartEndPacker(\n",
    "    sequence_length=window_size,\n",
    "    start_value=tokenizer.token_to_id(\"[BOS]\"),\n",
    "    #end_value = tokenizer.token_to_id(\"[EOS]\"),\n",
    ")\n",
    "\n",
    "def preprocess(inputs):\n",
    "    outputs = tokenizer(inputs)\n",
    "    features = start_packer(outputs)\n",
    "    labels = outputs\n",
    "    return features, labels\n",
    "\n",
    "train_tfds= train_tfds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).prefetch(\n",
    "    tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "dev_tfds= dev_tfds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).prefetch(\n",
    "    tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "\n",
    "test_tfds= test_tfds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).prefetch(\n",
    "    tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "print(\"Training set ...\")\n",
    "for i, o in train_tfds.take(1):\n",
    "    print(\"input : \",i[0])\n",
    "    print(\"output : \",o[0])\n",
    "    print(\"Detokenized input: \",tokenizer.detokenize(i[0]))\n",
    "    print(\"Detokenized output: \",tokenizer.detokenize(o[0]))\n",
    "\n",
    "\n",
    "print(\"Dev set ...\")\n",
    "for i, o in dev_tfds.take(1):\n",
    "    print(\"input : \",i[0])\n",
    "    print(\"output : \",o[0])\n",
    "    print(\"Detokenized input: \",tokenizer.detokenize(i[0]))\n",
    "    print(\"Detokenized output: \",tokenizer.detokenize(o[0]))\n",
    "\n",
    "\n",
    "print(\"Testing set ...\")\n",
    "for i, o in test_tfds.take(1):\n",
    "    print(\"input : \",i[0])\n",
    "    print(\"output : \",o[0])\n",
    "    print(\"Detokenized input: \",tokenizer.detokenize(i[0]))\n",
    "    print(\"Detokenized output: \",tokenizer.detokenize(o[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_len = vocab_len, \n",
    "        window_size = window_size, \n",
    "        embedding_dim = embedding_dim,  \n",
    "        n_layers = 1, \n",
    "        n_attention_head = 2, \n",
    "        feed_forward_dim = [128],\n",
    "        dropout_rate = [0.0]):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.num_layers = n_layers\n",
    "        self.n_attention_head =n_attention_head\n",
    "        self.feed_forward_dim = feed_forward_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "            vocabulary_size=vocab_len,\n",
    "            sequence_length=window_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            mask_zero=True,\n",
    "        )\n",
    "        \n",
    "        self.decoder_layer = [keras_nlp.layers.TransformerDecoder(num_heads=self.n_attention_head, intermediate_dim=self.feed_forward_dim[i],dropout = dropout_rate[i]) for i in range(self.num_layers)]\n",
    "        self.output_dense = Dense(vocab_len)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.embedding_layer(inputs)\n",
    "        for i in range(self.num_layers ):\n",
    "            x = self.decoder_layer[i](x)\n",
    "\n",
    "        outputs_logits  = self.output_dense(x)\n",
    "\n",
    "        return outputs_logits\n",
    "\n",
    "\n",
    "# generate_text_model = TextGenerator(\n",
    "#     vocab_len = vocab_len,\n",
    "#     window_size = window_size, \n",
    "#     embedding_dim = embedding_dim,  \n",
    "#     n_layers = 1, \n",
    "#     n_attention_head = 2, \n",
    "#     feed_forward_dim = [128],\n",
    "#     dropout_rate = [0.0]\n",
    "#     )\n",
    "\n",
    "# loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
    "\n",
    "# generate_text_model.compile(\n",
    "#     loss=loss_fn, \n",
    "#     optimizer=Adam(learning_rate=0.0005), \n",
    "#     metrics=[perplexity])\n",
    "\n",
    "# for i, o in train_tfds.take(1):\n",
    "#     print(i.shape, o.shape)\n",
    "#     print(generate_text_model(i).shape)\n",
    "\n",
    "# generate_text_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# generate_text_model.fit(train_tfds, validation_data= dev_tfds, epochs=epochs, batch_size=32, verbose =1, callbacks=[stop_early, tensorboard_callback, text_generation_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 40), dtype=int32, numpy=\n",
       "array([[2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_tokens = start_packer(tokenizer([\"\"]))\n",
    "prompt_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next(prompt, cache, index):\n",
    "    logits = generate_text_model(prompt)[:, index - 1, :]\n",
    "    # Ignore hidden states for now; only needed for contrastive search.\n",
    "    hidden_states = None\n",
    "    return logits, hidden_states, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and tunning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopKTextGenerator(keras.callbacks.Callback):\n",
    "    \"\"\"A callback to generate text from a trained model using top-k.\"\"\"\n",
    "\n",
    "    def __init__(self, k):\n",
    "        self.top_k_sampler = keras_nlp.samplers.TopKSampler(k)\n",
    "        #self.beam_search_sampler = keras_nlp.samplers.BeamSampler(num_beams=10)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        output_tokens = self.top_k_sampler(\n",
    "            next=next,\n",
    "            prompt=prompt_tokens,\n",
    "            index=1,\n",
    "        )\n",
    "        txt = tokenizer.detokenize(output_tokens)\n",
    "        print(f\"Top-K search generated text: \\n{txt}\\n\")\n",
    "        \n",
    "text_generation_callback = TopKTextGenerator(k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from Hyperparam_tunning/keras_att_tunning/tuner0.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras_tuner.src.engine.hyperparameters.hyperparameters.HyperParameters at 0x71e41476c690>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=f\"logs/training/\")\n",
    "\n",
    "def get_model_tunning(hp):\n",
    "\n",
    "\n",
    "    n_att_layers = hp.Int('n_att_layers', 1, 3)\n",
    "    n_attention_head= hp.Int('n_att_layers', 1, 3)\n",
    "    feed_forward_dim_list = []\n",
    "    dropout_rate = []\n",
    "    if n_att_layers > 0:\n",
    "        feed_forward_dim_list =  [hp.Int('att_units_'+str(i), 32, 128) for i in range(n_att_layers)]\n",
    "        dropout_rate=  [hp.Float('att_dropout_rate_'+str(i), 0.0, 0.5, step = 0.1) for i in range(n_att_layers)]\n",
    "    \n",
    "    hp_learning_rate = hp.Float('learning_rate', 0.000001, 0.001)\n",
    "    embedding_dim_list = 150#hp.Int('embedding_dim', 100,  200, step = 50)\n",
    "    \n",
    "    model = TextGenerator(\n",
    "        vocab_len = vocab_len,\n",
    "        window_size = window_size, \n",
    "        embedding_dim = embedding_dim_list,  \n",
    "        n_layers = n_att_layers, \n",
    "        n_attention_head = n_attention_head, \n",
    "        feed_forward_dim = feed_forward_dim_list,\n",
    "        dropout_rate = dropout_rate\n",
    "    )\n",
    "\n",
    "    loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
    "\n",
    "    model.compile(\n",
    "        loss=loss_fn, \n",
    "        optimizer=Adam(learning_rate=hp_learning_rate), \n",
    "        metrics=[perplexity],\n",
    "        )\n",
    "\n",
    "    for i, o in train_tfds.take(1):\n",
    "        print(i.shape, o.shape)\n",
    "        print(model(i).shape)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "        get_model_tunning,\n",
    "        objective= kt.Objective('val_perplexity', direction=\"min\"), #kt.Objective('val_auc', direction=\"max\"),# #val_binary_accuracy\n",
    "        max_trials = 30,\n",
    "        directory=r\"Hyperparam_tunning\",\n",
    "        project_name='keras_att_tunning',\n",
    "    )\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50)#val_auc\n",
    "if TUNNING:\n",
    "    tuner.search(train_tfds,  epochs=100, batch_size=batch_size, validation_data = test_tfds, verbose =1, callbacks=[stop_early, tensorboard_callback]) \n",
    "\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_hps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'_name_scopes': [], '_conditions': [], '_hps': defaultdict(<class 'list'>, {'n_att_layers': [Int(name: 'n_att_layers', min_value: 1, max_value: 3, step: 1, sampling: linear, default: 1)], 'att_units_0': [Int(name: 'att_units_0', min_value: 32, max_value: 128, step: 1, sampling: linear, default: 32)], 'att_dropout_rate_0': [Float(name: 'att_dropout_rate_0', min_value: '0.0', max_value: '0.5', step: '0.1', sampling: 'linear', default: '0.0')], 'learning_rate': [Float(name: 'learning_rate', min_value: '1e-06', max_value: '0.001', step: 'None', sampling: 'linear', default: '1e-06')], 'att_units_1': [Int(name: 'att_units_1', min_value: 32, max_value: 128, step: 1, sampling: linear, default: 32)], 'att_dropout_rate_1': [Float(name: 'att_dropout_rate_1', min_value: '0.0', max_value: '0.5', step: '0.1', sampling: 'linear', default: '0.0')], 'att_units_2': [Int(name: 'att_units_2', min_value: 32, max_value: 128, step: 1, sampling: linear, default: 32)], 'att_dropout_rate_2': [Float(name: 'att_dropout_rate_2', min_value: '0.0', max_value: '0.5', step: '0.1', sampling: 'linear', default: '0.0')]}), '_space': [Int(name: 'n_att_layers', min_value: 1, max_value: 3, step: 1, sampling: linear, default: 1), Int(name: 'att_units_0', min_value: 32, max_value: 128, step: 1, sampling: linear, default: 32), Float(name: 'att_dropout_rate_0', min_value: '0.0', max_value: '0.5', step: '0.1', sampling: 'linear', default: '0.0'), Float(name: 'learning_rate', min_value: '1e-06', max_value: '0.001', step: 'None', sampling: 'linear', default: '1e-06'), Int(name: 'att_units_1', min_value: 32, max_value: 128, step: 1, sampling: linear, default: 32), Float(name: 'att_dropout_rate_1', min_value: '0.0', max_value: '0.5', step: '0.1', sampling: 'linear', default: '0.0'), Int(name: 'att_units_2', min_value: 32, max_value: 128, step: 1, sampling: linear, default: 32), Float(name: 'att_dropout_rate_2', min_value: '0.0', max_value: '0.5', step: '0.1', sampling: 'linear', default: '0.0')], 'values': {'n_att_layers': 3, 'att_units_0': 32, 'att_dropout_rate_0': 0.0, 'learning_rate': 0.0007851818315362867, 'att_units_1': 128, 'att_dropout_rate_1': 0.4, 'att_units_2': 95, 'att_dropout_rate_2': 0.0}, 'active_scopes': [], 'inactive_scopes': []}\n",
      "(16, 40) (16, 40)\n",
      "(16, 40, 22255)\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 14:24:37.096057: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - loss: 6.5315 - perplexity: 1394.6074 - val_loss: 4.9800 - val_perplexity: 165.5515\n",
      "Epoch 2/300\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 4.8202 - perplexity: 140.9274 - val_loss: 4.6483 - val_perplexity: 117.8062\n",
      "Epoch 3/300\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 4.3936 - perplexity: 90.9605 - val_loss: 4.5105 - val_perplexity: 102.2704\n",
      "Epoch 4/300\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 4.1371 - perplexity: 69.6579 - val_loss: 4.4512 - val_perplexity: 96.2299\n",
      "Epoch 5/300\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 3.9007 - perplexity: 54.7768 - val_loss: 4.4449 - val_perplexity: 95.6116\n",
      "Epoch 6/300\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 3.6659 - perplexity: 43.1885 - val_loss: 4.4811 - val_perplexity: 99.2324\n",
      "Epoch 7/300\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 3.4613 - perplexity: 34.9248 - val_loss: 4.5564 - val_perplexity: 107.2001\n",
      "Epoch 8/300\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 3.2484 - perplexity: 28.1000 - val_loss: 4.6606 - val_perplexity: 119.2898\n",
      "Epoch 9/300\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 3.0618 - perplexity: 23.1680 - val_loss: 4.7584 - val_perplexity: 131.8857\n",
      "Epoch 10/300\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.8755 - perplexity: 19.1757 - val_loss: 4.8909 - val_perplexity: 151.0822\n",
      "Epoch 11/300\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.7082 - perplexity: 16.1496 - val_loss: 5.0385 - val_perplexity: 175.7842\n",
      "Epoch 12/300\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 2.5382 - perplexity: 13.5818 - val_loss: 5.1914 - val_perplexity: 205.6398\n",
      "Epoch 13/300\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.4075 - perplexity: 11.8720 - val_loss: 5.3752 - val_perplexity: 248.3266\n",
      "Epoch 14/300\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 2.2542 - perplexity: 10.1587 - val_loss: 5.5399 - val_perplexity: 294.0359\n",
      "Epoch 15/300\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.1403 - perplexity: 9.0308 - val_loss: 5.6878 - val_perplexity: 342.1660\n",
      "Epoch 16/300\n",
      "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.0455 - perplexity: 8.1808 - val_loss: 5.8577 - val_perplexity: 407.3437\n",
      "Epoch 17/300\n",
      "\u001b[1m474/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.9197 - perplexity: 7.1909"
     ]
    }
   ],
   "source": [
    "for i, best_hps in enumerate(tuner.get_best_hyperparameters(num_trials=3)):\n",
    "    print(f\"Best Hyperparameters: {best_hps.__dict__}\")\n",
    "    text_generator_model = tuner.hypermodel.build(best_hps)  # Build the model with best hyperparameters\n",
    "    \n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=f\"logs/attention_training/model{i}\")\n",
    "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50)#val_auc\n",
    "    text_generator_model.fit(train_tfds,  epochs=300, batch_size=batch_size, validation_data = test_tfds, verbose =1, callbacks=[stop_early, tensorboard_callback]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aaaaaaaaaaaaaaaaaaaaa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43maaaaaaaaaaaaaaaaaaaaa\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'aaaaaaaaaaaaaaaaaaaaa' is not defined"
     ]
    }
   ],
   "source": [
    "aaaaaaaaaaaaaaaaaaaaa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference using different techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy sampling approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = keras_nlp.samplers.GreedySampler()\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,  # Start sampling immediately after the [BOS] token.\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "print(f\"Greedy search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Search approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = keras_nlp.samplers.BeamSampler(num_beams=10)\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "print(f\"Beam search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = keras_nlp.samplers.RandomSampler()\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "print(f\"Random search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-K search approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = keras_nlp.samplers.TopKSampler(k=10)\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "print(f\"Top-K search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-P search approach\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = keras_nlp.samplers.TopPSampler(p=0.5)\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "print(f\"Top-P search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, o in dataset.take(1):aaaaaaaaaaaaaaaaaa\n",
    "    print(i)\n",
    "    \n",
    "emb_layer = Embedding(input_dim=vocab_len, input_length = window_size, output_dim=emb_dim, weights=[embeddings_matrix] ,name =\"emb_layer\", trainable=False, mask_zero = True)\n",
    "\n",
    "attention_layer = SelfMaskedDotProductAttention()\n",
    "attention_layer_causal1 = SelfMaskedDotProductAttention()\n",
    "#attention_layer_causal2 = SelfMaskedDotProductAttention(causal_mask_enabled=2)\n",
    "\n",
    "emb_layer = Embedding(input_dim=vocab_len, input_length = window_size, output_dim=emb_dim, weights=[embeddings_matrix] ,name =\"emb_layer\", trainable=False, mask_zero = True)\n",
    "query = emb_layer(i)\n",
    "values = emb_layer(i)\n",
    "\n",
    "context_vector, attention_weights = attention_layer(query,values,values, (i != 0))\n",
    "context_vector1, attention_weights1 = attention_layer_causal1(query,values,values, (i != 0))\n",
    "context_vector1, attention_weights_causal = attention_layer_causal1(query,values,values, (i != 0), (1 - tf.linalg.band_part(tf.ones((i.shape[1], i.shape[1])), -1, 0)))\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.subplot(3, 2, 1)\n",
    "plt.pcolormesh(attention_weights[:, 0, :])\n",
    "plt.title('Attention weights - Not causal')\n",
    "\n",
    "plt.subplot(3, 2, 2)\n",
    "plt.pcolormesh(i != 0)\n",
    "plt.title('Mask')\n",
    "\n",
    "plt.subplot(3, 2, 3)\n",
    "plt.pcolormesh(attention_weights1[:, 0, :])\n",
    "plt.title('Attention weights 1')\n",
    "\n",
    "plt.subplot(3, 2, 4)\n",
    "plt.pcolormesh(attention_weights[0])\n",
    "plt.title('Attention weights')\n",
    "\n",
    "plt.subplot(3, 2, 5)\n",
    "plt.pcolormesh(attention_weights1[0])\n",
    "plt.title('Attention weights 1')\n",
    "\n",
    "plt.subplot(3, 2, 5)\n",
    "plt.pcolormesh(attention_weights_causal[0])\n",
    "plt.title('Attention weights causal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blstm_enable = False\n",
    "\n",
    "if TRAIN_AND_SAVE:\n",
    "    model = ModelClass(vocab_len, model_type=1, causal_mask_enabled= 0, blstm_enable=blstm_enable, use_attention=False)\n",
    "    model.compile(loss=MaskedLossCustom(), optimizer=Adam(learning_rate=0.0005), metrics=[tf.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "    model_causal1 = ModelClass(vocab_len, model_type=1, causal_mask_enabled= 1, blstm_enable=blstm_enable, loop= False)\n",
    "    model_causal1.compile(loss=MaskedLossCustom(), optimizer=Adam(learning_rate=0.0005), metrics=[tf.metrics.SparseCategoricalAccuracy()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = vocab_len\n",
    "token_mask_ids = np.array([tokenizer.word_index[x] for x in  [ '<oov>', '[EOS]' ]])[:, None]#\n",
    "token_mask = np.zeros([vocab_size], dtype=np.bool)\n",
    "token_mask[np.array(token_mask_ids)] = True\n",
    "\n",
    "print(\"token_mask\", token_mask)\n",
    "\n",
    "\n",
    "def generate_sentence(model_inst, input_text, max_length=10, temperature=0.5, number_of_candidates = 20, ):\n",
    "    \n",
    "    \n",
    "    attention_per_candidate = []\n",
    "    result_text_per_candidate = []\n",
    "    input_text_per_candidate = []\n",
    "    output_text_per_candidate = []\n",
    "\n",
    "    for c in range(number_of_candidates):\n",
    "\n",
    "        last_state= None\n",
    "        last_activation = None\n",
    "        input_mask = None\n",
    "        last_attention = None\n",
    "\n",
    "        result_tokens = []\n",
    "    \n",
    "        attention = []\n",
    "\n",
    "        x = input_tokens\n",
    "        input_text= []\n",
    "        output_text = []\n",
    "        result_tokens = [w.numpy() for w in  x[0] if w != 0 ]\n",
    "        for k in range(max_length):\n",
    "\n",
    "            input_text.append(' '.join([tokenizer.index_word[w.numpy()] for w in  x[0] if w != 0 ]))\n",
    "            x = pad_sequences(x, padding = \"post\", maxlen = window_size)#\n",
    "\n",
    "            input_mask = x!= 0\n",
    "            t = np.count_nonzero(input_mask)\n",
    "\n",
    "            if no_state: last_state= None\n",
    "\n",
    "            predictions, last_state , attention_weights, last_activation, last_attention  =model_inst(x, last_attention, last_state,  last_activation)\n",
    "\n",
    "            if k > 0 and attention_weights is not None:\n",
    "                attention.append(attention_weights[0])\n",
    "\n",
    "            predicted_logits  = predictions[:,t-1,:]/temperature\n",
    "            predicted_logits = tf.where(token_mask, -np.inf, predicted_logits)\n",
    "            predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "            new_tokens = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "            if loop:\n",
    "            \n",
    "                x = new_tokens.numpy()\n",
    "            else : \n",
    "\n",
    "                x = list(x[0])\n",
    "                x = [i for i in x if i != 0]\n",
    "                x.append(new_tokens.numpy()[0])\n",
    "                \n",
    "\n",
    "            output_text.append(' '.join([tokenizer.index_word[w] for w in  x if w != 0 ][1:]))\n",
    "            x = tf.reshape(tf.convert_to_tensor(x), (len(input_tokens),len(x)))\n",
    "            result_tokens.append(new_tokens.numpy()[0])\n",
    "                \n",
    "            if tokenizer.word_index['[EOS]'] == new_tokens.numpy()[0]:\n",
    "                break\n",
    "\n",
    "\n",
    "        result_text =   ' '.join([tokenizer.index_word[w] for w in  result_tokens if w != 0 ])\n",
    "        \n",
    "        attention_per_candidate.append(attention)\n",
    "        result_text_per_candidate.append(result_text)\n",
    "        input_text_per_candidate.append(input_text)\n",
    "        output_text_per_candidate.append(output_text)\n",
    "\n",
    "    candidate_score, best_score_candidate, best_candidate_index = Minimum_Bayes_Risk(result_text_per_candidate)\n",
    "\n",
    "    attention_stack = attention_per_candidate[best_candidate_index]\n",
    "    result_text = result_text_per_candidate[best_candidate_index]\n",
    "    input_text = input_text_per_candidate[best_candidate_index]\n",
    "    output_text = output_text_per_candidate[best_candidate_index]\n",
    "    \n",
    "    return {'generated_text': result_text, 'attention': attention_stack, \"input_text\" : input_text, \"output_text\" : output_text, \"candidates\": result_text_per_candidate}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_AND_SAVE:\n",
    "    input_sent = \"[BOS] eu quero \"\n",
    "    inp_seq = np.array([tokenizer.word_index[x] for x in input_sent.split()])\n",
    "    print(\"inp_seq -->\",inp_seq)\n",
    "\n",
    "    result = generate_sentence(model_causal1, input_tokens = tf.expand_dims(inp_seq,0),max_length= 5)\n",
    "    print(\"RESULT : \",result[\"generated_text\"])\n",
    "    plot_weights(result, stop_time = 3, loop=True)\n",
    "\n",
    "    result = generate_sentence(model_causal1, input_tokens = tf.expand_dims(inp_seq,0),max_length= 5, loop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_AND_SAVE:\n",
    "    model.use_tf_function = False\n",
    "    for i, o in dataset.take(2):\n",
    "        print(model.train_step([i, o]))\n",
    "        print(model.test_step([i, o]))\n",
    "    print()\n",
    "\n",
    "    model_causal1.use_tf_function = False\n",
    "    for i, o in dataset.take(2):\n",
    "        print(model_causal1.train_step([i, o]))\n",
    "        print(model_causal1.test_step([i, o]))\n",
    "    print()\n",
    "\n",
    "    print(\"\\n---------------------\\n\")\n",
    "\n",
    "    model.use_tf_function = True\n",
    "    for i, o in dataset.take(2):\n",
    "        print(model.train_step([i, o]))\n",
    "        print(model.test_step([i, o]))\n",
    "    print()\n",
    "\n",
    "    model_causal1.use_tf_function = True\n",
    "    for i, o in dataset.take(2):\n",
    "        print(model_causal1.train_step([i, o]))\n",
    "        print(model_causal1.test_step([i, o]))\n",
    "\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_AND_SAVE:\n",
    "    \n",
    "    history = model.fit(dataset, epochs=epochs, shuffle= True,#6000\n",
    "                        steps_per_epoch = train_size, callbacks=[PlotLossesCallback()], validation_data=val_dataset, validation_steps=10)\n",
    "    model.save_weights(\"./checkpoints/model_ckpt\")\n",
    "\n",
    "    with open('./checkpoints/history.pickle', 'wb') as handle:\n",
    "        pickle.dump(history.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "else :\n",
    "    print(\"Loading saved data ...\")\n",
    "    model = ModelClass(vocab_len, model_type=1, causal_mask_enabled= 0, blstm_enable=blstm_enable, use_attention=False)\n",
    "    model.load_weights(\"./checkpoints/model_ckpt\")\n",
    "\n",
    "    with open('./checkpoints/history.pickle', 'rb') as handle:\n",
    "        history = pickle.load(handle)\n",
    "        plt.figure()\n",
    "        plt.plot(history['sparse_categorical_accuracy'])\n",
    "        plt.plot(history['val_sparse_categorical_accuracy'])\n",
    "        plt.title('Model accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(history['loss'])\n",
    "        plt.plot(history['val_loss'])\n",
    "        plt.title('Model training loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(history['perplexity'])\n",
    "        plt.plot(history['val_perplexity'])\n",
    "        plt.title('Model perplexity ')\n",
    "        plt.ylabel('Perplexity')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_AND_SAVE:\n",
    "    \n",
    "    history_causal1 = model_causal1.fit(dataset, epochs=epochs+100, shuffle= True, #6000\n",
    "                        steps_per_epoch = train_size, callbacks=[PlotLossesCallback()], validation_data=val_dataset, validation_steps=10)\n",
    "    model_causal1.save_weights(\"./checkpoints/model_causal1_ckpt\")\n",
    "\n",
    "    with open('./checkpoints/history_causal1.pickle', 'wb') as handle:\n",
    "        pickle.dump(history_causal1.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "else :\n",
    "    print(\"Loading saved data ...\")\n",
    "    model_causal1 = ModelClass(vocab_len, model_type=1, causal_mask_enabled= 1, blstm_enable=blstm_enable, loop= False)\n",
    "\n",
    "    model_causal1.load_weights(\"./checkpoints/model_causal1_ckpt\")\n",
    "\n",
    "    with open('./checkpoints/history_causal1.pickle', 'rb') as handle:\n",
    "        history_causal1 = pickle.load(handle)\n",
    "        plt.figure()\n",
    "        plt.plot(history_causal1['sparse_categorical_accuracy'])\n",
    "        plt.plot(history_causal1['val_sparse_categorical_accuracy'])\n",
    "        plt.title('Model accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(history_causal1['loss'])\n",
    "        plt.plot(history_causal1['val_loss'])\n",
    "        plt.title('Model training loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(history_causal1['perplexity'])\n",
    "        plt.plot(history_causal1['val_perplexity'])\n",
    "        plt.title('Model perplexity ')\n",
    "        plt.ylabel('Perplexity')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, padding_mask= None, look_ahead_mask= None):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead)\n",
    "  but it must be broadcastable for addition.\n",
    "\n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable\n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "  if padding_mask is not None:\n",
    "    scaled_attention_logits += (padding_mask * -1e9)\n",
    "\n",
    "  if look_ahead_mask is not None:\n",
    "    scaled_attention_logits += (look_ahead_mask * -1e9)\n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights\n",
    "\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self,*, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.wq = Dense(d_model)\n",
    "    self.wk = Dense(d_model)\n",
    "    self.wv = Dense(d_model)\n",
    "\n",
    "    self.dense = Dense(d_model)\n",
    "\n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, v, k, q, padding_mask, look_ahead_mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "\n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention( q, k, v, padding_mask, look_ahead_mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention,(batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "n_vectors = 150\n",
    "d_model = 256\n",
    "\n",
    "query = tf.random.uniform((batch_size, n_vectors, d_model), dtype=tf.float32)\n",
    "key = tf.random.uniform((batch_size, n_vectors, d_model), dtype=tf.float32)\n",
    "value = tf.random.uniform((batch_size, n_vectors, d_model), dtype=tf.float32)\n",
    "query.shape, key.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dot product attention\n",
    "x,_ = scaled_dot_product_attention(query, key, key, None)\n",
    "print(f\"Output from dot product attention: {x.shape}\")\n",
    "\n",
    "att = SelfMaskedDotProductAttention()\n",
    "x1, _ = att(query, key,key)\n",
    "#x = tf.concat(x, -1)\n",
    "print(f\"Output from dot product attention: {x1.shape}\")\n",
    "np.where((x-x1) > 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multi-head attention\n",
    "mh_layer = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "x, _ = mh_layer(query, key, value, None, None)\n",
    "print(f\"Output from multi-head attention: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff): #Point wise feed forward network consists of two fully-connected layers with a ReLU activation in between.\n",
    "  return tf.keras.Sequential([\n",
    "      Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])\n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self,*, d_model, num_heads, dff, rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    self.dropout1 = Dropout(rate)\n",
    "    self.dropout2 = Dropout(rate)\n",
    "\n",
    "  def call(self, x, training, padding_mask= None, look_ahead_mask = None):\n",
    "    attn_output, attn_weights_block1  = self.mha(x, x, x, padding_mask, look_ahead_mask)  # (batch_size, input_seq_len, d_model)\n",
    "    attn_output = self.dropout1(attn_output, training=training)\n",
    "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    return out2, attn_weights_block1\n",
    "\n",
    "\n",
    "sample_encoder_layer = DecoderLayer(d_model=512, num_heads=8, dff=batch_size)\n",
    "sample_encoder_layer_output,attn_weights_block1 = sample_encoder_layer(tf.random.uniform((64, 43, 512)), False, None)\n",
    "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "  pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "n, d = 2048, vocab_len\n",
    "pos_encoding = positional_encoding(n, d)\n",
    "print(pos_encoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self,*, num_layers, d_model, embeddings_matrix, window_size, num_heads, dff, vocab_len,\n",
    "               rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.emb_layer = Embedding(input_dim=vocab_len, input_length = window_size, output_dim=d_model, weights=[embeddings_matrix] ,name =\"emb_layer2\", trainable=False)#, mask_zero = True\n",
    "    self.pos_encoding = positional_encoding(window_size, self.d_model)\n",
    "\n",
    "    self.dec_layers = [DecoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate) for _ in range(num_layers)]\n",
    "\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "  def call(self, x, training, padding_mask =None, look_ahead_mask= None):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    attention_weights = {}\n",
    "\n",
    "    x = self.emb_layer(x)  # (batch_size, input_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "    x = self.dropout(x, training=training)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x, block1 = self.dec_layers[i](x, training, padding_mask, look_ahead_mask)\n",
    "      attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "\n",
    "\n",
    "    return x ,attention_weights # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "sample_decoder = Decoder(num_layers=2, d_model=emb_dim, embeddings_matrix = embeddings_matrix, window_size= window_size, num_heads=10,\n",
    "                         dff=2048, vocab_len=vocab_len)\n",
    "\n",
    "for i, o in dataset.take(1):\n",
    "  print(i.shape, o.shape)\n",
    "  output, attn = sample_decoder(i, training=True, padding_mask =None, look_ahead_mask= None)\n",
    "  \n",
    "output.shape, attn['decoder_layer2_block1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(tf.keras.losses.Loss):\n",
    "  def __init__(self):\n",
    "    self.name = 'masked_loss_transformer_custom'\n",
    "    self.loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "  def __call__(self, real, pred):\n",
    "      \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = self.loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "  real = tf.cast(real, dtype=tf.int32)\n",
    "  accuracies = tf.equal(real, tf.cast(tf.argmax(pred, axis=2), dtype=tf.int32))\n",
    "\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  accuracies = tf.math.logical_and(mask, accuracies)\n",
    "\n",
    "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "  mask = tf.cast(mask, dtype=tf.float32)\n",
    "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)\n",
    "  \n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.xlabel('Train Step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,*, num_layers, d_model,embeddings_matrix,window_size, num_heads, dff,  vocab_len,rate=0.1, use_tf_function= False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.decoder = Decoder(num_layers=num_layers, d_model=d_model, embeddings_matrix = embeddings_matrix,\n",
    "                            window_size= window_size, num_heads=num_heads, dff=dff, vocab_len=vocab_len, rate=rate)\n",
    "\n",
    "        self.final_layer = Dense(vocab_len)\n",
    "        self.use_tf_function = use_tf_function\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        # Keras models prefer if you pass all your inputs in the first argument\n",
    "        inp = inputs\n",
    "\n",
    "        padding_mask, look_ahead_mask = self.create_masks(inp)\n",
    "        \n",
    "        # dec_output.shape == (batch_size, inp_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(inp, training, padding_mask, look_ahead_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, vocab_len)\n",
    "\n",
    "        return final_output, attention_weights\n",
    "\n",
    "    def create_masks(self, inp):\n",
    "        # Encoder padding mask (Used in the 2nd attention block in the decoder too.)\n",
    "        \n",
    "        padding_mask = self.create_padding_mask(inp)\n",
    "\n",
    "        # Used in the 1st attention block in the decoder.\n",
    "        # It is used to pad and mask future tokens in the input received by\n",
    "        # the decoder.\n",
    "        look_ahead_mask = self.create_look_ahead_mask(tf.shape(inp)[1])\n",
    "\n",
    "        return padding_mask, look_ahead_mask\n",
    "\n",
    "    def create_padding_mask(self,seq):\n",
    "        seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "        # add extra dimensions to add the padding\n",
    "        # to the attention logits.\n",
    "        return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "    def create_look_ahead_mask(self, size):\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "        return mask  # (seq_len, seq_len)\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        if self.use_tf_function:\n",
    "            return self._tf_train_step(inputs)\n",
    "        else:\n",
    "            return self._train_step(inputs)\n",
    "\n",
    "    @tf.function(input_signature=[[tf.TensorSpec(dtype=tf.int32, shape=[None, window_size]),\n",
    "                                tf.TensorSpec(dtype=tf.int32, shape=[None, window_size])]])\n",
    "                                \n",
    "    def _tf_train_step(self, inputs):\n",
    "        return self._train_step(inputs)\n",
    "\n",
    "\n",
    "\n",
    "    def test_step(self, inputs):\n",
    "        if self.use_tf_function:\n",
    "            return self._tf_test_step(inputs)\n",
    "        else:\n",
    "            return self._test_step(inputs)\n",
    "\n",
    "\n",
    "    @tf.function(input_signature=[[tf.TensorSpec(dtype=tf.int32, shape=[None, window_size]),\n",
    "                                tf.TensorSpec(dtype=tf.int32, shape=[None, window_size])]])\n",
    "    def _tf_test_step(self, inputs):\n",
    "        return self._test_step(inputs)\n",
    "\n",
    "\n",
    "    def _train_step(self, inputs):\n",
    "        inp, tar = inputs\n",
    "        target_mask = tar != 0     \n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            acc = []\n",
    "            loss = tf.constant(0.0)\n",
    "\n",
    "            predictions, _ = self.call(inp,training = True)\n",
    "            loss += self.loss(tar, predictions)\n",
    "            self.compiled_metrics.update_state(tar, predictions)\n",
    "\n",
    "            acc.append(self.metrics[0].result())\n",
    "\n",
    "            average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))  \n",
    "            average_acc  = tf.reduce_mean(tf.convert_to_tensor(acc) )\n",
    "            average_perplexity  = tf.reduce_mean(tf.convert_to_tensor(perplexity(tar, predictions) ))\n",
    "            average_cc  = tf.reduce_mean(tf.convert_to_tensor(accuracy_function(tf.cast(tar, tf.int32), predictions) ))\n",
    "\n",
    "        variables = self.trainable_variables \n",
    "        gradients = tape.gradient(average_loss, variables)\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "        m = {m.name: average_acc  for m in self.metrics} #.update({'loss': batch_loss})m.result() \n",
    "        m[\"loss\"] = average_loss\n",
    "        m[\"perplexity\"] = average_perplexity\n",
    "        m[\"custom_acc\"] = average_cc\n",
    "\n",
    "        return m\n",
    "\n",
    "    def _test_step(self, inputs):\n",
    "        inp, tar = inputs\n",
    "        target_mask = tar != 0     \n",
    "\n",
    "        acc = []\n",
    "        loss = tf.constant(0.0)\n",
    "\n",
    "        predictions, _ = self.call(inp,training = True)\n",
    "        loss += self.loss(tar, predictions)\n",
    "        self.compiled_metrics.update_state(tar, predictions)\n",
    "\n",
    "        acc.append(self.metrics[0].result())\n",
    "\n",
    "        average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))  \n",
    "        average_acc  = tf.reduce_mean(tf.convert_to_tensor(acc) )\n",
    "        average_perplexity  = tf.reduce_mean(tf.convert_to_tensor(perplexity(tar, predictions) ))\n",
    "        average_cc  = tf.reduce_mean(tf.convert_to_tensor(accuracy_function(tf.cast(tar, tf.int32), predictions) ))\n",
    "\n",
    "        m = {m.name: average_acc  for m in self.metrics} #.update({'loss': batch_loss})m.result() \n",
    "        m[\"loss\"] = average_loss\n",
    "        m[\"perplexity\"] = average_perplexity\n",
    "        m[\"custom_acc\"] = average_cc\n",
    "\n",
    "        return m\n",
    "\n",
    "\n",
    "transformer = Transformer(\n",
    "    num_layers=4,\n",
    "    d_model=emb_dim,\n",
    "    embeddings_matrix = embeddings_matrix,\n",
    "    window_size = window_size,\n",
    "    num_heads=10,\n",
    "    dff=256,\n",
    "    vocab_len=vocab_len,\n",
    "    rate=0.1)\n",
    "\n",
    "for i, o in dataset.take(1):\n",
    "    print(i.shape, o.shape)\n",
    "    fn_out, _ = transformer(i, training=False)\n",
    "\n",
    "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "transformer.compile(loss=CustomLoss(), optimizer=optimizer, metrics=[tf.metrics.SparseCategoricalAccuracy()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_sentence_for_transformer(transformer_inst, input_tokens, max_length=10, temperature=1, number_of_candidates = 20, loop= False):\n",
    "    \n",
    "    attention_per_candidate = []\n",
    "    result_text_per_candidate = []\n",
    "    input_text_per_candidate = []\n",
    "    output_text_per_candidate = []\n",
    "\n",
    "    for c in range(number_of_candidates):\n",
    "\n",
    "        input_mask = None\n",
    "        result_tokens = []\n",
    "        attention = []\n",
    "\n",
    "        x = input_tokens\n",
    "        input_text= []\n",
    "        output_text = []\n",
    "        result_tokens = [w.numpy() for w in  x[0] if w != 0 ]\n",
    "        for k in range(max_length):\n",
    "\n",
    "            input_text.append(' '.join([tokenizer.index_word[w.numpy()] for w in  x[0] if w != 0 ]))\n",
    "            x = pad_sequences(x, padding = \"post\", maxlen = window_size)#window_size\n",
    "\n",
    "            input_mask = x!= 0\n",
    "            t = np.count_nonzero(input_mask)\n",
    "\n",
    "            predictions, attention_weights  =transformer_inst(x, training=False)\n",
    "\n",
    "            if k > 0 and attention_weights is not None:\n",
    "                attention.append(attention_weights)\n",
    "\n",
    "            predicted_logits  = predictions[:,t-1,:]/temperature\n",
    "            predicted_logits = tf.where(token_mask, -np.inf, predicted_logits)\n",
    "            predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "            new_tokens = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "            if loop:\n",
    "                x = new_tokens.numpy()\n",
    "            else : \n",
    "\n",
    "                x = list(x[0])\n",
    "                x = [i for i in x if i != 0]\n",
    "                x.append(new_tokens.numpy()[0])\n",
    "                \n",
    "\n",
    "            output_text.append(' '.join([tokenizer.index_word[w] for w in  x if w != 0 ][1:]))\n",
    "            x = tf.reshape(tf.convert_to_tensor(x), (len(input_tokens),len(x)))\n",
    "            result_tokens.append(new_tokens.numpy()[0])\n",
    "                \n",
    "            if tokenizer.word_index['[EOS]'] == new_tokens.numpy()[0]:\n",
    "                break\n",
    "\n",
    "        result_text =   ' '.join([tokenizer.index_word[w] for w in  result_tokens if w != 0 ])\n",
    "        \n",
    "        attention_per_candidate.append(attention)\n",
    "        result_text_per_candidate.append(result_text)\n",
    "        input_text_per_candidate.append(input_text)\n",
    "        output_text_per_candidate.append(output_text)\n",
    "        \n",
    "\n",
    "\n",
    "    candidate_score, best_score_candidate, best_candidate_index = Minimum_Bayes_Risk(result_text_per_candidate)\n",
    "\n",
    "    attention_stack = attention_per_candidate[best_candidate_index]\n",
    "    result_text = result_text_per_candidate[best_candidate_index]\n",
    "    input_text = input_text_per_candidate[best_candidate_index]\n",
    "    output_text = output_text_per_candidate[best_candidate_index]\n",
    "    \n",
    "    return {'generated_text': result_text, 'attention': attention_stack, \"input_text\" : input_text, \"output_text\" : output_text, \"candidates\": result_text_per_candidate}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_AND_SAVE_TRANFORMER:\n",
    "    transformer.use_tf_function = False\n",
    "    for i, o in dataset.take(2):\n",
    "        print(transformer.train_step([i, o]))\n",
    "        print(transformer.test_step([i, o]))\n",
    "    print()\n",
    "\n",
    "\n",
    "    transformer.use_tf_function = True\n",
    "    for i, o in dataset.take(2):\n",
    "        print(transformer.train_step([i, o]))\n",
    "        print(transformer.test_step([i, o]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_AND_SAVE_TRANFORMER:\n",
    "    history_transformer = transformer.fit(dataset, epochs=epochs+50, shuffle= True,#6000\n",
    "                            steps_per_epoch = train_size, callbacks=[PlotLossesCallback()], validation_data=val_dataset, validation_steps=10)\n",
    "    transformer.save_weights(\"./checkpoints/transformer_ckpt\")\n",
    "\n",
    "    with open('./checkpoints/history_transformer.pickle', 'wb') as handle:\n",
    "        pickle.dump(history_transformer.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    print(\"Loading saved data ...\")\n",
    "    transformer = Transformer( num_layers=4, d_model=emb_dim, embeddings_matrix = embeddings_matrix, window_size = window_size, num_heads=10, dff=256, vocab_len=vocab_len, rate=0.1)\n",
    "    transformer.load_weights(\"./checkpoints/transformer_ckpt\")\n",
    "\n",
    "    with open('./checkpoints/history_transformer.pickle', 'rb') as handle:\n",
    "        history_transformer = pickle.load(handle)\n",
    "        plt.figure()\n",
    "        plt.plot(history_transformer['sparse_categorical_accuracy'])\n",
    "        plt.plot(history_transformer['val_sparse_categorical_accuracy'])\n",
    "        plt.title('Model accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(history_transformer['loss'])\n",
    "        plt.plot(history_transformer['val_loss'])\n",
    "        plt.title('Model training loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(history_transformer['perplexity'])\n",
    "        plt.plot(history_transformer['val_perplexity'])\n",
    "        plt.title('Model perplexity ')\n",
    "        plt.ylabel('Perplexity')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(history_transformer['custom_acc'])\n",
    "        plt.plot(history_transformer['val_custom_acc'])\n",
    "        plt.title('Model Custom acc')\n",
    "        plt.ylabel('Custom acc')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, o in val_dataset.take(1):\n",
    "    print(i[0])\n",
    "    print(' '.join([tokenizer.index_word[x.numpy()] for x in i[0] if x.numpy() != 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sent = \"[BOS] no inverno , as familias necessitadas\"\n",
    "\n",
    "#tokenizer.texts_to_sequences(\"[BOS] o rato roeu a roupa do rei de roma \")\n",
    "#inp_seq = np.array([tokenizer.word_index[x] for x in input_sent.split()])\n",
    "\n",
    "inp_seq = np.array([ w[0] for w in  tokenizer.texts_to_sequences(input_sent.split(\" \")) if len(w)>0])\n",
    "temp = 0.9\n",
    "candidates_n = 50\n",
    "max_len = 30\n",
    "\n",
    "inp_seq#, tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE_TEXT:\n",
    "    result1 = generate_sentence(model, input_tokens = tf.expand_dims(inp_seq,0), max_length= max_len, temperature= temp , number_of_candidates=candidates_n, no_state = False)\n",
    "    result2 = generate_sentence(model, input_tokens = tf.expand_dims(inp_seq,0), max_length= max_len, temperature= temp , number_of_candidates=candidates_n , loop= True)\n",
    "\n",
    "    print(\"RESULT WITH STATE : \",result1[\"generated_text\"],\"\\n\")\n",
    "    print(\"RESULT WITH STATE AND LOOP : \",result2[\"generated_text\"],\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention with causal mask on upper triangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE_TEXT:\n",
    "    result = generate_sentence(model_causal1, input_tokens = tf.expand_dims(inp_seq,0), max_length= max_len, temperature= temp , number_of_candidates=candidates_n, no_state = True)\n",
    "    result2 = generate_sentence(model_causal1, input_tokens = tf.expand_dims(inp_seq,0), max_length= max_len, temperature= temp , number_of_candidates=candidates_n , loop= True)\n",
    "\n",
    "    plot_weights(result, stop_time = 3, loop = False)\n",
    "    print(\"RESULT NO STATE : \",result[\"generated_text\"],\"\\n\")\n",
    "    print(\"RESULT WITH STATE AND LOOP : \",result2[\"generated_text\"],\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer with causal mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE_TEXT:\n",
    "    result = generate_sentence_for_transformer(transformer, input_tokens = tf.expand_dims(inp_seq,0), max_length= max_len, temperature= temp , number_of_candidates=candidates_n, )\n",
    "    result1 = generate_sentence_for_transformer(transformer, input_tokens = tf.expand_dims(inp_seq,0), max_length= max_len, temperature= temp , number_of_candidates=candidates_n , loop= True)\n",
    "    print(\"RESULT  : \",result[\"generated_text\"],\"\\n\")\n",
    "    print(\"RESULT WITH  LOOP : \",result1[\"generated_text\"],\"\\n\")\n",
    "\n",
    "    t = -1\n",
    "    plt.figure()\n",
    "    plot_attention(result['attention'][t][\"decoder_layer4_block1\"][0][-1],  result[\"output_text\"][t],  result[\"output_text\"][t])#input_text\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "TextGenerator-In progress.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "dffee67a034c262da22ecd866efe5fa1174a6876e1e96ed512a9acb5833498b8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('tf_cpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
