{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8048,
     "status": "ok",
     "timestamp": 1595880019461,
     "user": {
      "displayName": "yaniel barbosa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjeEwoPVbOEW9Pwf_Z5Q-LyxKX0GvQrJ-S-pfKh=s64",
      "userId": "13057870575473273651"
     },
     "user_tz": -60
    },
    "id": "eR2oejU7JhSE",
    "outputId": "b803b1cb-a567-4b08-84de-09862d595617"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-26 08:12:21.149022: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-26 08:12:21.187271: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-26 08:12:21.187301: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-26 08:12:21.188346: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-26 08:12:21.194863: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-26 08:12:21.986432: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/n/anaconda3/envs/AI_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Using TensorFlow backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-26 08:12:23.406332: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-26 08:12:23.453029: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-26 08:12:23.460410: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re #regex\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Dense, LayerNormalization, Dropout,  LSTM, Dense, Embedding, Bidirectional,BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import io\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "import random\n",
    "\n",
    "from rouge import Rouge #https://towardsdatascience.com/introduction-to-text-summarization-with-rouge-scores-84140c64b471\n",
    "\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "import keras_tuner as kt\n",
    "from transformers import GPT2TokenizerFast, GPT2Config, GPT2Model, TFAutoModelForCausalLM, AutoTokenizer, TFGPT2Model\n",
    "\n",
    "print(\"Num GPUs Available: \", (tf.config.experimental.list_physical_devices()))\n",
    "import keras_nlp\n",
    "\n",
    "ROUGE = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8007,
     "status": "ok",
     "timestamp": 1595880019463,
     "user": {
      "displayName": "yaniel barbosa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjeEwoPVbOEW9Pwf_Z5Q-LyxKX0GvQrJ-S-pfKh=s64",
      "userId": "13057870575473273651"
     },
     "user_tz": -60
    },
    "id": "CBl11TrADX6-"
   },
   "outputs": [],
   "source": [
    "buffer_size = 10000\n",
    "batch_size= 16\n",
    "window_size= 50 \n",
    "epochs= 150\n",
    "embedding_dim = 200\n",
    "\n",
    "TRAIN_AND_SAVE = True\n",
    "TRAIN_AND_SAVE_TRANFORMER = True\n",
    "\n",
    "TRAIN_TEXT= True\n",
    "SEED=42\n",
    "TRAIN_TOKEN = True\n",
    "TUNNING = True\n",
    "GENERATE_TEXT= True\n",
    "\n",
    "\n",
    "translationTable = str.maketrans(\"áéíóúàèìòùâêîôûãõç\", \"aeiouaeiouaeiouaoc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow.data as tf_data\n",
    "# import tensorflow.strings as tf_strings\n",
    "\n",
    "# keras.utils.get_file(\n",
    "#     origin=\"https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip\",\n",
    "#     extract=True,\n",
    "# )\n",
    "# dir = os.path.expanduser(\"~/.keras/datasets/simplebooks/\")\n",
    "\n",
    "# # Load simplebooks-92 train set and filter out short lines.\n",
    "# raw_train_ds = (\n",
    "#     tf_data.TextLineDataset(dir + \"simplebooks-92-raw/train.txt\")\n",
    "#     .filter(lambda x: tf_strings.length(x) > 512)\n",
    "#     .batch(batch_size)\n",
    "#     .shuffle(buffer_size=256)\n",
    "# )\n",
    "# for x in raw_train_ds.take(1):\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_CHECKPOINTS_DIR = './checkpoints'\n",
    "tfrecord_filename = \"train_tmp.tfrecord\"\n",
    "train_tmp_record_path = f'{DATA_CHECKPOINTS_DIR}/{tfrecord_filename}'\n",
    "\n",
    "!mkdir -p {DATA_CHECKPOINTS_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 1266856\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a casa que vieram habitar em lisboa , no outon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apesar deste fresco nome de vivenda campestre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>longos anos o ramalhete permanecera desabitad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>em monsenhor bucarini , nuncio de sao santida...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>este inutil pardieiro como lhe chamava vilaca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10205</th>\n",
       "      <td>ega , ao lado , ajuntava , ofegante , atirand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10206</th>\n",
       "      <td>a lanterna vermelha do americano , ao longe ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10207</th>\n",
       "      <td>e foi em carlos e em joao da ega uma esperanc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10208</th>\n",
       "      <td>entao , para apanhar o americano , os dois am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10209</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10210 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentences\n",
       "0      a casa que vieram habitar em lisboa , no outon...\n",
       "1       apesar deste fresco nome de vivenda campestre...\n",
       "2       longos anos o ramalhete permanecera desabitad...\n",
       "3       em monsenhor bucarini , nuncio de sao santida...\n",
       "4       este inutil pardieiro como lhe chamava vilaca...\n",
       "...                                                  ...\n",
       "10205   ega , ao lado , ajuntava , ofegante , atirand...\n",
       "10206   a lanterna vermelha do americano , ao longe ,...\n",
       "10207   e foi em carlos e em joao da ega uma esperanc...\n",
       "10208   entao , para apanhar o americano , os dois am...\n",
       "10209                                                   \n",
       "\n",
       "[10210 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_data(path_to_file ='./nietzsche.txt'):\n",
    "    path_to_file = \"./eca_de_queiros_os_maias.txt\"\n",
    "    \n",
    "    with io.open(path_to_file, encoding='utf-8') as f:\n",
    "        text = f.read().lower()\n",
    "\n",
    "    print('corpus length:', len(text))\n",
    "    text = text.translate(translationTable)\n",
    "\n",
    "    string_check= re.compile('[^a-zA-Z.?!,:\\'<>]')\n",
    "\n",
    "    text = re.sub(string_check, ' ', (text)\n",
    "                        .replace(\"-\",\" \")\n",
    "                        .replace(\"´\",\"'\")\n",
    "                        .replace(\"`\",\"'\")\n",
    "                        .replace(\",\",\" , \")\n",
    "                        .replace(\"s.\",\" sao \")\n",
    "                        .replace(\"d.\",\"don\")\n",
    "                        .replace(\"v.\",\"v\")\n",
    "                        .replace(\"sr.\", \"senhor\")\n",
    "                        .replace(\"sra.\", \"senhora\")\n",
    "                        .replace(\"exmo.\", \"exmo\")\n",
    "                        .replace(\"exma.\", \"exma\")\n",
    "                        .replace(\"x.\", \"x\")\n",
    "    )\n",
    "\n",
    "    text = re.sub(' +', ' ',text)\n",
    "    text = (re.sub('\\.+', \" . \\n\", text).replace(\"<br />\",\" \"))\n",
    "\n",
    "    lines_list = list()\n",
    "    lines_list = text.split(\"\\n\")\n",
    "\n",
    "    cols = ['sentences']\n",
    "    df_tmp= pd.DataFrame(columns=cols)\n",
    "    df_tmp[\"sentences\"] = lines_list\n",
    "\n",
    "    return df_tmp\n",
    "complete_text= prepare_data()\n",
    "complete_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 1266856\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a casa que vieram habitar em lisboa , no outon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apesar deste fresco nome de vivenda campestre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>longos anos o ramalhete permanecera desabitad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>em monsenhor bucarini , nuncio de sao santida...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>este inutil pardieiro como lhe chamava vilaca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10205</th>\n",
       "      <td>ega , ao lado , ajuntava , ofegante , atirand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10206</th>\n",
       "      <td>a lanterna vermelha do americano , ao longe ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10207</th>\n",
       "      <td>e foi em carlos e em joao da ega uma esperanc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10208</th>\n",
       "      <td>entao , para apanhar o americano , os dois am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10209</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10210 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentences\n",
       "0      a casa que vieram habitar em lisboa , no outon...\n",
       "1       apesar deste fresco nome de vivenda campestre...\n",
       "2       longos anos o ramalhete permanecera desabitad...\n",
       "3       em monsenhor bucarini , nuncio de sao santida...\n",
       "4       este inutil pardieiro como lhe chamava vilaca...\n",
       "...                                                  ...\n",
       "10205   ega , ao lado , ajuntava , ofegante , atirand...\n",
       "10206   a lanterna vermelha do americano , ao longe ,...\n",
       "10207   e foi em carlos e em joao da ega uma esperanc...\n",
       "10208   entao , para apanhar o americano , os dois am...\n",
       "10209                                                   \n",
       "\n",
       "[10210 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "22255"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'gasta',\n",
       " 'procurar',\n",
       " 'seculares',\n",
       " 'anjinho',\n",
       " 'colera',\n",
       " 'farejando',\n",
       " 'desejei',\n",
       " 'diabo?',\n",
       " 'abismar',\n",
       " 'alegraram',\n",
       " 'descobre',\n",
       " 'leito',\n",
       " 'camara!',\n",
       " 'explicaram',\n",
       " 'ornavam',\n",
       " 'falada!',\n",
       " 'sandalo',\n",
       " 'cessou',\n",
       " 'gourmet',\n",
       " 'efusao',\n",
       " 'general',\n",
       " 'inspira',\n",
       " 'quente!',\n",
       " 'recebido',\n",
       " 'timidez',\n",
       " 'recapitular',\n",
       " 'parceiros',\n",
       " 'dia:',\n",
       " 'reclamacoes',\n",
       " 'irrisoes',\n",
       " 'esguias',\n",
       " 'traga',\n",
       " 'vivendo',\n",
       " 'inaproveitado',\n",
       " 'indefinida',\n",
       " 'craveirote',\n",
       " 'cabeca:',\n",
       " 'estragada?',\n",
       " 'proporcoes',\n",
       " 'ruga!',\n",
       " 'palestrar',\n",
       " 'cronica',\n",
       " 'conferiu',\n",
       " 'selar',\n",
       " 'bons?',\n",
       " 'divagacoes:',\n",
       " 'iam',\n",
       " 'veneracao',\n",
       " 'romanos!',\n",
       " 'agasalhado',\n",
       " 'cobarde!',\n",
       " 'substituido',\n",
       " 'cavalinhos',\n",
       " 'revistas',\n",
       " 'achegou',\n",
       " 'regata',\n",
       " 'pronunciou',\n",
       " 'pescoco',\n",
       " 'prato',\n",
       " 'casais',\n",
       " 'servir!',\n",
       " 'cachenez',\n",
       " 'pagando',\n",
       " 'empresa',\n",
       " 'social?',\n",
       " 'gemia',\n",
       " 'montada',\n",
       " 'nuvensinhas',\n",
       " 'argentino:',\n",
       " 'nova!',\n",
       " 'senhores?',\n",
       " 'louros',\n",
       " 'solteiroe',\n",
       " 'viam',\n",
       " 'fama',\n",
       " 'faias',\n",
       " 'guela',\n",
       " 'ideara',\n",
       " 'quarta',\n",
       " 'vintista',\n",
       " 'desforra',\n",
       " 'triunfou',\n",
       " 'chambre?',\n",
       " 'dedos!',\n",
       " 'gargalhada!',\n",
       " 'pregas',\n",
       " 'ensaboar',\n",
       " 'levar',\n",
       " 'papar',\n",
       " 'ocuparia',\n",
       " 'ressequidas',\n",
       " 'pegajoso',\n",
       " 'fim',\n",
       " 'tinir',\n",
       " 'risada',\n",
       " 'exalando',\n",
       " 'pernas!',\n",
       " 'escuros!',\n",
       " 'contemplara',\n",
       " 'resto',\n",
       " 'globos',\n",
       " 'nosso',\n",
       " 'conservavam',\n",
       " 'linho',\n",
       " 'passear',\n",
       " 'beaconsfield',\n",
       " 'cuco',\n",
       " 'instituicoe',\n",
       " 'variedades',\n",
       " 'trigueiras',\n",
       " 'ceus!',\n",
       " 'chofre',\n",
       " 'assombrado',\n",
       " 'disfarcadamente',\n",
       " 'desapercebido',\n",
       " 'odiosa',\n",
       " 'apontava',\n",
       " 'amortalhados',\n",
       " 'oiro',\n",
       " 'estas:',\n",
       " 'embaciavam',\n",
       " 'depusera',\n",
       " 'raiada',\n",
       " 'esforcando',\n",
       " 'raparigas',\n",
       " 'moviam',\n",
       " 'seco',\n",
       " 'retoques',\n",
       " 'desanimado',\n",
       " 'armando',\n",
       " 'examinando',\n",
       " 'bastardo',\n",
       " 'racial',\n",
       " 'mudaram',\n",
       " 'canonico',\n",
       " 'levo',\n",
       " 'bruma',\n",
       " 'garrafa?',\n",
       " 'recolhido',\n",
       " 'devagarinho',\n",
       " 'comunista',\n",
       " 'tropel',\n",
       " 'patacos',\n",
       " 'carregando',\n",
       " 'piada',\n",
       " 'graca',\n",
       " 'reentrar',\n",
       " 'fitando',\n",
       " 'agudos',\n",
       " 'faces',\n",
       " 'tijolo',\n",
       " 'durabilidade',\n",
       " 'carnaval',\n",
       " 'saco',\n",
       " 'decidira:',\n",
       " 'merecer!',\n",
       " 'direitos',\n",
       " 'relia',\n",
       " 'nenhum!',\n",
       " 'assustaste!',\n",
       " 'ruga',\n",
       " 'dona',\n",
       " 'engrandecer',\n",
       " 'club!',\n",
       " 'berro',\n",
       " 'seguiriam',\n",
       " 'haja',\n",
       " 'impudente!',\n",
       " 'disfarce',\n",
       " 'enternecera',\n",
       " 'renque',\n",
       " 'fechando',\n",
       " 'randon',\n",
       " 'trouvile',\n",
       " 'mundano',\n",
       " 'endereco',\n",
       " 'blanc',\n",
       " 'sousa',\n",
       " 'titulos',\n",
       " 'espolinhar',\n",
       " 'tivemos',\n",
       " 'cavalo',\n",
       " 'seguirem',\n",
       " 'vibravam',\n",
       " 'empenhara',\n",
       " 'coca!',\n",
       " 'rabbino',\n",
       " 'indiscreto',\n",
       " 'tumbuctu',\n",
       " 'falto',\n",
       " 'orenoque',\n",
       " 'lume:',\n",
       " 'misteriosa',\n",
       " 'macada',\n",
       " 'estroinice',\n",
       " 'telegrama',\n",
       " 'desfolhada',\n",
       " 'repelao',\n",
       " 'exmo',\n",
       " 'cavalarica',\n",
       " 'apurado',\n",
       " 'filhos',\n",
       " 'possuiam',\n",
       " 'fotografias',\n",
       " 'nossas',\n",
       " 'joga',\n",
       " 'rabino',\n",
       " 'generais?',\n",
       " 'complemento',\n",
       " 'criatura!',\n",
       " 'enxotar',\n",
       " 'stores',\n",
       " 'quadrilhas?',\n",
       " 'possivel',\n",
       " 'ordenavam',\n",
       " 'aceitado',\n",
       " 'fale',\n",
       " 'sabuja',\n",
       " 'ofende',\n",
       " 'precaucao',\n",
       " 'decotado',\n",
       " 'surgir',\n",
       " 'aspera',\n",
       " 'vocemece',\n",
       " 'absorvendo',\n",
       " 'crescido',\n",
       " 'estatueta',\n",
       " 'ali',\n",
       " 'navalha',\n",
       " 'esbanjar',\n",
       " 'cobertas',\n",
       " 'pratos',\n",
       " 'reclamar',\n",
       " 'tetis',\n",
       " 'dentro',\n",
       " 'arrastava',\n",
       " 'apostado?',\n",
       " 'superiores',\n",
       " 'solidamente',\n",
       " 'seguir:',\n",
       " 'jazendo',\n",
       " 'inimigo',\n",
       " 'impressionava',\n",
       " 'tracada?',\n",
       " 'percorrer',\n",
       " 'gozou',\n",
       " 'fedro',\n",
       " 'jornada',\n",
       " 'parou',\n",
       " 'alteracao',\n",
       " 'seculos',\n",
       " 'contactos',\n",
       " 'cecilia',\n",
       " 'particularidades',\n",
       " 'tout',\n",
       " 'visitasse',\n",
       " 'fiacre',\n",
       " 'precursor',\n",
       " 'lavradeira',\n",
       " 'poses',\n",
       " 'apanhaste',\n",
       " 'parte!',\n",
       " 'heranca?',\n",
       " 'justica',\n",
       " 'ressoou:',\n",
       " 'pedra:',\n",
       " 'divisa:',\n",
       " 'inconscientemente',\n",
       " 'buena',\n",
       " 'romantismo',\n",
       " 'rico',\n",
       " 'desabafou',\n",
       " 'aprovassemos',\n",
       " 'cavava',\n",
       " 'caiam',\n",
       " 'seguintes',\n",
       " 'pedregulhos',\n",
       " 'fatigada',\n",
       " 'orar',\n",
       " 'pesadamente',\n",
       " 'afrontaria',\n",
       " 'acudira',\n",
       " 'compreendeste',\n",
       " 'heraldico',\n",
       " 'sacrifico',\n",
       " 'convencera',\n",
       " 'elogios',\n",
       " 'estilhacos',\n",
       " 'farejara',\n",
       " 'calcava',\n",
       " 'brigue',\n",
       " 'baterdes',\n",
       " 'pisara',\n",
       " 'formatura',\n",
       " 'supusera',\n",
       " 'injustica',\n",
       " 'armados',\n",
       " 'suas',\n",
       " 'chorou',\n",
       " 'taveira!',\n",
       " 'cartilha:',\n",
       " 'czar',\n",
       " 'anarquista',\n",
       " 'moca',\n",
       " 'panteismo!',\n",
       " 'governador',\n",
       " 'carvao',\n",
       " 'comecei',\n",
       " 'honesta!',\n",
       " 'desculpar',\n",
       " 'parenta',\n",
       " 'viagen',\n",
       " 'preparada',\n",
       " 'c',\n",
       " 'parenta?',\n",
       " 'mesa',\n",
       " 'casar',\n",
       " 'ler',\n",
       " 'comecaram',\n",
       " 'centrosinho',\n",
       " 'retardando',\n",
       " 'respeitavel',\n",
       " 'inofensivo!',\n",
       " 'costurava',\n",
       " 'imagem',\n",
       " 'risonho',\n",
       " 'finalmente!',\n",
       " 'abracando',\n",
       " 'acaso:',\n",
       " 'leitozinho',\n",
       " 'falhamos',\n",
       " 'granitico',\n",
       " 'enormidade:',\n",
       " 'pateadas',\n",
       " 'escreve',\n",
       " 'pandoli!',\n",
       " 'palido:',\n",
       " 'blasfemias',\n",
       " 'alargava',\n",
       " 'nevrose',\n",
       " 'alegre:',\n",
       " 'pequena',\n",
       " 'esmagando',\n",
       " 'estremunhada',\n",
       " 'brilhante',\n",
       " 'responderam',\n",
       " 'escutar',\n",
       " 'ignoradas',\n",
       " 'retrato',\n",
       " 'cima',\n",
       " 'tarifada',\n",
       " 'rija',\n",
       " 'patetas',\n",
       " 'abrira',\n",
       " 'debeis',\n",
       " 'alivio:',\n",
       " 'faculdade',\n",
       " 'dela:',\n",
       " 'bonita',\n",
       " 'ameacado',\n",
       " 'pavorosas',\n",
       " 'gemente',\n",
       " 'terminou',\n",
       " 'voos',\n",
       " 'clientela',\n",
       " 'curvava',\n",
       " 'ganha',\n",
       " 'aonde',\n",
       " 'rouxinois',\n",
       " 'delas',\n",
       " 'desesperado',\n",
       " 'tipico',\n",
       " 'democracia!',\n",
       " 'impacientou',\n",
       " 'copiosas',\n",
       " 'delicioso!',\n",
       " 'arranjara',\n",
       " 'animando',\n",
       " 'he',\n",
       " 'pessoalmente',\n",
       " 'clima',\n",
       " 'queixa',\n",
       " 'superior!',\n",
       " 'pedrosos',\n",
       " 'cerravam',\n",
       " 'empurrado',\n",
       " 'corremos',\n",
       " 'pleno',\n",
       " 'arrebatou',\n",
       " 'firme:',\n",
       " 'pires',\n",
       " 'chanceler',\n",
       " 'exaltado',\n",
       " 'estalos',\n",
       " 'volteando',\n",
       " 'despenhado',\n",
       " 'quadrados',\n",
       " 'barras',\n",
       " 'canto:',\n",
       " 'rodeado',\n",
       " 'vestida:',\n",
       " 'china',\n",
       " 'luz?',\n",
       " 'herculano',\n",
       " 'afundar',\n",
       " 'barbeado',\n",
       " 'muita',\n",
       " 'christo',\n",
       " 'audaz:',\n",
       " 'superior:',\n",
       " 'ranger',\n",
       " 'carne',\n",
       " 'entraria',\n",
       " 'convertida',\n",
       " 'pudico',\n",
       " 'falsos',\n",
       " 'chuvinha',\n",
       " 'boemia',\n",
       " 'missas!',\n",
       " 'misturara',\n",
       " 'retomado',\n",
       " 'humilhacao',\n",
       " 'jamais!',\n",
       " 'preta',\n",
       " 'purificasse',\n",
       " 'canelas',\n",
       " 'embrutecido',\n",
       " 'operarios',\n",
       " 'imitar',\n",
       " 'fazias',\n",
       " 'falava',\n",
       " 'ponto',\n",
       " 'pekin',\n",
       " 'cavalheiro!',\n",
       " 'alem',\n",
       " 'melancolia',\n",
       " 'neta',\n",
       " 'acomodando',\n",
       " 'condenando',\n",
       " 'cruzeiros',\n",
       " 'rapagoes',\n",
       " 'glorioso!',\n",
       " 'termometro',\n",
       " 'binoculo',\n",
       " 'carrinho',\n",
       " 'garcon',\n",
       " 'caladas',\n",
       " 'roubo',\n",
       " 'imprudencias',\n",
       " 'habilmente',\n",
       " 'sonoro',\n",
       " 'assistindo',\n",
       " 'manifestado',\n",
       " 'faiscar',\n",
       " 'livreiro',\n",
       " 'cuidar',\n",
       " 'urgencia',\n",
       " 'desfaleceram',\n",
       " 'saxonio',\n",
       " 'entaladela',\n",
       " 'podem',\n",
       " 'tormento!',\n",
       " 'chapa',\n",
       " 'realizavel',\n",
       " 'prova',\n",
       " 'aperfeicoada',\n",
       " 'criador',\n",
       " 'atras',\n",
       " 'isolados',\n",
       " 'frequentemente',\n",
       " 'sequiosa',\n",
       " 'vestimenta',\n",
       " 'caramania',\n",
       " 'importava!',\n",
       " 'acessit',\n",
       " 'heroi!',\n",
       " 'peristilo',\n",
       " 'entusiasmo?',\n",
       " 'torno',\n",
       " 'tormentosa',\n",
       " 'ilustracoe',\n",
       " 'pequeninos',\n",
       " 'estridencias',\n",
       " 'o?',\n",
       " 'folheando',\n",
       " 'estrofes',\n",
       " 'ajudante',\n",
       " 'altos',\n",
       " 'ar!',\n",
       " 'contemplativa',\n",
       " 'fort!',\n",
       " 'pungente',\n",
       " 'roble',\n",
       " 'difusamente',\n",
       " 'darque!',\n",
       " 'fazem',\n",
       " 'dispersavam',\n",
       " 'tom',\n",
       " 'perfumara',\n",
       " 'escasso',\n",
       " 'diga!',\n",
       " 'infanta',\n",
       " 'ferragens',\n",
       " 'burriqueiros',\n",
       " 'repreensao:',\n",
       " 'tagarelar',\n",
       " 'ponto?',\n",
       " 'alamares',\n",
       " 'tapecaria:',\n",
       " 'actrizes',\n",
       " 'contraste',\n",
       " 'exigiu',\n",
       " 'declinacoes',\n",
       " 'retinia',\n",
       " 'espertar',\n",
       " 'chale',\n",
       " 'cabecinha',\n",
       " 'pendia',\n",
       " 'renovar',\n",
       " 'cobardia',\n",
       " 'mim',\n",
       " 'escorregou',\n",
       " 'genios',\n",
       " 'valia',\n",
       " 'fragilidade',\n",
       " 'indignar',\n",
       " 'ambas',\n",
       " 'recusava',\n",
       " 'moderou',\n",
       " 'tenazes',\n",
       " 'arrebitados',\n",
       " 'diz',\n",
       " 'maxima:',\n",
       " 'deitei',\n",
       " 'leu',\n",
       " 'justos',\n",
       " 'un',\n",
       " 'democratas',\n",
       " 'tribunas',\n",
       " 'crescia',\n",
       " 'viste',\n",
       " 'livros?',\n",
       " 'bagagens:',\n",
       " 'rogou',\n",
       " 'murchava',\n",
       " 'arriscado',\n",
       " 'prevenisse',\n",
       " 'guardara',\n",
       " 'justo!',\n",
       " 'maldizendo',\n",
       " 'repreensao',\n",
       " 'conspirador',\n",
       " 'rolava',\n",
       " 'dava',\n",
       " 'surgiu',\n",
       " 'visitaram',\n",
       " 'abalar',\n",
       " 'cita',\n",
       " 'outras',\n",
       " 'pende',\n",
       " 'cana!',\n",
       " 'israelita',\n",
       " 'navio',\n",
       " 'enfraquecia',\n",
       " 'resposta?',\n",
       " 'curvara',\n",
       " 'tivesse',\n",
       " 'empanando',\n",
       " 'tapecaria',\n",
       " 'demagogo!',\n",
       " 'logo',\n",
       " 'don',\n",
       " 'vendaval!',\n",
       " 'populares',\n",
       " 'maquinalmente:',\n",
       " 'dominos',\n",
       " 'seio',\n",
       " 'carlota',\n",
       " 'desorientado',\n",
       " 'querem',\n",
       " 'casinhas',\n",
       " 'furioso!',\n",
       " 'cantanhede',\n",
       " 'malo',\n",
       " 'narizinho',\n",
       " 'preferiu',\n",
       " 'estacou',\n",
       " 'alojados',\n",
       " 'recusaram',\n",
       " 'esquadrinhava',\n",
       " 'conviccao:',\n",
       " 'arqueava',\n",
       " 'golgota',\n",
       " 'penetrados',\n",
       " 'cerrado',\n",
       " 'reconfortando',\n",
       " 'organizar',\n",
       " 'alvadias',\n",
       " 'rima!',\n",
       " 'faceto',\n",
       " 'alternacoes',\n",
       " 'murmurar',\n",
       " 'permitiria',\n",
       " 'liquido!',\n",
       " 'papeira',\n",
       " 'negro',\n",
       " 'fria:',\n",
       " 'gordos',\n",
       " 'dormirem',\n",
       " 'helas!',\n",
       " 'mendonca',\n",
       " 'bebedos',\n",
       " 'tratou',\n",
       " 'romperem',\n",
       " 'vi!',\n",
       " 'pilulas',\n",
       " 'rebecas',\n",
       " 'ocultado',\n",
       " 'resolucao',\n",
       " 'convalescente',\n",
       " 'peanha',\n",
       " 'salcede',\n",
       " 'caixas',\n",
       " 'varandas',\n",
       " 'pior!',\n",
       " 'saudosas',\n",
       " 'noticia',\n",
       " 'cansa',\n",
       " 'chegava',\n",
       " 'restabelecer',\n",
       " 'tudo',\n",
       " 'formato',\n",
       " 'choco',\n",
       " 'faltava',\n",
       " 'miudos',\n",
       " 'cativara',\n",
       " 'afogueada',\n",
       " 'valete',\n",
       " 'debaixo',\n",
       " 'retumbantemente',\n",
       " 'criara:',\n",
       " 'encontrava',\n",
       " 'rastejante!',\n",
       " 'excedido',\n",
       " 'absinto',\n",
       " 'davamo',\n",
       " 'desta!',\n",
       " 'chamandolhes',\n",
       " 'frente',\n",
       " 'batalhador',\n",
       " 'incerteza',\n",
       " 'esfolou',\n",
       " 'acreditam',\n",
       " 'neves:',\n",
       " 'cobertos',\n",
       " 'gesto',\n",
       " 'condenavel',\n",
       " 'fincada',\n",
       " 'redoma',\n",
       " 'exagerava',\n",
       " 'correm',\n",
       " 'dresde',\n",
       " 'crroliseu',\n",
       " 'empinado',\n",
       " 'planeou',\n",
       " 'chamine',\n",
       " 'avez',\n",
       " 'posta',\n",
       " 'atarracado',\n",
       " 'simpatias',\n",
       " 'picadas',\n",
       " 'soltava',\n",
       " 'assobiava',\n",
       " 'blusa',\n",
       " 'duelos',\n",
       " 'sandice',\n",
       " 'remendos',\n",
       " 'abominaveis',\n",
       " 'pusera',\n",
       " 'danada!',\n",
       " 'velhice!',\n",
       " 'gabinete:',\n",
       " 'parasitaire',\n",
       " 'segurando',\n",
       " 'monsieur!',\n",
       " 'acompanha',\n",
       " 'cantarolou:',\n",
       " 'puder',\n",
       " 'planeando',\n",
       " 'destino',\n",
       " 'mobilara',\n",
       " 'confessor',\n",
       " 'ainda:',\n",
       " 'religioso',\n",
       " 'judith',\n",
       " 'quem!',\n",
       " 'xadrezinho:',\n",
       " 'refugiados',\n",
       " 'apanhava',\n",
       " 'sentir',\n",
       " 'modernos:',\n",
       " 'indiscretamente',\n",
       " 'regatos',\n",
       " 'engenhoso',\n",
       " 'icava',\n",
       " 'superioridade',\n",
       " 'caloteiro!',\n",
       " 'atenta',\n",
       " 'castanholas',\n",
       " 'ria',\n",
       " 'silvavam',\n",
       " 'esmagadas',\n",
       " 'absorveu',\n",
       " 'baixaram',\n",
       " 'defronte',\n",
       " 'deitando',\n",
       " 'handicap!',\n",
       " 'levem',\n",
       " 'exilado',\n",
       " 'subissem?',\n",
       " 'vagueando',\n",
       " 'tacto?',\n",
       " 'ruidosa',\n",
       " 'arnica!',\n",
       " 'convivencia',\n",
       " 'ficavam',\n",
       " 'abracada',\n",
       " 'preto',\n",
       " 'serenar',\n",
       " 'devotos',\n",
       " 'adocicadas',\n",
       " 'sufocados',\n",
       " 'dedicava',\n",
       " 'tosses',\n",
       " 'entusiastas',\n",
       " 'instinctivamente',\n",
       " 'agarrem',\n",
       " 'volte',\n",
       " 'inclinada',\n",
       " 'planos',\n",
       " 'tardia',\n",
       " 'sacrificio',\n",
       " 'encontra',\n",
       " 'arco:',\n",
       " 'interminavel',\n",
       " 'comecou',\n",
       " 'pai:',\n",
       " 'haver',\n",
       " 'indignava',\n",
       " 'retomou',\n",
       " 'composicao',\n",
       " 'acredite',\n",
       " 'balcao',\n",
       " 'deteveo',\n",
       " 'indiferencas',\n",
       " 'adulterios',\n",
       " 'recobertas',\n",
       " 'semita?',\n",
       " 'anos',\n",
       " 'paternal',\n",
       " 'combate',\n",
       " 'hospitalidade',\n",
       " 'insaciavel!',\n",
       " 'pico',\n",
       " 'severidade:',\n",
       " 'sabujice',\n",
       " 'claustro',\n",
       " 'agitacoe',\n",
       " 'comedia',\n",
       " 'benevolencia',\n",
       " 'maganoes',\n",
       " 'recostada',\n",
       " 'assustes',\n",
       " 'direita',\n",
       " 'noutros',\n",
       " 'telegramas:',\n",
       " 'folhetinistas',\n",
       " 'harmonia',\n",
       " 'desmanchadamente',\n",
       " 'clifordon',\n",
       " 'detestando',\n",
       " 'transia',\n",
       " 'actor',\n",
       " 'colada',\n",
       " 'chef',\n",
       " 'enrodilhar',\n",
       " 'indiferentes',\n",
       " 'cada',\n",
       " 'fatigasse',\n",
       " 'comunicado',\n",
       " 'marrasquino',\n",
       " 'remexiam',\n",
       " 'importo',\n",
       " 'heroica?',\n",
       " 'benemerito',\n",
       " 'vendas',\n",
       " 'agosto?',\n",
       " 'ajudados',\n",
       " 'admiraram',\n",
       " 'drogues',\n",
       " 'desmaiadas',\n",
       " 'desfalecida',\n",
       " 'transparecia',\n",
       " 'vestia:',\n",
       " 'vibrar',\n",
       " 'mania',\n",
       " 'asco!',\n",
       " 'serpeavam',\n",
       " 'estorade',\n",
       " 'pequenos',\n",
       " 'inspiracao:',\n",
       " 'trauteando',\n",
       " 'bati',\n",
       " 'gabar',\n",
       " 'esbocou',\n",
       " 'misturada',\n",
       " 'exacerbara',\n",
       " 'errava',\n",
       " 'redondinha',\n",
       " 'colossal',\n",
       " 'aproxima',\n",
       " 'desapertava',\n",
       " 'deitado',\n",
       " 'ocultamente',\n",
       " 'saio',\n",
       " 'consolou:',\n",
       " 'torrava',\n",
       " 'instalado',\n",
       " 'humilde',\n",
       " 'supersticioso',\n",
       " 'ouco',\n",
       " 'grevy',\n",
       " 'excelencia',\n",
       " 'inventor',\n",
       " 'rasgada',\n",
       " 'alumiar',\n",
       " 'roxane',\n",
       " 'olhou',\n",
       " 'ciencia?',\n",
       " 'perguntava',\n",
       " 'desfalecendo',\n",
       " 'intelectual:',\n",
       " 'esplendidos',\n",
       " 'gluzk',\n",
       " 'intimacao',\n",
       " 'financas:',\n",
       " 'testemunha',\n",
       " 'proporcao',\n",
       " 'sufocante',\n",
       " 'gomes:',\n",
       " 'consultorio',\n",
       " 'baleia',\n",
       " 'concertos',\n",
       " 'possuisse',\n",
       " 'embucado',\n",
       " 'pavoroso',\n",
       " 'vouzeias',\n",
       " 'sensibilizou',\n",
       " 'fita',\n",
       " 'pinheiros',\n",
       " 'contem',\n",
       " 'folhetim',\n",
       " 'baixinho',\n",
       " 'apresentacao:',\n",
       " 'citera',\n",
       " 'complicacao',\n",
       " 'triunfo',\n",
       " 'cocotes',\n",
       " 'recheio',\n",
       " 'brutal',\n",
       " 'estalando',\n",
       " 'tremula',\n",
       " 'ansiou',\n",
       " 'sujo',\n",
       " 'pai!',\n",
       " 'eles',\n",
       " 'esverdinhado',\n",
       " 'pequena!',\n",
       " 'suando',\n",
       " 'macilento',\n",
       " 'desplante',\n",
       " 'mexendo',\n",
       " 'times',\n",
       " 'confissoes',\n",
       " 'colina',\n",
       " 'poderoso!',\n",
       " 'felicitou',\n",
       " 'cerimonia?',\n",
       " 'capitulo',\n",
       " 'escorrer',\n",
       " 'desconfortos',\n",
       " 'eleicao',\n",
       " 'suprimi',\n",
       " 'amima',\n",
       " 'recanto',\n",
       " 'catarreira',\n",
       " 'abre',\n",
       " 'gravesinha',\n",
       " 'daudet',\n",
       " 'acochado',\n",
       " 'portugal!',\n",
       " 'adivinhara',\n",
       " 'extravagante',\n",
       " 'tapete:',\n",
       " 'investiu',\n",
       " 'gravuras',\n",
       " 'odiando',\n",
       " 'uma',\n",
       " 'desafinado',\n",
       " 'chateau',\n",
       " 'silveirinha',\n",
       " 'leve:',\n",
       " 'desembarcou',\n",
       " 'stuart',\n",
       " 'tentou',\n",
       " 'pitoresco',\n",
       " 'alastrando',\n",
       " 'modistas!',\n",
       " 'reviver!',\n",
       " 'matarme',\n",
       " 'santissimo!',\n",
       " 'maltratam',\n",
       " 'indicando',\n",
       " 'embrulho',\n",
       " 'frescas',\n",
       " 'tafeta',\n",
       " 'falira',\n",
       " 'conheceu',\n",
       " 'psyche',\n",
       " 'descida',\n",
       " 'indignacao:',\n",
       " 'aparencia',\n",
       " 'malmequer',\n",
       " 'sitiais',\n",
       " 'gosto',\n",
       " 'oco',\n",
       " 'quina',\n",
       " 'bilhas',\n",
       " 'politicos',\n",
       " 'enevoavam',\n",
       " 'procurou!',\n",
       " 'festa',\n",
       " 'rodela',\n",
       " 'romagem',\n",
       " 'contrario!',\n",
       " 'urros',\n",
       " 'adeus',\n",
       " 'oui!',\n",
       " 'baviera',\n",
       " 'largas',\n",
       " 'bye',\n",
       " 'meteras',\n",
       " 'findarem!',\n",
       " 'dissipar',\n",
       " 'indiferenca',\n",
       " 'gritavam',\n",
       " 'realcava',\n",
       " 'fincara',\n",
       " 'terriveis',\n",
       " 'faiscantes',\n",
       " 'farrapos',\n",
       " 'encher',\n",
       " 'lado!',\n",
       " 'repuxo',\n",
       " 'valente:',\n",
       " 'absurdamente',\n",
       " 'aventura?',\n",
       " 'bastarda!',\n",
       " 'sai',\n",
       " 'vulto',\n",
       " 'alcantara',\n",
       " 'pronunciava',\n",
       " 'politiquetes',\n",
       " 'isto',\n",
       " 'sossego!',\n",
       " 'macao',\n",
       " 'empolgava',\n",
       " 'decomposicao',\n",
       " 'ferindo',\n",
       " 'raios!',\n",
       " 'agitado',\n",
       " 'empunhando',\n",
       " 'ruminava',\n",
       " 'perguntar:',\n",
       " 'agarra',\n",
       " 'priori',\n",
       " 'durindanas',\n",
       " 'brilhando',\n",
       " 'entusiasta',\n",
       " 'merecimentos',\n",
       " 'porcao',\n",
       " 'tudo?',\n",
       " 'esbarrondado',\n",
       " 'imbecilissimo',\n",
       " 'sport',\n",
       " 'cobrar',\n",
       " 'conviccao',\n",
       " 'verbosidade',\n",
       " 'loirinhas',\n",
       " ...]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df= prepare_data()\n",
    "\n",
    "complete_text = ' '.join(df.sentences.values.tolist())\n",
    "complete_unique_words_list = list(set(complete_text.split(' ')))\n",
    "vocab_len = len(complete_unique_words_list)\n",
    "display(df)\n",
    "display(vocab_len)\n",
    "display(complete_unique_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>sent_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apesar deste fresco nome de vivenda campestre...</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>em monsenhor bucarini , nuncio de sao santida...</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>este inutil pardieiro como lhe chamava vilaca...</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nessa ocasiao vendera se outra propriedade d ...</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>eram uma antiga familia da beira , sempre pou...</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10153</th>\n",
       "      <td>parou diante do alto espelho suspenso entre a...</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10162</th>\n",
       "      <td>passeio a cavalo no bois almoco no bignon uma...</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10172</th>\n",
       "      <td>carlos pos tambem o chapeu: e desceram pelas ...</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10188</th>\n",
       "      <td>tudo aceitar , o que vem e o que foge , com a...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10196</th>\n",
       "      <td>ja avistavam o aterro , a sua longa fila de l...</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1142 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentences  sent_length\n",
       "1       apesar deste fresco nome de vivenda campestre...          129\n",
       "3       em monsenhor bucarini , nuncio de sao santida...          210\n",
       "4       este inutil pardieiro como lhe chamava vilaca...           64\n",
       "5       nessa ocasiao vendera se outra propriedade d ...           53\n",
       "7       eram uma antiga familia da beira , sempre pou...           57\n",
       "...                                                  ...          ...\n",
       "10153   parou diante do alto espelho suspenso entre a...           55\n",
       "10162   passeio a cavalo no bois almoco no bignon uma...           71\n",
       "10172   carlos pos tambem o chapeu: e desceram pelas ...           96\n",
       "10188   tudo aceitar , o que vem e o que foge , com a...           61\n",
       "10196   ja avistavam o aterro , a sua longa fila de l...           58\n",
       "\n",
       "[1142 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"sent_length\"] = df.sentences.apply(lambda x: len(x.split()))\n",
    "\n",
    "plt.figure()\n",
    "pd.Series(df.sent_length).hist()\n",
    "plt.title(\"Sentence Size for training dataset\")\n",
    "df[df.sent_length>window_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>sent_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a casa que vieram habitar em lisboa , no outon...</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apesar deste fresco nome de vivenda campestre...</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>longos anos o ramalhete permanecera desabitad...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>em monsenhor bucarini , nuncio de sao santida...</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>este inutil pardieiro como lhe chamava vilaca...</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10204</th>\n",
       "      <td>com efeito , nao vale a pena fazer um esforco...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10205</th>\n",
       "      <td>ega , ao lado , ajuntava , ofegante , atirand...</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10206</th>\n",
       "      <td>a lanterna vermelha do americano , ao longe ,...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10207</th>\n",
       "      <td>e foi em carlos e em joao da ega uma esperanc...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10208</th>\n",
       "      <td>entao , para apanhar o americano , os dois am...</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9687 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentences  sent_length\n",
       "0      a casa que vieram habitar em lisboa , no outon...           42\n",
       "1       apesar deste fresco nome de vivenda campestre...          129\n",
       "2       longos anos o ramalhete permanecera desabitad...           25\n",
       "3       em monsenhor bucarini , nuncio de sao santida...          210\n",
       "4       este inutil pardieiro como lhe chamava vilaca...           64\n",
       "...                                                  ...          ...\n",
       "10204   com efeito , nao vale a pena fazer um esforco...           18\n",
       "10205   ega , ao lado , ajuntava , ofegante , atirand...           33\n",
       "10206   a lanterna vermelha do americano , ao longe ,...           14\n",
       "10207   e foi em carlos e em joao da ega uma esperanc...           29\n",
       "10208   entao , para apanhar o americano , os dois am...           31\n",
       "\n",
       "[9687 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.sent_length>=5]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'data scientist tasks': 0.5333333283333335,\n",
       "  'data scientist jobs': 0.5333333283333335,\n",
       "  'data scientist job': 0.4666666616666668,\n",
       "  'data scientist, and': 0.3333333283333334,\n",
       "  'data engineer tasks': 0.4666666616666667,\n",
       "  'data engineer jobs': 0.4666666616666667},\n",
       " 'data scientist jobs',\n",
       " 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_list=['data scientist tasks', 'data scientist jobs','data scientist job','data scientist, and','data engineer tasks','data engineer jobs']\n",
    "\n",
    "def Minimum_Bayes_Risk(list_of_candidates, field = \"rouge-l\", candidate_reference = None):\n",
    "    candidate_score = {}\n",
    "    max_score = 0\n",
    "    best_score_candidate= ''\n",
    "    best_candidate_index = -1\n",
    "    if candidate_reference == None :\n",
    "        for candidate_ref in list_of_candidates:\n",
    "            \n",
    "            rest_of_candidates = copy.deepcopy(list_of_candidates)\n",
    "            if candidate_ref in rest_of_candidates : rest_of_candidates.remove(candidate_ref)\n",
    "            score = 0\n",
    "\n",
    "            for candidate in rest_of_candidates:\n",
    "                score += ROUGE.get_scores(candidate, candidate_ref)[0][field][\"f\"]\n",
    "            \n",
    "            score = score/len(rest_of_candidates)\n",
    "\n",
    "            candidate_score[candidate_ref] = score\n",
    "\n",
    "            if score >= max_score:\n",
    "                best_score_candidate =candidate_ref\n",
    "                best_candidate_index = list_of_candidates.index(candidate_ref)\n",
    "                max_score = score\n",
    "\n",
    "    else:\n",
    "            \n",
    "            for candidate in list_of_candidates:\n",
    "                score = 0\n",
    "                score = ROUGE.get_scores(candidate, candidate_reference)[0][field][\"f\"]\n",
    "                if score >= max_score:\n",
    "                    best_score_candidate =candidate_ref\n",
    "\n",
    "\n",
    "    return candidate_score, best_score_candidate, best_candidate_index\n",
    "\n",
    "candidate_score, best_score_candidate, best_candidate_index = Minimum_Bayes_Risk(candidate_list)\n",
    "candidate_score, best_score_candidate , best_candidate_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Dataset ready and Tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>sent_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5678</th>\n",
       "      <td>tu nao tens visto o damaso? nunca mais me apa...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>nao temos nada capaz de dar a um rapaz um boc...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7831</th>\n",
       "      <td>foi ao cruges que se dirigiu , entalando o mo...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8357</th>\n",
       "      <td>ve que tudo isso de fidalguia , pergaminhos ,...</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617</th>\n",
       "      <td>olavia , depois de ter servido em lisboa , na...</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3737</th>\n",
       "      <td>e nem so a estreia do ega era pessima tambem ...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5523</th>\n",
       "      <td>ainda na vespera , havia ainda instantes , co...</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9453</th>\n",
       "      <td>carlos beijou a mao fria que pendia .</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5231</th>\n",
       "      <td>mas ega , que estivera um pouco silencioso , ...</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7370</th>\n",
       "      <td>o simplorio , que bate ai pilecas bifes , que...</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7749 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentences  sent_length\n",
       "5678   tu nao tens visto o damaso? nunca mais me apa...           19\n",
       "3996   nao temos nada capaz de dar a um rapaz um boc...           14\n",
       "7831   foi ao cruges que se dirigiu , entalando o mo...           14\n",
       "8357   ve que tudo isso de fidalguia , pergaminhos ,...           33\n",
       "1617   olavia , depois de ter servido em lisboa , na...           52\n",
       "...                                                 ...          ...\n",
       "3737   e nem so a estreia do ega era pessima tambem ...           13\n",
       "5523   ainda na vespera , havia ainda instantes , co...           47\n",
       "9453             carlos beijou a mao fria que pendia .             8\n",
       "5231   mas ega , que estivera um pouco silencioso , ...           41\n",
       "7370   o simplorio , que bate ai pilecas bifes , que...           43\n",
       "\n",
       "[7749 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>sent_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6946</th>\n",
       "      <td>mas eu nao posso gostar mais dele do que gost...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>podia antes ter ferido o alencar , um rapaz i...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4453</th>\n",
       "      <td>amanha a noite , na estacao .</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306</th>\n",
       "      <td>errara nos vastos aneis de saturno e as madru...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>teve um suspiro cansada e lento , murmurou: n...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7857</th>\n",
       "      <td>la tem a explicacao que tudo cobre , uma gota...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6215</th>\n",
       "      <td>passados dias , passeando com maria nos arred...</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2974</th>\n",
       "      <td>a pena para ele era outro ninho de recordacoe...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5486</th>\n",
       "      <td>porque se for para la instalar se , e depois ...</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10021</th>\n",
       "      <td>e mostrava os altos da cidade , os velhos out...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1938 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentences  sent_length\n",
       "6946    mas eu nao posso gostar mais dele do que gost...           11\n",
       "321     podia antes ter ferido o alencar , um rapaz i...           19\n",
       "4453                      amanha a noite , na estacao .             7\n",
       "1306    errara nos vastos aneis de saturno e as madru...           24\n",
       "444     teve um suspiro cansada e lento , murmurou: n...           13\n",
       "...                                                  ...          ...\n",
       "7857    la tem a explicacao que tudo cobre , uma gota...           22\n",
       "6215    passados dias , passeando com maria nos arred...           71\n",
       "2974    a pena para ele era outro ninho de recordacoe...           16\n",
       "5486    porque se for para la instalar se , e depois ...           48\n",
       "10021   e mostrava os altos da cidade , os velhos out...           29\n",
       "\n",
       "[1938 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_val_test = train_test_split(df,test_size=0.2,train_size=0.8)\n",
    "\n",
    "display(df_train)\n",
    "display(df_val_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>sent_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>e um ingles , uma especie de doido? .</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040</th>\n",
       "      <td>pegaram se outra vez , veio dizer damaso a ca...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5422</th>\n",
       "      <td>mas agora ja nao eram as visitas de medico qu...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7248</th>\n",
       "      <td>nasceste , acudiu o ega , para colher as flor...</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2547</th>\n",
       "      <td>e engenhoso , disse ela dando familiarmente a...</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2317</th>\n",
       "      <td>ora esta sao ex restabelecida! exclamou damas...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9161</th>\n",
       "      <td>ja nao ha disso! o jantar terminava friamente .</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5807</th>\n",
       "      <td>na estrada , silenciosa por ora , ia so passa...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5175</th>\n",
       "      <td>a senhora de vermelho , sentada defronte , de...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7001</th>\n",
       "      <td>eram valachos de grandes bigodes , peruanos c...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1453 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentences  sent_length\n",
       "1254             e um ingles , uma especie de doido? .             9\n",
       "2040   pegaram se outra vez , veio dizer damaso a ca...           16\n",
       "5422   mas agora ja nao eram as visitas de medico qu...           23\n",
       "7248   nasceste , acudiu o ega , para colher as flor...           42\n",
       "2547   e engenhoso , disse ela dando familiarmente a...           39\n",
       "...                                                 ...          ...\n",
       "2317   ora esta sao ex restabelecida! exclamou damas...           15\n",
       "9161   ja nao ha disso! o jantar terminava friamente .             9\n",
       "5807   na estrada , silenciosa por ora , ia so passa...           17\n",
       "5175   a senhora de vermelho , sentada defronte , de...           22\n",
       "7001   eram valachos de grandes bigodes , peruanos c...           25\n",
       "\n",
       "[1453 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>sent_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2291</th>\n",
       "      <td>e o figaro diz que ela teve aventuras , natur...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8338</th>\n",
       "      <td>foi para don maria da cunha uma alegria , uma...</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7391</th>\n",
       "      <td>e isto agora ameacava para sempre a sua vida ...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2787</th>\n",
       "      <td>e nunca parecera tao funebre , tao reles , co...</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7206</th>\n",
       "      <td>a mim todos esses dourados , esses enramalhet...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9145</th>\n",
       "      <td>ele , por exemplo , ouvira uma noite uma mala...</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9105</th>\n",
       "      <td>nao termos um pretexto para irmos fora , a um...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8741</th>\n",
       "      <td>e pouco a pouco , sob o tepido aconchego dos ...</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>teve logo conflitos com os chefes liberais fo...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3802</th>\n",
       "      <td>a condessa , ao estender os dedos a carlos , ...</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>485 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentences  sent_length\n",
       "2291   e o figaro diz que ela teve aventuras , natur...           17\n",
       "8338   foi para don maria da cunha uma alegria , uma...           45\n",
       "7391   e isto agora ameacava para sempre a sua vida ...           16\n",
       "2787   e nunca parecera tao funebre , tao reles , co...           37\n",
       "7206   a mim todos esses dourados , esses enramalhet...           19\n",
       "...                                                 ...          ...\n",
       "9145   ele , por exemplo , ouvira uma noite uma mala...           28\n",
       "9105   nao termos um pretexto para irmos fora , a um...           23\n",
       "8741   e pouco a pouco , sob o tepido aconchego dos ...           36\n",
       "86     teve logo conflitos com os chefes liberais fo...           19\n",
       "3802   a condessa , ao estender os dedos a carlos , ...           34\n",
       "\n",
       "[485 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "df_test, df_val = train_test_split(df_val_test, test_size = 0.25,train_size =0.75)\n",
    "display(df_test)\n",
    "display(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"  # You can also use other decoder models like \"gpt2-medium\", \"gpt2-large\", etc.\n",
    "#gpt2_model = TFGPT2Model.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "#tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "# Example input text\n",
    "input_text = [\"Once upon a time,\",\"hey you \"]\n",
    "\n",
    "# Tokenize the input text\n",
    "input_ids = tokenizer(input_text, padding=True, return_tensors=\"tf\")\n",
    "#embeddings = gpt2_model(input_ids)[\"last_hidden_state\"]\n",
    "\n",
    "\n",
    "input_ids#, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates overlapping sub-sequences (sliding windows) of size window_size. The shift parameter controls how much the window slides (a shift of 1 will move the window by 1 token at a time).\n",
    "def create_dataset(text, k = 5):\n",
    "\n",
    "    #Create tf.dataset\n",
    "    tfds = tf.data.Dataset.from_tensor_slices((text.split()))\n",
    "    for x in tfds.take(1):\n",
    "        print(\"tf.dataset ->\",x)\n",
    "\n",
    "\n",
    "    #Create sliding windows for inputs\n",
    "    input_windows = tfds.window(window_size, shift=1, drop_remainder=False)\n",
    "\n",
    "    # Flattens the windows (since window() creates datasets of datasets) and ensures each sliding window is a sequence of window_size.\n",
    "    input_windows = input_windows.flat_map(lambda window: window.batch(window_size))\n",
    "    for x in input_windows.take(1):\n",
    "        print(\"tf.dataset windowed ->\",x)\n",
    "\n",
    "    # Create sliding windows for outputs (shift by 1 more to make next window the target)\n",
    "    output_windows = tfds.skip(1).window(window_size, shift=1, drop_remainder=True)\n",
    "    output_windows = output_windows.flat_map(lambda window: window.batch(window_size))\n",
    "\n",
    "    # Zip the input and output windows together\n",
    "    dataset = tf.data.Dataset.zip((input_windows, output_windows))\n",
    "    dataset = dataset.batch(batch_size).shuffle(buffer_size=len(df_train)).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # for i, o  in dataset.take(1):\n",
    "    #     print(\"Input\",i)\n",
    "    #     print(\"Output\",o)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# def create_dataset(text):\n",
    "#     words = text.split()\n",
    "#     num_windows = len(words) - window_size + 1\n",
    "#     columns = [\"sentence\", \"next_sentence\"]\n",
    "\n",
    "    \n",
    "\n",
    "#     list_of_sentences= []\n",
    "#     for i in range(num_windows):\n",
    "#         window = ' '.join(words[i:i+window_size])\n",
    "#         next_window = ' '.join(words[i+1:i+window_size+1])\n",
    "#         list_of_sentences.append((window,next_window))\n",
    "\n",
    "#     df = pd.DataFrame(list_of_sentences, columns=columns)      \n",
    "\n",
    "#     tokenized_inputs = tokenizer(df.sentence.tolist(),padding=True, return_tensors=\"tf\") \n",
    "#     tokenized_outputs = tokenizer(df.next_sentence.tolist(),padding=True, return_tensors=\"tf\") \n",
    "\n",
    "#     t_tds = tf.data.Dataset.from_tensor_slices((tokenized_inputs, tokenized_outputs)).batch(batch_size).shuffle(buffer_size=len(df)).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "#     return df\n",
    "\n",
    "train_tfds = create_dataset(''.join(df_train.sentences.tolist()))\n",
    "for i, o in train_tfds.take(1):\n",
    "    print(i)\n",
    "    print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_tfds = create_dataset(''.join(df_val.sentences.tolist()))\n",
    "test_tfds = create_dataset(''.join(df_test.sentences.tolist()))\n",
    "\n",
    "for i, o in test_tfds.take(1):\n",
    "    print(i)\n",
    "    print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting tokenizer and model for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_len = vocab_len, \n",
    "        window_size = window_size, \n",
    "        embedding_dim = embedding_dim,  \n",
    "        n_layers = 1, \n",
    "        n_attention_head = 2, \n",
    "        feed_forward_dim = [128],\n",
    "        att_dropout_rate = [0.0],\n",
    "        n_dense_layers = 1, \n",
    "        dense_layers = [32], \n",
    "        dropout_rate = []\n",
    "        ):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.num_layers = n_layers\n",
    "        self.n_attention_head =n_attention_head\n",
    "        self.feed_forward_dim = feed_forward_dim\n",
    "        self.att_dropout_rate = att_dropout_rate\n",
    "\n",
    "        self.decoder = TFGPT2Model.from_pretrained(model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        \n",
    "        self.decoder_layer = [keras_nlp.layers.TransformerDecoder(num_heads=self.n_attention_head, intermediate_dim=self.feed_forward_dim[i],dropout = att_dropout_rate[i]) for i in range(self.num_layers)]\n",
    "        \n",
    "        self.n_dense_layers = n_dense_layers\n",
    "\n",
    "        if self.n_dense_layers >0:\n",
    "            self.dense_layers = [Dense(dense_layers[i], activation='relu') for i in range(self.n_dense_layers)]\n",
    "\n",
    "            if self.n_dense_layers > 1:\n",
    "                self.dropout_layers = [Dropout(dropout_rate[i]) for i in range(self.n_dense_layers-1)]\n",
    "        \n",
    "        self.output_dense = Dense(vocab_len)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.decoder(inputs)[\"last_hidden_state\"]\n",
    "        print(x.shape)\n",
    "\n",
    "        if self.num_layers > 0:\n",
    "            for i in range(self.num_layers ):\n",
    "                x = self.decoder_layer[i](x)\n",
    "\n",
    "        else: \n",
    "            x = tf.reduce_mean(x, axis=1)\n",
    "\n",
    "        if self.n_dense_layers > 0:\n",
    "            for i in range(self.n_dense_layers):\n",
    "                x = self.dense_layers[i](x)\n",
    "\n",
    "                if i < (self.n_dense_layers - 1) and  self.n_dense_layers > 1:\n",
    "                    x = self.dropout_layers[i](x)\n",
    "\n",
    "        outputs_logits  = self.output_dense(x)\n",
    "\n",
    "        return outputs_logits\n",
    "\n",
    "\n",
    "generate_text_model = TextGenerator(\n",
    "    vocab_len = vocab_len,\n",
    "    window_size = window_size, \n",
    "    embedding_dim = embedding_dim,  \n",
    "    n_layers = 1, \n",
    "    n_attention_head = 2, \n",
    "    feed_forward_dim = [128],\n",
    "    dropout_rate = [0.0]\n",
    "    )\n",
    "\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
    "\n",
    "generate_text_model.compile(\n",
    "    loss=loss_fn, \n",
    "    optimizer=Adam(learning_rate=0.0005), \n",
    "    metrics=[perplexity])\n",
    "\n",
    "#print(generate_text_model(train_tfds.sentence.tolist()[:10]).shape)\n",
    "for i in train_tfds.take(1):\n",
    "    print(i)\n",
    "    print(generate_text_model(i))\n",
    "\n",
    "generate_text_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaaaaaaa\n",
    "generate_text_model.fit(train_tfds, validation_data= dev_tfds, epochs=epochs, batch_size=32, verbose =1, callbacks=[stop_early, tensorboard_callback, text_generation_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tokens = start_packer(tokenizer([\"\"]))\n",
    "prompt_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next(prompt, cache, index):\n",
    "    logits = generate_text_model(prompt)[:, index - 1, :]\n",
    "    # Ignore hidden states for now; only needed for contrastive search.\n",
    "    hidden_states = None\n",
    "    return logits, hidden_states, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and tunning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopKTextGenerator(keras.callbacks.Callback):\n",
    "    \"\"\"A callback to generate text from a trained model using top-k.\"\"\"\n",
    "\n",
    "    def __init__(self, k):\n",
    "        self.top_k_sampler = keras_nlp.samplers.TopKSampler(k)\n",
    "        #self.beam_search_sampler = keras_nlp.samplers.BeamSampler(num_beams=10)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        output_tokens = self.top_k_sampler(\n",
    "            next=next,\n",
    "            prompt=prompt_tokens,\n",
    "            index=1,\n",
    "        )\n",
    "        txt = tokenizer.detokenize(output_tokens)\n",
    "        print(f\"Top-K search generated text: \\n{txt}\\n\")\n",
    "        \n",
    "text_generation_callback = TopKTextGenerator(k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"_v1\"\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=f\"logs/attention_training{version}/\")\n",
    "\n",
    "def get_model_tunning(hp):\n",
    "\n",
    "\n",
    "    n_att_layers = hp.Int('n_att_layers', 1, 3)\n",
    "    n_attention_head= hp.Int('n_attention_head', 1, 2)\n",
    "    feed_forward_dim_list =  [hp.Int('att_units_'+str(i), 32, 128) for i in range(n_att_layers)]\n",
    "    att_dropout_rate=  [hp.Float('att_dropout_rate_'+str(i), 0.0, 0.5, step = 0.1) for i in range(n_att_layers)]\n",
    "\n",
    "    n_dense_layers = hp.Int('n_dense_layers', 0, 2)\n",
    "    dropout_rate = []\n",
    "    dense_layers_list = []\n",
    "    if n_dense_layers> 0:\n",
    "        dense_layers_list =  [hp.Int('dense_layers_'+str(i), 32, 128) for i in range(n_dense_layers)]\n",
    "\n",
    "        if n_dense_layers > 1:\n",
    "            dropout_rate=  [hp.Float('dropout_rate_'+str(i), 0.0, 0.5, step = 0.1) for i in range(n_dense_layers-1)]\n",
    "    \n",
    "    hp_learning_rate = hp.Float('learning_rate', 0.000001, 0.001)\n",
    "    embedding_dim_list = 50#hp.Int('embedding_dim', 50,  200, step = 50)\n",
    "    \n",
    "    model = TextGenerator(\n",
    "        vocab_len = vocab_len,\n",
    "        window_size = window_size, \n",
    "        embedding_dim = embedding_dim_list,  \n",
    "        n_layers = n_att_layers, \n",
    "        n_attention_head = n_attention_head, \n",
    "        feed_forward_dim = feed_forward_dim_list,\n",
    "        att_dropout_rate = att_dropout_rate,\n",
    "        n_dense_layers = n_dense_layers, \n",
    "        dense_layers = dense_layers_list, \n",
    "        dropout_rate = dropout_rate\n",
    "    )\n",
    "\n",
    "    loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
    "\n",
    "    model.compile(\n",
    "        loss=loss_fn, \n",
    "        optimizer=Adam(learning_rate=hp_learning_rate), \n",
    "        metrics=[perplexity],\n",
    "        )\n",
    "\n",
    "    for i, o in train_tfds.take(1):\n",
    "        print(i.shape, o.shape)\n",
    "        print(model(i).shape)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "        get_model_tunning,\n",
    "        objective= kt.Objective('val_perplexity', direction=\"min\"), #kt.Objective('val_auc', direction=\"max\"),# #val_binary_accuracy\n",
    "        max_trials = 60,\n",
    "        directory=r\"Hyperparam_tunning\",\n",
    "        project_name=f'keras_att_tunning{version}',\n",
    "    )\n",
    "\n",
    "if TUNNING:\n",
    "    tuner.search(train_tfds,  epochs=200, batch_size=batch_size, validation_data = dev_tfds, verbose =1, callbacks=[stop_early, tensorboard_callback]) \n",
    "\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_hps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, trials in enumerate(tuner.oracle.get_best_trials(num_trials=50)):\n",
    "    print(f\"[{i}] Trial id :{trials.trial_id} | Score :{trials.score} -->\",trials.hyperparameters.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, trials in enumerate(tuner.oracle.get_best_trials(num_trials=60)):\n",
    "    print(f\"[{i}] Trial id :{trials.trial_id} | Score :{trials.score} -->\",trials.hyperparameters.values)\n",
    "    if i == 34:\n",
    "        print(f\"[{i}] Trial id :{trials.trial_id} | Score :{trials.score} -->\",trials.hyperparameters.values)\n",
    "\n",
    "        n_att_layers = 1#trials.hyperparameters.values['n_att_layers']\n",
    "        n_attention_head= trials.hyperparameters.values['n_attention_head']\n",
    "\n",
    "\n",
    "        feed_forward_dim_list = []\n",
    "        att_dropout_rate = []\n",
    "        if n_att_layers > 0 :\n",
    "            feed_forward_dim_list =  [32 for i in range(n_att_layers)] #trials.hyperparameters.values['att_units_'+str(i)] \n",
    "            att_dropout_rate=  [0.4 for i in range(n_att_layers)] #trials.hyperparameters.values['att_dropout_rate_'+str(i)]\n",
    "        \n",
    "        hp_learning_rate = 0.000005 #trials.hyperparameters.values['learning_rate']\n",
    "        embedding_dim_list = 100\n",
    "        \n",
    "\n",
    "        n_dense_layers = trials.hyperparameters.values['n_dense_layers']\n",
    "        dropout_rate = []\n",
    "        dense_layers_list = []\n",
    "        if n_dense_layers> 0:\n",
    "            dense_layers_list =  [trials.hyperparameters.values['dense_layers_'+str(i)] for i in range(n_dense_layers)]\n",
    "\n",
    "            if n_dense_layers > 1:\n",
    "                dropout_rate=  [trials.hyperparameters.values['dropout_rate_'+str(i)] for i in range(n_dense_layers-1)]\n",
    "\n",
    "\n",
    "        generate_text_model = TextGenerator(\n",
    "            vocab_len = vocab_len,\n",
    "            window_size = window_size, \n",
    "            embedding_dim = embedding_dim_list,  \n",
    "            n_layers = n_att_layers, \n",
    "            n_attention_head = n_attention_head, \n",
    "            feed_forward_dim = feed_forward_dim_list,\n",
    "            att_dropout_rate = att_dropout_rate,\n",
    "            n_dense_layers = n_dense_layers, \n",
    "            dense_layers = dense_layers_list, \n",
    "            dropout_rate = dropout_rate\n",
    "        )\n",
    "\n",
    "        loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
    "\n",
    "        generate_text_model.compile(\n",
    "            loss=loss_fn, \n",
    "            optimizer=Adam(learning_rate=hp_learning_rate), \n",
    "            metrics=[perplexity],\n",
    "            )\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=f\"logs/attention_training{version}/model{i}_v2\")\n",
    "        for i, o in train_tfds.take(1):\n",
    "            print(i.shape, o.shape)\n",
    "            print(generate_text_model(i).shape)\n",
    "\n",
    "        \n",
    "        generate_text_model.fit(train_tfds,  epochs=500, batch_size=batch_size, validation_data = dev_tfds, verbose =1, callbacks=[tensorboard_callback,text_generation_callback]) \n",
    "        break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, best_hps in enumerate(tuner.get_best_hyperparameters(num_trials=40)):\n",
    "#     if i == 34:\n",
    "#         print(f\"Best Hyperparameters: {best_hps.__dict__}\")\n",
    "#         generate_text_model = tuner.hypermodel.build(best_hps)  # Build the model with best hyperparameters\n",
    "        \n",
    "#         tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=f\"logs/attention_training{version}/model{i}\")\n",
    "#         generate_text_model.fit(train_tfds,  epochs=250,\n",
    "#          batch_size=batch_size, validation_data = dev_tfds, verbose =1, callbacks=[tensorboard_callback, text_generation_callback]) \n",
    "\n",
    "\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference using different techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy sampling approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = keras_nlp.samplers.GreedySampler()aaaaaaaaaaaa\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,  # Start sampling immediately after the [BOS] token.\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "print(f\"Greedy search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Search approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = keras_nlp.samplers.BeamSampler(num_beams=10)\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "print(f\"Beam search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = keras_nlp.samplers.RandomSampler()\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "print(f\"Random search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-K search approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = keras_nlp.samplers.TopKSampler(k=10)\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "print(f\"Top-K search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-P search approach\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = keras_nlp.samplers.TopPSampler(p=0.5)\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "print(f\"Top-P search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, o in dataset.take(1):aaaaaaaaaaaaaaaaaa\n",
    "    print(i)\n",
    "    \n",
    "emb_layer = Embedding(input_dim=vocab_len, input_length = window_size, output_dim=emb_dim, weights=[embeddings_matrix] ,name =\"emb_layer\", trainable=False, mask_zero = True)\n",
    "\n",
    "attention_layer = SelfMaskedDotProductAttention()\n",
    "attention_layer_causal1 = SelfMaskedDotProductAttention()\n",
    "#attention_layer_causal2 = SelfMaskedDotProductAttention(causal_mask_enabled=2)\n",
    "\n",
    "emb_layer = Embedding(input_dim=vocab_len, input_length = window_size, output_dim=emb_dim, weights=[embeddings_matrix] ,name =\"emb_layer\", trainable=False, mask_zero = True)\n",
    "query = emb_layer(i)\n",
    "values = emb_layer(i)\n",
    "\n",
    "context_vector, attention_weights = attention_layer(query,values,values, (i != 0))\n",
    "context_vector1, attention_weights1 = attention_layer_causal1(query,values,values, (i != 0))\n",
    "context_vector1, attention_weights_causal = attention_layer_causal1(query,values,values, (i != 0), (1 - tf.linalg.band_part(tf.ones((i.shape[1], i.shape[1])), -1, 0)))\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.subplot(3, 2, 1)\n",
    "plt.pcolormesh(attention_weights[:, 0, :])\n",
    "plt.title('Attention weights - Not causal')\n",
    "\n",
    "plt.subplot(3, 2, 2)\n",
    "plt.pcolormesh(i != 0)\n",
    "plt.title('Mask')\n",
    "\n",
    "plt.subplot(3, 2, 3)\n",
    "plt.pcolormesh(attention_weights1[:, 0, :])\n",
    "plt.title('Attention weights 1')\n",
    "\n",
    "plt.subplot(3, 2, 4)\n",
    "plt.pcolormesh(attention_weights[0])\n",
    "plt.title('Attention weights')\n",
    "\n",
    "plt.subplot(3, 2, 5)\n",
    "plt.pcolormesh(attention_weights1[0])\n",
    "plt.title('Attention weights 1')\n",
    "\n",
    "plt.subplot(3, 2, 5)\n",
    "plt.pcolormesh(attention_weights_causal[0])\n",
    "plt.title('Attention weights causal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blstm_enable = False\n",
    "\n",
    "if TRAIN_AND_SAVE:\n",
    "    model = ModelClass(vocab_len, model_type=1, causal_mask_enabled= 0, blstm_enable=blstm_enable, use_attention=False)\n",
    "    model.compile(loss=MaskedLossCustom(), optimizer=Adam(learning_rate=0.0005), metrics=[tf.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "    model_causal1 = ModelClass(vocab_len, model_type=1, causal_mask_enabled= 1, blstm_enable=blstm_enable, loop= False)\n",
    "    model_causal1.compile(loss=MaskedLossCustom(), optimizer=Adam(learning_rate=0.0005), metrics=[tf.metrics.SparseCategoricalAccuracy()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = vocab_len\n",
    "token_mask_ids = np.array([tokenizer.word_index[x] for x in  [ '<oov>', '[EOS]' ]])[:, None]#\n",
    "token_mask = np.zeros([vocab_size], dtype=np.bool)\n",
    "token_mask[np.array(token_mask_ids)] = True\n",
    "\n",
    "print(\"token_mask\", token_mask)\n",
    "\n",
    "\n",
    "def generate_sentence(model_inst, input_text, max_length=10, temperature=0.5, number_of_candidates = 20, ):\n",
    "    \n",
    "    \n",
    "    attention_per_candidate = []\n",
    "    result_text_per_candidate = []\n",
    "    input_text_per_candidate = []\n",
    "    output_text_per_candidate = []\n",
    "\n",
    "    for c in range(number_of_candidates):\n",
    "\n",
    "        last_state= None\n",
    "        last_activation = None\n",
    "        input_mask = None\n",
    "        last_attention = None\n",
    "\n",
    "        result_tokens = []\n",
    "    \n",
    "        attention = []\n",
    "\n",
    "        x = input_tokens\n",
    "        input_text= []\n",
    "        output_text = []\n",
    "        result_tokens = [w.numpy() for w in  x[0] if w != 0 ]\n",
    "        for k in range(max_length):\n",
    "\n",
    "            input_text.append(' '.join([tokenizer.index_word[w.numpy()] for w in  x[0] if w != 0 ]))\n",
    "            x = pad_sequences(x, padding = \"post\", maxlen = window_size)#\n",
    "\n",
    "            input_mask = x!= 0\n",
    "            t = np.count_nonzero(input_mask)\n",
    "\n",
    "            if no_state: last_state= None\n",
    "\n",
    "            predictions, last_state , attention_weights, last_activation, last_attention  =model_inst(x, last_attention, last_state,  last_activation)\n",
    "\n",
    "            if k > 0 and attention_weights is not None:\n",
    "                attention.append(attention_weights[0])\n",
    "\n",
    "            predicted_logits  = predictions[:,t-1,:]/temperature\n",
    "            predicted_logits = tf.where(token_mask, -np.inf, predicted_logits)\n",
    "            predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "            new_tokens = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "            if loop:\n",
    "            \n",
    "                x = new_tokens.numpy()\n",
    "            else : \n",
    "\n",
    "                x = list(x[0])\n",
    "                x = [i for i in x if i != 0]\n",
    "                x.append(new_tokens.numpy()[0])\n",
    "                \n",
    "\n",
    "            output_text.append(' '.join([tokenizer.index_word[w] for w in  x if w != 0 ][1:]))\n",
    "            x = tf.reshape(tf.convert_to_tensor(x), (len(input_tokens),len(x)))\n",
    "            result_tokens.append(new_tokens.numpy()[0])\n",
    "                \n",
    "            if tokenizer.word_index['[EOS]'] == new_tokens.numpy()[0]:\n",
    "                break\n",
    "\n",
    "\n",
    "        result_text =   ' '.join([tokenizer.index_word[w] for w in  result_tokens if w != 0 ])\n",
    "        \n",
    "        attention_per_candidate.append(attention)\n",
    "        result_text_per_candidate.append(result_text)\n",
    "        input_text_per_candidate.append(input_text)\n",
    "        output_text_per_candidate.append(output_text)\n",
    "\n",
    "    candidate_score, best_score_candidate, best_candidate_index = Minimum_Bayes_Risk(result_text_per_candidate)\n",
    "\n",
    "    attention_stack = attention_per_candidate[best_candidate_index]\n",
    "    result_text = result_text_per_candidate[best_candidate_index]\n",
    "    input_text = input_text_per_candidate[best_candidate_index]\n",
    "    output_text = output_text_per_candidate[best_candidate_index]\n",
    "    \n",
    "    return {'generated_text': result_text, 'attention': attention_stack, \"input_text\" : input_text, \"output_text\" : output_text, \"candidates\": result_text_per_candidate}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_AND_SAVE:\n",
    "    input_sent = \"[BOS] eu quero \"\n",
    "    inp_seq = np.array([tokenizer.word_index[x] for x in input_sent.split()])\n",
    "    print(\"inp_seq -->\",inp_seq)\n",
    "\n",
    "    result = generate_sentence(model_causal1, input_tokens = tf.expand_dims(inp_seq,0),max_length= 5)\n",
    "    print(\"RESULT : \",result[\"generated_text\"])\n",
    "    plot_weights(result, stop_time = 3, loop=True)\n",
    "\n",
    "    result = generate_sentence(model_causal1, input_tokens = tf.expand_dims(inp_seq,0),max_length= 5, loop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_AND_SAVE:\n",
    "    model.use_tf_function = False\n",
    "    for i, o in dataset.take(2):\n",
    "        print(model.train_step([i, o]))\n",
    "        print(model.test_step([i, o]))\n",
    "    print()\n",
    "\n",
    "    model_causal1.use_tf_function = False\n",
    "    for i, o in dataset.take(2):\n",
    "        print(model_causal1.train_step([i, o]))\n",
    "        print(model_causal1.test_step([i, o]))\n",
    "    print()\n",
    "\n",
    "    print(\"\\n---------------------\\n\")\n",
    "\n",
    "    model.use_tf_function = True\n",
    "    for i, o in dataset.take(2):\n",
    "        print(model.train_step([i, o]))\n",
    "        print(model.test_step([i, o]))\n",
    "    print()\n",
    "\n",
    "    model_causal1.use_tf_function = True\n",
    "    for i, o in dataset.take(2):\n",
    "        print(model_causal1.train_step([i, o]))\n",
    "        print(model_causal1.test_step([i, o]))\n",
    "\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_AND_SAVE:\n",
    "    \n",
    "    history = model.fit(dataset, epochs=epochs, shuffle= True,#6000\n",
    "                        steps_per_epoch = train_size, callbacks=[PlotLossesCallback()], validation_data=val_dataset, validation_steps=10)\n",
    "    model.save_weights(\"./checkpoints/model_ckpt\")\n",
    "\n",
    "    with open('./checkpoints/history.pickle', 'wb') as handle:\n",
    "        pickle.dump(history.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "else :\n",
    "    print(\"Loading saved data ...\")\n",
    "    model = ModelClass(vocab_len, model_type=1, causal_mask_enabled= 0, blstm_enable=blstm_enable, use_attention=False)\n",
    "    model.load_weights(\"./checkpoints/model_ckpt\")\n",
    "\n",
    "    with open('./checkpoints/history.pickle', 'rb') as handle:\n",
    "        history = pickle.load(handle)\n",
    "        plt.figure()\n",
    "        plt.plot(history['sparse_categorical_accuracy'])\n",
    "        plt.plot(history['val_sparse_categorical_accuracy'])\n",
    "        plt.title('Model accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(history['loss'])\n",
    "        plt.plot(history['val_loss'])\n",
    "        plt.title('Model training loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(history['perplexity'])\n",
    "        plt.plot(history['val_perplexity'])\n",
    "        plt.title('Model perplexity ')\n",
    "        plt.ylabel('Perplexity')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_AND_SAVE:\n",
    "    \n",
    "    history_causal1 = model_causal1.fit(dataset, epochs=epochs+100, shuffle= True, #6000\n",
    "                        steps_per_epoch = train_size, callbacks=[PlotLossesCallback()], validation_data=val_dataset, validation_steps=10)\n",
    "    model_causal1.save_weights(\"./checkpoints/model_causal1_ckpt\")\n",
    "\n",
    "    with open('./checkpoints/history_causal1.pickle', 'wb') as handle:\n",
    "        pickle.dump(history_causal1.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "else :\n",
    "    print(\"Loading saved data ...\")\n",
    "    model_causal1 = ModelClass(vocab_len, model_type=1, causal_mask_enabled= 1, blstm_enable=blstm_enable, loop= False)\n",
    "\n",
    "    model_causal1.load_weights(\"./checkpoints/model_causal1_ckpt\")\n",
    "\n",
    "    with open('./checkpoints/history_causal1.pickle', 'rb') as handle:\n",
    "        history_causal1 = pickle.load(handle)\n",
    "        plt.figure()\n",
    "        plt.plot(history_causal1['sparse_categorical_accuracy'])\n",
    "        plt.plot(history_causal1['val_sparse_categorical_accuracy'])\n",
    "        plt.title('Model accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(history_causal1['loss'])\n",
    "        plt.plot(history_causal1['val_loss'])\n",
    "        plt.title('Model training loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(history_causal1['perplexity'])\n",
    "        plt.plot(history_causal1['val_perplexity'])\n",
    "        plt.title('Model perplexity ')\n",
    "        plt.ylabel('Perplexity')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, padding_mask= None, look_ahead_mask= None):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead)\n",
    "  but it must be broadcastable for addition.\n",
    "\n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable\n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "  if padding_mask is not None:\n",
    "    scaled_attention_logits += (padding_mask * -1e9)\n",
    "\n",
    "  if look_ahead_mask is not None:\n",
    "    scaled_attention_logits += (look_ahead_mask * -1e9)\n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights\n",
    "\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self,*, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.wq = Dense(d_model)\n",
    "    self.wk = Dense(d_model)\n",
    "    self.wv = Dense(d_model)\n",
    "\n",
    "    self.dense = Dense(d_model)\n",
    "\n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, v, k, q, padding_mask, look_ahead_mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "\n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention( q, k, v, padding_mask, look_ahead_mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention,(batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "n_vectors = 150\n",
    "d_model = 256\n",
    "\n",
    "query = tf.random.uniform((batch_size, n_vectors, d_model), dtype=tf.float32)\n",
    "key = tf.random.uniform((batch_size, n_vectors, d_model), dtype=tf.float32)\n",
    "value = tf.random.uniform((batch_size, n_vectors, d_model), dtype=tf.float32)\n",
    "query.shape, key.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dot product attention\n",
    "x,_ = scaled_dot_product_attention(query, key, key, None)\n",
    "print(f\"Output from dot product attention: {x.shape}\")\n",
    "\n",
    "att = SelfMaskedDotProductAttention()\n",
    "x1, _ = att(query, key,key)\n",
    "#x = tf.concat(x, -1)\n",
    "print(f\"Output from dot product attention: {x1.shape}\")\n",
    "np.where((x-x1) > 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multi-head attention\n",
    "mh_layer = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "x, _ = mh_layer(query, key, value, None, None)\n",
    "print(f\"Output from multi-head attention: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff): #Point wise feed forward network consists of two fully-connected layers with a ReLU activation in between.\n",
    "  return tf.keras.Sequential([\n",
    "      Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])\n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self,*, d_model, num_heads, dff, rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    self.dropout1 = Dropout(rate)\n",
    "    self.dropout2 = Dropout(rate)\n",
    "\n",
    "  def call(self, x, training, padding_mask= None, look_ahead_mask = None):\n",
    "    attn_output, attn_weights_block1  = self.mha(x, x, x, padding_mask, look_ahead_mask)  # (batch_size, input_seq_len, d_model)\n",
    "    attn_output = self.dropout1(attn_output, training=training)\n",
    "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    return out2, attn_weights_block1\n",
    "\n",
    "\n",
    "sample_encoder_layer = DecoderLayer(d_model=512, num_heads=8, dff=batch_size)\n",
    "sample_encoder_layer_output,attn_weights_block1 = sample_encoder_layer(tf.random.uniform((64, 43, 512)), False, None)\n",
    "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "  pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "n, d = 2048, vocab_len\n",
    "pos_encoding = positional_encoding(n, d)\n",
    "print(pos_encoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self,*, num_layers, d_model, embeddings_matrix, window_size, num_heads, dff, vocab_len,\n",
    "               rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.emb_layer = Embedding(input_dim=vocab_len, input_length = window_size, output_dim=d_model, weights=[embeddings_matrix] ,name =\"emb_layer2\", trainable=False)#, mask_zero = True\n",
    "    self.pos_encoding = positional_encoding(window_size, self.d_model)\n",
    "\n",
    "    self.dec_layers = [DecoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate) for _ in range(num_layers)]\n",
    "\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "  def call(self, x, training, padding_mask =None, look_ahead_mask= None):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    attention_weights = {}\n",
    "\n",
    "    x = self.emb_layer(x)  # (batch_size, input_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "    x = self.dropout(x, training=training)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x, block1 = self.dec_layers[i](x, training, padding_mask, look_ahead_mask)\n",
    "      attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "\n",
    "\n",
    "    return x ,attention_weights # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "sample_decoder = Decoder(num_layers=2, d_model=emb_dim, embeddings_matrix = embeddings_matrix, window_size= window_size, num_heads=10,\n",
    "                         dff=2048, vocab_len=vocab_len)\n",
    "\n",
    "for i, o in dataset.take(1):\n",
    "  print(i.shape, o.shape)\n",
    "  output, attn = sample_decoder(i, training=True, padding_mask =None, look_ahead_mask= None)\n",
    "  \n",
    "output.shape, attn['decoder_layer2_block1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(tf.keras.losses.Loss):\n",
    "  def __init__(self):\n",
    "    self.name = 'masked_loss_transformer_custom'\n",
    "    self.loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "  def __call__(self, real, pred):\n",
    "      \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = self.loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "  real = tf.cast(real, dtype=tf.int32)\n",
    "  accuracies = tf.equal(real, tf.cast(tf.argmax(pred, axis=2), dtype=tf.int32))\n",
    "\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  accuracies = tf.math.logical_and(mask, accuracies)\n",
    "\n",
    "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "  mask = tf.cast(mask, dtype=tf.float32)\n",
    "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)\n",
    "  \n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.xlabel('Train Step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,*, num_layers, d_model,embeddings_matrix,window_size, num_heads, dff,  vocab_len,rate=0.1, use_tf_function= False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.decoder = Decoder(num_layers=num_layers, d_model=d_model, embeddings_matrix = embeddings_matrix,\n",
    "                            window_size= window_size, num_heads=num_heads, dff=dff, vocab_len=vocab_len, rate=rate)\n",
    "\n",
    "        self.final_layer = Dense(vocab_len)\n",
    "        self.use_tf_function = use_tf_function\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        # Keras models prefer if you pass all your inputs in the first argument\n",
    "        inp = inputs\n",
    "\n",
    "        padding_mask, look_ahead_mask = self.create_masks(inp)\n",
    "        \n",
    "        # dec_output.shape == (batch_size, inp_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(inp, training, padding_mask, look_ahead_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, vocab_len)\n",
    "\n",
    "        return final_output, attention_weights\n",
    "\n",
    "    def create_masks(self, inp):\n",
    "        # Encoder padding mask (Used in the 2nd attention block in the decoder too.)\n",
    "        \n",
    "        padding_mask = self.create_padding_mask(inp)\n",
    "\n",
    "        # Used in the 1st attention block in the decoder.\n",
    "        # It is used to pad and mask future tokens in the input received by\n",
    "        # the decoder.\n",
    "        look_ahead_mask = self.create_look_ahead_mask(tf.shape(inp)[1])\n",
    "\n",
    "        return padding_mask, look_ahead_mask\n",
    "\n",
    "    def create_padding_mask(self,seq):\n",
    "        seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "        # add extra dimensions to add the padding\n",
    "        # to the attention logits.\n",
    "        return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "    def create_look_ahead_mask(self, size):\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "        return mask  # (seq_len, seq_len)\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        if self.use_tf_function:\n",
    "            return self._tf_train_step(inputs)\n",
    "        else:\n",
    "            return self._train_step(inputs)\n",
    "\n",
    "    @tf.function(input_signature=[[tf.TensorSpec(dtype=tf.int32, shape=[None, window_size]),\n",
    "                                tf.TensorSpec(dtype=tf.int32, shape=[None, window_size])]])\n",
    "                                \n",
    "    def _tf_train_step(self, inputs):\n",
    "        return self._train_step(inputs)\n",
    "\n",
    "\n",
    "\n",
    "    def test_step(self, inputs):\n",
    "        if self.use_tf_function:\n",
    "            return self._tf_test_step(inputs)\n",
    "        else:\n",
    "            return self._test_step(inputs)\n",
    "\n",
    "\n",
    "    @tf.function(input_signature=[[tf.TensorSpec(dtype=tf.int32, shape=[None, window_size]),\n",
    "                                tf.TensorSpec(dtype=tf.int32, shape=[None, window_size])]])\n",
    "    def _tf_test_step(self, inputs):\n",
    "        return self._test_step(inputs)\n",
    "\n",
    "\n",
    "    def _train_step(self, inputs):\n",
    "        inp, tar = inputs\n",
    "        target_mask = tar != 0     \n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            acc = []\n",
    "            loss = tf.constant(0.0)\n",
    "\n",
    "            predictions, _ = self.call(inp,training = True)\n",
    "            loss += self.loss(tar, predictions)\n",
    "            self.compiled_metrics.update_state(tar, predictions)\n",
    "\n",
    "            acc.append(self.metrics[0].result())\n",
    "\n",
    "            average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))  \n",
    "            average_acc  = tf.reduce_mean(tf.convert_to_tensor(acc) )\n",
    "            average_perplexity  = tf.reduce_mean(tf.convert_to_tensor(perplexity(tar, predictions) ))\n",
    "            average_cc  = tf.reduce_mean(tf.convert_to_tensor(accuracy_function(tf.cast(tar, tf.int32), predictions) ))\n",
    "\n",
    "        variables = self.trainable_variables \n",
    "        gradients = tape.gradient(average_loss, variables)\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "        m = {m.name: average_acc  for m in self.metrics} #.update({'loss': batch_loss})m.result() \n",
    "        m[\"loss\"] = average_loss\n",
    "        m[\"perplexity\"] = average_perplexity\n",
    "        m[\"custom_acc\"] = average_cc\n",
    "\n",
    "        return m\n",
    "\n",
    "    def _test_step(self, inputs):\n",
    "        inp, tar = inputs\n",
    "        target_mask = tar != 0     \n",
    "\n",
    "        acc = []\n",
    "        loss = tf.constant(0.0)\n",
    "\n",
    "        predictions, _ = self.call(inp,training = True)\n",
    "        loss += self.loss(tar, predictions)\n",
    "        self.compiled_metrics.update_state(tar, predictions)\n",
    "\n",
    "        acc.append(self.metrics[0].result())\n",
    "\n",
    "        average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))  \n",
    "        average_acc  = tf.reduce_mean(tf.convert_to_tensor(acc) )\n",
    "        average_perplexity  = tf.reduce_mean(tf.convert_to_tensor(perplexity(tar, predictions) ))\n",
    "        average_cc  = tf.reduce_mean(tf.convert_to_tensor(accuracy_function(tf.cast(tar, tf.int32), predictions) ))\n",
    "\n",
    "        m = {m.name: average_acc  for m in self.metrics} #.update({'loss': batch_loss})m.result() \n",
    "        m[\"loss\"] = average_loss\n",
    "        m[\"perplexity\"] = average_perplexity\n",
    "        m[\"custom_acc\"] = average_cc\n",
    "\n",
    "        return m\n",
    "\n",
    "\n",
    "transformer = Transformer(\n",
    "    num_layers=4,\n",
    "    d_model=emb_dim,\n",
    "    embeddings_matrix = embeddings_matrix,\n",
    "    window_size = window_size,\n",
    "    num_heads=10,\n",
    "    dff=256,\n",
    "    vocab_len=vocab_len,\n",
    "    rate=0.1)\n",
    "\n",
    "for i, o in dataset.take(1):\n",
    "    print(i.shape, o.shape)\n",
    "    fn_out, _ = transformer(i, training=False)\n",
    "\n",
    "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "transformer.compile(loss=CustomLoss(), optimizer=optimizer, metrics=[tf.metrics.SparseCategoricalAccuracy()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_sentence_for_transformer(transformer_inst, input_tokens, max_length=10, temperature=1, number_of_candidates = 20, loop= False):\n",
    "    \n",
    "    attention_per_candidate = []\n",
    "    result_text_per_candidate = []\n",
    "    input_text_per_candidate = []\n",
    "    output_text_per_candidate = []\n",
    "\n",
    "    for c in range(number_of_candidates):\n",
    "\n",
    "        input_mask = None\n",
    "        result_tokens = []\n",
    "        attention = []\n",
    "\n",
    "        x = input_tokens\n",
    "        input_text= []\n",
    "        output_text = []\n",
    "        result_tokens = [w.numpy() for w in  x[0] if w != 0 ]\n",
    "        for k in range(max_length):\n",
    "\n",
    "            input_text.append(' '.join([tokenizer.index_word[w.numpy()] for w in  x[0] if w != 0 ]))\n",
    "            x = pad_sequences(x, padding = \"post\", maxlen = window_size)#window_size\n",
    "\n",
    "            input_mask = x!= 0\n",
    "            t = np.count_nonzero(input_mask)\n",
    "\n",
    "            predictions, attention_weights  =transformer_inst(x, training=False)\n",
    "\n",
    "            if k > 0 and attention_weights is not None:\n",
    "                attention.append(attention_weights)\n",
    "\n",
    "            predicted_logits  = predictions[:,t-1,:]/temperature\n",
    "            predicted_logits = tf.where(token_mask, -np.inf, predicted_logits)\n",
    "            predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "            new_tokens = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "            if loop:\n",
    "                x = new_tokens.numpy()\n",
    "            else : \n",
    "\n",
    "                x = list(x[0])\n",
    "                x = [i for i in x if i != 0]\n",
    "                x.append(new_tokens.numpy()[0])\n",
    "                \n",
    "\n",
    "            output_text.append(' '.join([tokenizer.index_word[w] for w in  x if w != 0 ][1:]))\n",
    "            x = tf.reshape(tf.convert_to_tensor(x), (len(input_tokens),len(x)))\n",
    "            result_tokens.append(new_tokens.numpy()[0])\n",
    "                \n",
    "            if tokenizer.word_index['[EOS]'] == new_tokens.numpy()[0]:\n",
    "                break\n",
    "\n",
    "        result_text =   ' '.join([tokenizer.index_word[w] for w in  result_tokens if w != 0 ])\n",
    "        \n",
    "        attention_per_candidate.append(attention)\n",
    "        result_text_per_candidate.append(result_text)\n",
    "        input_text_per_candidate.append(input_text)\n",
    "        output_text_per_candidate.append(output_text)\n",
    "        \n",
    "\n",
    "\n",
    "    candidate_score, best_score_candidate, best_candidate_index = Minimum_Bayes_Risk(result_text_per_candidate)\n",
    "\n",
    "    attention_stack = attention_per_candidate[best_candidate_index]\n",
    "    result_text = result_text_per_candidate[best_candidate_index]\n",
    "    input_text = input_text_per_candidate[best_candidate_index]\n",
    "    output_text = output_text_per_candidate[best_candidate_index]\n",
    "    \n",
    "    return {'generated_text': result_text, 'attention': attention_stack, \"input_text\" : input_text, \"output_text\" : output_text, \"candidates\": result_text_per_candidate}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_AND_SAVE_TRANFORMER:\n",
    "    transformer.use_tf_function = False\n",
    "    for i, o in dataset.take(2):\n",
    "        print(transformer.train_step([i, o]))\n",
    "        print(transformer.test_step([i, o]))\n",
    "    print()\n",
    "\n",
    "\n",
    "    transformer.use_tf_function = True\n",
    "    for i, o in dataset.take(2):\n",
    "        print(transformer.train_step([i, o]))\n",
    "        print(transformer.test_step([i, o]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_AND_SAVE_TRANFORMER:\n",
    "    history_transformer = transformer.fit(dataset, epochs=epochs+50, shuffle= True,#6000\n",
    "                            steps_per_epoch = train_size, callbacks=[PlotLossesCallback()], validation_data=val_dataset, validation_steps=10)\n",
    "    transformer.save_weights(\"./checkpoints/transformer_ckpt\")\n",
    "\n",
    "    with open('./checkpoints/history_transformer.pickle', 'wb') as handle:\n",
    "        pickle.dump(history_transformer.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    print(\"Loading saved data ...\")\n",
    "    transformer = Transformer( num_layers=4, d_model=emb_dim, embeddings_matrix = embeddings_matrix, window_size = window_size, num_heads=10, dff=256, vocab_len=vocab_len, rate=0.1)\n",
    "    transformer.load_weights(\"./checkpoints/transformer_ckpt\")\n",
    "\n",
    "    with open('./checkpoints/history_transformer.pickle', 'rb') as handle:\n",
    "        history_transformer = pickle.load(handle)\n",
    "        plt.figure()\n",
    "        plt.plot(history_transformer['sparse_categorical_accuracy'])\n",
    "        plt.plot(history_transformer['val_sparse_categorical_accuracy'])\n",
    "        plt.title('Model accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(history_transformer['loss'])\n",
    "        plt.plot(history_transformer['val_loss'])\n",
    "        plt.title('Model training loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(history_transformer['perplexity'])\n",
    "        plt.plot(history_transformer['val_perplexity'])\n",
    "        plt.title('Model perplexity ')\n",
    "        plt.ylabel('Perplexity')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(history_transformer['custom_acc'])\n",
    "        plt.plot(history_transformer['val_custom_acc'])\n",
    "        plt.title('Model Custom acc')\n",
    "        plt.ylabel('Custom acc')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, o in val_dataset.take(1):\n",
    "    print(i[0])\n",
    "    print(' '.join([tokenizer.index_word[x.numpy()] for x in i[0] if x.numpy() != 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sent = \"[BOS] no inverno , as familias necessitadas\"\n",
    "\n",
    "#tokenizer.texts_to_sequences(\"[BOS] o rato roeu a roupa do rei de roma \")\n",
    "#inp_seq = np.array([tokenizer.word_index[x] for x in input_sent.split()])\n",
    "\n",
    "inp_seq = np.array([ w[0] for w in  tokenizer.texts_to_sequences(input_sent.split(\" \")) if len(w)>0])\n",
    "temp = 0.9\n",
    "candidates_n = 50\n",
    "max_len = 30\n",
    "\n",
    "inp_seq#, tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE_TEXT:\n",
    "    result1 = generate_sentence(model, input_tokens = tf.expand_dims(inp_seq,0), max_length= max_len, temperature= temp , number_of_candidates=candidates_n, no_state = False)\n",
    "    result2 = generate_sentence(model, input_tokens = tf.expand_dims(inp_seq,0), max_length= max_len, temperature= temp , number_of_candidates=candidates_n , loop= True)\n",
    "\n",
    "    print(\"RESULT WITH STATE : \",result1[\"generated_text\"],\"\\n\")\n",
    "    print(\"RESULT WITH STATE AND LOOP : \",result2[\"generated_text\"],\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention with causal mask on upper triangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE_TEXT:\n",
    "    result = generate_sentence(model_causal1, input_tokens = tf.expand_dims(inp_seq,0), max_length= max_len, temperature= temp , number_of_candidates=candidates_n, no_state = True)\n",
    "    result2 = generate_sentence(model_causal1, input_tokens = tf.expand_dims(inp_seq,0), max_length= max_len, temperature= temp , number_of_candidates=candidates_n , loop= True)\n",
    "\n",
    "    plot_weights(result, stop_time = 3, loop = False)\n",
    "    print(\"RESULT NO STATE : \",result[\"generated_text\"],\"\\n\")\n",
    "    print(\"RESULT WITH STATE AND LOOP : \",result2[\"generated_text\"],\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer with causal mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE_TEXT:\n",
    "    result = generate_sentence_for_transformer(transformer, input_tokens = tf.expand_dims(inp_seq,0), max_length= max_len, temperature= temp , number_of_candidates=candidates_n, )\n",
    "    result1 = generate_sentence_for_transformer(transformer, input_tokens = tf.expand_dims(inp_seq,0), max_length= max_len, temperature= temp , number_of_candidates=candidates_n , loop= True)\n",
    "    print(\"RESULT  : \",result[\"generated_text\"],\"\\n\")\n",
    "    print(\"RESULT WITH  LOOP : \",result1[\"generated_text\"],\"\\n\")\n",
    "\n",
    "    t = -1\n",
    "    plt.figure()\n",
    "    plot_attention(result['attention'][t][\"decoder_layer4_block1\"][0][-1],  result[\"output_text\"][t],  result[\"output_text\"][t])#input_text\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "TextGenerator-In progress.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "dffee67a034c262da22ecd866efe5fa1174a6876e1e96ed512a9acb5833498b8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('tf_cpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
