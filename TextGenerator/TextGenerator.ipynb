{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8048,
     "status": "ok",
     "timestamp": 1595880019461,
     "user": {
      "displayName": "yaniel barbosa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjeEwoPVbOEW9Pwf_Z5Q-LyxKX0GvQrJ-S-pfKh=s64",
      "userId": "13057870575473273651"
     },
     "user_tz": -60
    },
    "id": "eR2oejU7JhSE",
    "outputId": "b803b1cb-a567-4b08-84de-09862d595617"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 15:32:07.355042: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-22 15:32:07.413395: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-22 15:32:07.418559: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re #regex\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Dense, LayerNormalization, Dropout,  LSTM, Dense, Embedding, Bidirectional,BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import io\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "import random\n",
    "\n",
    "from rouge import Rouge #https://towardsdatascience.com/introduction-to-text-summarization-with-rouge-scores-84140c64b471\n",
    "\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from transformers import  RobertaTokenizer, TFRobertaModel\n",
    "\n",
    "\n",
    "print(\"Num GPUs Available: \", (tf.config.experimental.list_physical_devices()))\n",
    "\n",
    "ROUGE = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras-nlp\n",
    "import keras_nlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8007,
     "status": "ok",
     "timestamp": 1595880019463,
     "user": {
      "displayName": "yaniel barbosa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjeEwoPVbOEW9Pwf_Z5Q-LyxKX0GvQrJ-S-pfKh=s64",
      "userId": "13057870575473273651"
     },
     "user_tz": -60
    },
    "id": "CBl11TrADX6-"
   },
   "outputs": [],
   "source": [
    "buffer_size = 10000\n",
    "batch_size= 256\n",
    "window_size= 40 \n",
    "num_epoch =20000\n",
    "epochs= 150\n",
    "embedding_dim = 512\n",
    "\n",
    "TRAIN_AND_SAVE = True\n",
    "TRAIN_AND_SAVE_TRANFORMER = True\n",
    "\n",
    "TRAIN_TEXT= True\n",
    "SEED=42\n",
    "TRAIN_TOKEN = True\n",
    "GENERATE_TEXT= True\n",
    "\n",
    "\n",
    "translationTable = str.maketrans(\"áéíóúàèìòùâêîôûãõç\", \"aeiouaeiouaeiouaoc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_CHECKPOINTS_DIR = './checkpoints'\n",
    "tfrecord_filename = \"train_tmp.tfrecord\"\n",
    "train_tmp_record_path = f'{DATA_CHECKPOINTS_DIR}/{tfrecord_filename}'\n",
    "\n",
    "!mkdir -p {DATA_CHECKPOINTS_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(path_to_file ='./nietzsche.txt'):\n",
    "    path_to_file = \"./eca_de_queiros_os_maias.txt\"\n",
    "    \n",
    "    with io.open(path_to_file, encoding='utf-8') as f:\n",
    "        text = f.read().lower()\n",
    "\n",
    "    print('corpus length:', len(text))\n",
    "    text = text.translate(translationTable)\n",
    "\n",
    "    string_check= re.compile('[^a-zA-Z.?!,:\\'<>]')\n",
    "\n",
    "    text = re.sub(string_check, ' ', (text)\n",
    "                        .replace(\"-\",\" \")\n",
    "                        .replace(\"´\",\"'\")\n",
    "                        .replace(\"`\",\"'\")\n",
    "                        .replace(\",\",\" , \")\n",
    "                        .replace(\"s.\",\" sao \")\n",
    "                        .replace(\"d.\",\"don\")\n",
    "                        .replace(\"v.\",\"v\")\n",
    "                        .replace(\"sr.\", \"senhor\")\n",
    "                        .replace(\"sra.\", \"senhora\")\n",
    "                        .replace(\"exmo.\", \"exmo\")\n",
    "                        .replace(\"exma.\", \"exma\")\n",
    "                        .replace(\"x.\", \"x\")\n",
    "    )\n",
    "\n",
    "    text = re.sub(' +', ' ',text)\n",
    "    text = (re.sub('\\.+', \" . [EOS]  [BOS]\", text).replace(\"?\",\" ? [EOS]  [BOS]\").replace(\"!\",\" !  [EOS]  [BOS]\").replace(\":\",\" : \").replace(\"<br />\",\" \"))\n",
    "\n",
    "    lines_list = list()\n",
    "\n",
    "    text = \"[BOS] \" +text\n",
    "    lines_list = text.split(\"[EOS]  [BOS]\")\n",
    "\n",
    "    for i in range(len(lines_list)):\n",
    "        if i == 0:\n",
    "            lines_list[i] += \" [BOS] \"\n",
    "        else:\n",
    "            lines_list[i] = \"[BOS] \"+ lines_list[i]+\" [EOS]\"\n",
    "\n",
    "    cols = ['sentences']\n",
    "    df_tmp= pd.DataFrame(columns=cols)\n",
    "    df_tmp[\"sentences\"] = lines_list\n",
    "\n",
    "    return df_tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 1266856\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;sos&gt; a casa que vieram habitar em lisboa , no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;sos&gt;  apesar deste fresco nome de vivenda cam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;sos&gt;  longos anos o ramalhete permanecera des...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;sos&gt;  em monsenhor bucarini , nuncio de sao s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;sos&gt;  este inutil pardieiro como lhe chamava ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14586</th>\n",
       "      <td>&lt;sos&gt;  e foi em carlos e em joao da ega uma es...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14587</th>\n",
       "      <td>&lt;sos&gt;  ainda o apanhamos !   &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14588</th>\n",
       "      <td>&lt;sos&gt;  de novo a lanterna deslizou , e fugiu ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14589</th>\n",
       "      <td>&lt;sos&gt;  entao , para apanhar o americano , os d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14590</th>\n",
       "      <td>&lt;sos&gt;  &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14591 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentences\n",
       "0      <sos> a casa que vieram habitar em lisboa , no...\n",
       "1      <sos>  apesar deste fresco nome de vivenda cam...\n",
       "2      <sos>  longos anos o ramalhete permanecera des...\n",
       "3      <sos>  em monsenhor bucarini , nuncio de sao s...\n",
       "4      <sos>  este inutil pardieiro como lhe chamava ...\n",
       "...                                                  ...\n",
       "14586  <sos>  e foi em carlos e em joao da ega uma es...\n",
       "14587                 <sos>  ainda o apanhamos !   <eos>\n",
       "14588  <sos>  de novo a lanterna deslizou , e fugiu ....\n",
       "14589  <sos>  entao , para apanhar o americano , os d...\n",
       "14590                                       <sos>  <eos>\n",
       "\n",
       "[14591 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "19343"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'calco',\n",
       " 'aqueles',\n",
       " 'sobressaiam',\n",
       " 'delicadas',\n",
       " 'tenebrosa',\n",
       " 'morta',\n",
       " 'sincero',\n",
       " 'desfeiassem',\n",
       " 'roubada',\n",
       " 'loirinhas',\n",
       " 'declaravam',\n",
       " 'sarcasmos',\n",
       " 'assustados',\n",
       " 'rezes',\n",
       " 'amiudadas',\n",
       " 'veludos',\n",
       " 'arrefecia',\n",
       " 'retorce',\n",
       " 'corredor',\n",
       " 'fila',\n",
       " 'desabitado',\n",
       " 'lampejava',\n",
       " 'retido',\n",
       " 'pruderie',\n",
       " 'molossos',\n",
       " 'scriptum',\n",
       " 'desata',\n",
       " 'pau',\n",
       " 'firmeza',\n",
       " 'instrucaosinha',\n",
       " 'conversao',\n",
       " 'trepar',\n",
       " 'levados',\n",
       " 'precisava',\n",
       " 'cornija',\n",
       " 'catando',\n",
       " 'heresia',\n",
       " 'dediquei',\n",
       " 'facadas',\n",
       " 'usei',\n",
       " 'envernizados',\n",
       " 'cravasse',\n",
       " 'tumidos',\n",
       " 'italicos',\n",
       " 'destacavam',\n",
       " 'bolo',\n",
       " 'atirou',\n",
       " 'fugido',\n",
       " 'decide',\n",
       " 'moderacao',\n",
       " 'rolava',\n",
       " 'golgota',\n",
       " 'foi',\n",
       " 'pardais',\n",
       " 'conheceo',\n",
       " 'verso',\n",
       " 'perle',\n",
       " 'atirar',\n",
       " 'falo',\n",
       " 'tenro',\n",
       " 'embasbacado',\n",
       " 'relembrando',\n",
       " 'segredou',\n",
       " 'prostrada',\n",
       " 'ab',\n",
       " 'libreto',\n",
       " 'encarregado',\n",
       " 'jantavam',\n",
       " 'sentissem',\n",
       " 'renovado',\n",
       " 'lajes',\n",
       " 'enredar',\n",
       " 'madalena',\n",
       " 'conservado',\n",
       " 'burros',\n",
       " 'estrago',\n",
       " 'rica',\n",
       " 'caira',\n",
       " 'complicou',\n",
       " 'ordenou',\n",
       " 'polainas',\n",
       " 'fraternidade',\n",
       " 'principio',\n",
       " 'espalhafato',\n",
       " 'vomitado',\n",
       " 'entusiasta',\n",
       " 'apareceram',\n",
       " 'meditava',\n",
       " 'rescendendo',\n",
       " 'guerra',\n",
       " 'atabalhoava',\n",
       " 'salomao',\n",
       " 'varrer',\n",
       " 'jornal',\n",
       " 'pareceu',\n",
       " 'trespassasse',\n",
       " 'aparece',\n",
       " 'exacto',\n",
       " 'mugiu',\n",
       " 'cavada',\n",
       " 'fiar',\n",
       " 'ressuscita',\n",
       " 'amar',\n",
       " 'papado',\n",
       " 'pagem',\n",
       " 'recordo',\n",
       " 'pular',\n",
       " 'gloriosos',\n",
       " 'mortico',\n",
       " 'maduro',\n",
       " 'acolhem',\n",
       " 'navalhao',\n",
       " 'imagines',\n",
       " 'emplumado',\n",
       " 'ingles',\n",
       " 'acolheu',\n",
       " 'ocupavam',\n",
       " 'boxa',\n",
       " 'mefisto',\n",
       " 'sobranceria',\n",
       " 'abencoador',\n",
       " 'publicava',\n",
       " 'invalido',\n",
       " 'monaco',\n",
       " 'palhacos',\n",
       " 'dispor',\n",
       " 'insensata',\n",
       " 'capendu',\n",
       " 'silence',\n",
       " 'desapertava',\n",
       " 'avancado',\n",
       " 'escondidos',\n",
       " 'sarrabulhada',\n",
       " 'classico',\n",
       " 'cai',\n",
       " 'agitava',\n",
       " 'menciono',\n",
       " 'transido',\n",
       " 'destinando',\n",
       " 'carvalho',\n",
       " 'reaparece',\n",
       " 'picnic',\n",
       " 'beetoven',\n",
       " 'herbert',\n",
       " 'andando',\n",
       " 'diogo',\n",
       " 'quinto',\n",
       " 'autoridade',\n",
       " 'bochechinha',\n",
       " 'levantada',\n",
       " 'serafim',\n",
       " 'imundo',\n",
       " 'exprimisse',\n",
       " 'cranios',\n",
       " 'juncavam',\n",
       " 'rodeado',\n",
       " 'desonrava',\n",
       " 'carolice',\n",
       " 'tapetada',\n",
       " 'clamamos',\n",
       " 'acesa',\n",
       " 'abominavel',\n",
       " 'roga',\n",
       " 'veneno',\n",
       " 'atacado',\n",
       " 'safira',\n",
       " 'suportar',\n",
       " 'rapada',\n",
       " 'balofa',\n",
       " 'soalho',\n",
       " 'banhava',\n",
       " 'esticado',\n",
       " 'indicou',\n",
       " 'omnipotencia',\n",
       " 'penedos',\n",
       " 'perdue',\n",
       " 'estereoscopicas',\n",
       " 'calhou',\n",
       " 'petiscar',\n",
       " 'semelhancas',\n",
       " 'criara',\n",
       " 'ridiculo',\n",
       " 'feudais',\n",
       " 'nossos',\n",
       " 'poltronas',\n",
       " 'visco',\n",
       " 'brat',\n",
       " 'desterro',\n",
       " 'lagrimas',\n",
       " 'mugia',\n",
       " 'calatrava',\n",
       " 'realizadas',\n",
       " 'portuguesas',\n",
       " 'ala',\n",
       " 'mucios',\n",
       " 'conspirador',\n",
       " 'pilham',\n",
       " 'cantinho',\n",
       " 'trata',\n",
       " 'passeio',\n",
       " 'tan',\n",
       " 'corria',\n",
       " 'enterrava',\n",
       " 'esperarei',\n",
       " 'penetrando',\n",
       " 'encantos',\n",
       " 'merecem',\n",
       " 'cozinheiro',\n",
       " 'afirmar',\n",
       " 'vaos',\n",
       " 'levado',\n",
       " 'pos',\n",
       " 'nacoes',\n",
       " 'tiros',\n",
       " 'desmaiada',\n",
       " 'orgulhosos',\n",
       " 'mascaras',\n",
       " 'posicao',\n",
       " 'monossilabos',\n",
       " 'perfilava',\n",
       " 'fornos',\n",
       " 'esforcado',\n",
       " 'medalhao',\n",
       " 'tolda',\n",
       " 'azedume',\n",
       " 'viseira',\n",
       " 'afigurava',\n",
       " 'devagarinho',\n",
       " 'desoladora',\n",
       " 'presidios',\n",
       " 'igreja',\n",
       " 'batia',\n",
       " 'desenxabida',\n",
       " 'ergueu',\n",
       " 'sossegou',\n",
       " 'canteirinho',\n",
       " 'segurava',\n",
       " 'pinheiral',\n",
       " 'rasgado',\n",
       " 'barao',\n",
       " 'desagradavelmente',\n",
       " 'proteccao',\n",
       " 'carnacao',\n",
       " 'descaiam',\n",
       " 'alegrar',\n",
       " 'imaginamos',\n",
       " 'arte',\n",
       " 'viuvo',\n",
       " 'davamo',\n",
       " 'revolvia',\n",
       " 'falai',\n",
       " 'gago',\n",
       " 'assustador',\n",
       " 'termina',\n",
       " 'semanas',\n",
       " 'viajou',\n",
       " 'dezoito',\n",
       " 'emulacao',\n",
       " 'nocturnos',\n",
       " 'achouse',\n",
       " 'esbaforidamente',\n",
       " 'aparecer',\n",
       " 'desligada',\n",
       " 'volumes',\n",
       " 'ditou',\n",
       " 'alongava',\n",
       " 'magnificos',\n",
       " 'suplicio',\n",
       " 'vitoria',\n",
       " 'unidas',\n",
       " 'sacarei',\n",
       " 'deixavam',\n",
       " 'fertil',\n",
       " 'cavalao',\n",
       " 'absorvido',\n",
       " 'continha',\n",
       " 'melopeia',\n",
       " 'reaparecendo',\n",
       " 'esquecia',\n",
       " 'forradas',\n",
       " 'castigai',\n",
       " 'prudencia',\n",
       " 'sabor',\n",
       " 'desagradavel',\n",
       " 'parlamentar',\n",
       " 'cornetear',\n",
       " 'cavalinhos',\n",
       " 'retiniu',\n",
       " 'fortuny',\n",
       " 'barcarola',\n",
       " 'patetica',\n",
       " 'romaria',\n",
       " 'monarquia',\n",
       " 'exprimir',\n",
       " 'ficara',\n",
       " 'aparentada',\n",
       " 'primas',\n",
       " 'pasmando',\n",
       " 'desafiados',\n",
       " 'adoravel',\n",
       " 'beatriz',\n",
       " 'senhormaia',\n",
       " 'engalfinhados',\n",
       " 'contem',\n",
       " 'insinuacao',\n",
       " 'razao',\n",
       " 'remar',\n",
       " 'mocos',\n",
       " 'chicotada',\n",
       " 'terreiro',\n",
       " 'esbaforido',\n",
       " 'digam',\n",
       " 'tornado',\n",
       " 'vemos',\n",
       " 'inglese',\n",
       " 'camoes',\n",
       " 'ministro',\n",
       " 'estore',\n",
       " 'cevava',\n",
       " 'bambino',\n",
       " 'falsidades',\n",
       " 'assimilar',\n",
       " 'preopinante',\n",
       " 'graca',\n",
       " 'diverso',\n",
       " 'prelado',\n",
       " 'realmente',\n",
       " 'botada',\n",
       " 'aparatosas',\n",
       " 'revistas',\n",
       " 'abafando',\n",
       " 'terminara',\n",
       " 'missoes',\n",
       " 'ventresinho',\n",
       " 'desaparecesse',\n",
       " 'cordeiros',\n",
       " 'indignou',\n",
       " 'submissa',\n",
       " 'poraosinho',\n",
       " 'amigas',\n",
       " 'trago',\n",
       " 'impertinente',\n",
       " 'percebes',\n",
       " 'direi',\n",
       " 'odiando',\n",
       " 'turbar',\n",
       " 'lumiar',\n",
       " 'macios',\n",
       " 'ganhando',\n",
       " 'vieux',\n",
       " 'antigos',\n",
       " 'apostolica',\n",
       " 'oferecimento',\n",
       " 'residencia',\n",
       " 'anhos',\n",
       " 'intenso',\n",
       " 'tombado',\n",
       " 'visiveis',\n",
       " 'atropelavam',\n",
       " 'copas',\n",
       " 'ornar',\n",
       " 'queres',\n",
       " 'crescendo',\n",
       " 'calou',\n",
       " 'examinadore',\n",
       " 'censuro',\n",
       " 'jazia',\n",
       " 'humedecido',\n",
       " 'sabedoria',\n",
       " 'apossando',\n",
       " 'gas',\n",
       " 'favores',\n",
       " 'cadencia',\n",
       " 'ruborizavam',\n",
       " 'pilheria',\n",
       " 'ricado',\n",
       " 'rebuscava',\n",
       " 'verdura',\n",
       " 'pregou',\n",
       " 'sofregos',\n",
       " 'aturdido',\n",
       " 'provava',\n",
       " 'sinalsinho',\n",
       " 'despregar',\n",
       " 'rodando',\n",
       " 'solenemente',\n",
       " 'inclinava',\n",
       " 'grosseiro',\n",
       " 'ondeou',\n",
       " 'traduz',\n",
       " 'debilitante',\n",
       " 'senhor',\n",
       " 'esconde',\n",
       " 'reforma',\n",
       " 'desanuviado',\n",
       " 'ficou',\n",
       " 'serpentinas',\n",
       " 'esbocado',\n",
       " 'soberanos',\n",
       " 'herois',\n",
       " 'espacosa',\n",
       " 'sportman',\n",
       " 'voltava',\n",
       " 'teatral',\n",
       " 'nervosas',\n",
       " 'terminar',\n",
       " 'casados',\n",
       " 'conteve',\n",
       " 'trigueiras',\n",
       " 'consolo',\n",
       " 'autoridades',\n",
       " 'velado',\n",
       " 'perdendo',\n",
       " 'cataplasma',\n",
       " 'secas',\n",
       " 'isca',\n",
       " 'catedral',\n",
       " 'socialismo',\n",
       " 'vigor',\n",
       " 'deitados',\n",
       " 'reunia',\n",
       " 'quedo',\n",
       " 'universal',\n",
       " 'videira',\n",
       " 'solteirona',\n",
       " 'conservasse',\n",
       " 'complacencia',\n",
       " 'provasse',\n",
       " 'francaise',\n",
       " 'aquilino',\n",
       " 'admirara',\n",
       " 'prendera',\n",
       " 'lancou',\n",
       " 'smit',\n",
       " 'entufada',\n",
       " 'nitida',\n",
       " 'messagerie',\n",
       " 'bestialisados',\n",
       " 'altos',\n",
       " 'ficasse',\n",
       " 'resplandeciam',\n",
       " 'acabais',\n",
       " 'meditando',\n",
       " 'esfarrapada',\n",
       " 'silencios',\n",
       " 'render',\n",
       " 'duvidas',\n",
       " 'tostoe',\n",
       " 'domesticas',\n",
       " 'animai',\n",
       " 'moderno',\n",
       " 'aveluda',\n",
       " 'quisesse',\n",
       " 'brilhando',\n",
       " 'cohens',\n",
       " 'afiancava',\n",
       " 'voou',\n",
       " 'patas',\n",
       " 'trilhar',\n",
       " 'bacharel',\n",
       " 'pandoli',\n",
       " 'absolutamente',\n",
       " 'intolerantes',\n",
       " 'caliente',\n",
       " 'acudira',\n",
       " 'degolada',\n",
       " 'olivedos',\n",
       " 'ajoelhe',\n",
       " 'arrepiava',\n",
       " 'examinando',\n",
       " 'opalas',\n",
       " 'boquinha',\n",
       " 'bicho',\n",
       " 'disseram',\n",
       " 'suplicantes',\n",
       " 'fumegou',\n",
       " 'vir',\n",
       " 'praxes',\n",
       " 'chamar',\n",
       " 'celeste',\n",
       " 'adoraveis',\n",
       " 'dezembro',\n",
       " 'lecon',\n",
       " 'austerlitz',\n",
       " 'esgatanhando',\n",
       " 'depois',\n",
       " 'saindo',\n",
       " 'confortavelmente',\n",
       " 'terca',\n",
       " 'jardineira',\n",
       " 'brota',\n",
       " 'rolou',\n",
       " 'xvii',\n",
       " 'cinza',\n",
       " 'assentar',\n",
       " 'antiquado',\n",
       " 'esfregando',\n",
       " 'decisiva',\n",
       " 'catitinha',\n",
       " 'salinha',\n",
       " 'soirees',\n",
       " 'emudecia',\n",
       " 'regeladamente',\n",
       " 'mares',\n",
       " 'relva',\n",
       " 'consentir',\n",
       " 'despido',\n",
       " 'imaterial',\n",
       " 'indignado',\n",
       " 'parando',\n",
       " 'saturado',\n",
       " 'perguntavam',\n",
       " 'condicoes',\n",
       " 'homen',\n",
       " 'recto',\n",
       " 'silenciosas',\n",
       " 'panteismo',\n",
       " 'riscadinho',\n",
       " 'lenha',\n",
       " 'pastora',\n",
       " 'constitucionalismo',\n",
       " 'dorminhoco',\n",
       " 'arroios',\n",
       " 'exacta',\n",
       " 'desengonca',\n",
       " 'pandilhas',\n",
       " 'papistas',\n",
       " 'introduzira',\n",
       " 'vibrando',\n",
       " 'manquejava',\n",
       " 'alvadia',\n",
       " 'aspecto',\n",
       " 'arabe',\n",
       " 'botequins',\n",
       " 'resolviam',\n",
       " 'bonitas',\n",
       " 'formavam',\n",
       " 'ramagens',\n",
       " 'tiro',\n",
       " 'recrescia',\n",
       " 'ociosas',\n",
       " 'fases',\n",
       " 'permitiam',\n",
       " 'dramalhao',\n",
       " 'pintada',\n",
       " 'figado',\n",
       " 'artiguinhos',\n",
       " 'implorar',\n",
       " 'promiscuidade',\n",
       " 'alcool',\n",
       " 'contemplara',\n",
       " 'testa',\n",
       " 'pensara',\n",
       " 'atropelando',\n",
       " 'consolado',\n",
       " 'castanhola',\n",
       " 'caes',\n",
       " 'corre',\n",
       " 'pudicicia',\n",
       " 'encantado',\n",
       " 'esplendida',\n",
       " 'sobrenatural',\n",
       " 'alsaciano',\n",
       " 'claraboia',\n",
       " 'orla',\n",
       " 'dirigiu',\n",
       " 'proclamada',\n",
       " 'atencao',\n",
       " 'recomendou',\n",
       " 'fisica',\n",
       " 'rigidos',\n",
       " 'idilico',\n",
       " 'corando',\n",
       " 'genro',\n",
       " 'rebentava',\n",
       " 'acercava',\n",
       " 'pedacos',\n",
       " 'envidracado',\n",
       " 'durabilidade',\n",
       " 'soltava',\n",
       " 'estragar',\n",
       " 'ye',\n",
       " 'desencaixotadas',\n",
       " 'sorvia',\n",
       " 'encontrasse',\n",
       " 'proporcoes',\n",
       " 'disposto',\n",
       " 'fedia',\n",
       " 'trezentas',\n",
       " 'vadiava',\n",
       " 'ficado',\n",
       " 'beneficios',\n",
       " 'proibido',\n",
       " 'justo',\n",
       " 'madrugada',\n",
       " 'tabernaculo',\n",
       " 'povoacao',\n",
       " 'escompte',\n",
       " 'emaranha',\n",
       " 'treno',\n",
       " 'quinhentos',\n",
       " 'concordou',\n",
       " 'leoes',\n",
       " 'curvas',\n",
       " 'positivamente',\n",
       " 'condessa',\n",
       " 'agostinho',\n",
       " 'escolhendo',\n",
       " 'corrido',\n",
       " 'ceus',\n",
       " 'rudeza',\n",
       " 'frase',\n",
       " 'balofice',\n",
       " 'suspendera',\n",
       " 'viril',\n",
       " 'pequenez',\n",
       " 'incumbido',\n",
       " 'realcada',\n",
       " 'consternada',\n",
       " 'encalhara',\n",
       " 'procura',\n",
       " 'biografia',\n",
       " 'satisfactoriamente',\n",
       " 'beicos',\n",
       " 'brasil',\n",
       " 'ofereceu',\n",
       " 'percebeu',\n",
       " 'invisivel',\n",
       " 'triunfantemente',\n",
       " 'feto',\n",
       " 'afectada',\n",
       " 'elo',\n",
       " 'atroz',\n",
       " 'pontape',\n",
       " 'concordavam',\n",
       " 'tencionava',\n",
       " 'beduino',\n",
       " 'recebe',\n",
       " 'nenhum',\n",
       " 'travassos',\n",
       " 'valachos',\n",
       " 'escorropichando',\n",
       " 'fisionomia',\n",
       " 'conjuntamente',\n",
       " 'grilheta',\n",
       " 'fugida',\n",
       " 'debilidade',\n",
       " 'convidara',\n",
       " 'tomei',\n",
       " 'predios',\n",
       " 'esvaziou',\n",
       " 'apareceste',\n",
       " 'euzebio',\n",
       " 'steinbroken',\n",
       " 'realizavel',\n",
       " 'estupidamente',\n",
       " 'viela',\n",
       " 'olha',\n",
       " 'vicenta',\n",
       " 'soubessem',\n",
       " 'mosqueteira',\n",
       " 'servira',\n",
       " 'rechonchuda',\n",
       " 'escuridao',\n",
       " 'excrementicia',\n",
       " 'feitas',\n",
       " 'tamisa',\n",
       " 'mantilha',\n",
       " 'atiravam',\n",
       " 'povos',\n",
       " 'misericordiosa',\n",
       " 'gravado',\n",
       " 'estragando',\n",
       " 'acre',\n",
       " 'comoda',\n",
       " 'neta',\n",
       " 'voltaste',\n",
       " 'vosse',\n",
       " 'camponesas',\n",
       " 'adocar',\n",
       " 'retros',\n",
       " 'tristonhos',\n",
       " 'hao',\n",
       " 'frisk',\n",
       " 'angouleme',\n",
       " 'acenderam',\n",
       " 'roubar',\n",
       " 'sotao',\n",
       " 'ginastica',\n",
       " 'injustas',\n",
       " 'contentam',\n",
       " 'revelando',\n",
       " 'crasso',\n",
       " 'antepassado',\n",
       " 'directamente',\n",
       " 'tiritando',\n",
       " 'cotilon',\n",
       " 'braseira',\n",
       " 'apontando',\n",
       " 'consiste',\n",
       " 'esquecendo',\n",
       " 'queimar',\n",
       " 'anoitecer',\n",
       " 'continua',\n",
       " 'esperamo',\n",
       " 'compensacoes',\n",
       " 'purificasse',\n",
       " 'comprometer',\n",
       " 'cairia',\n",
       " 'vou',\n",
       " 'rapaziada',\n",
       " 'agueda',\n",
       " 'indeciso',\n",
       " 'ferro',\n",
       " 'inconvenientes',\n",
       " 'intimos',\n",
       " 'puxado',\n",
       " 'materiais',\n",
       " 'turvos',\n",
       " 'acuses',\n",
       " 'barbeiro',\n",
       " 'astro',\n",
       " 'colocava',\n",
       " 'adquirir',\n",
       " 'rosiere',\n",
       " 'robusto',\n",
       " 'convem',\n",
       " 'dados',\n",
       " 'nota',\n",
       " 'mortas',\n",
       " 'armado',\n",
       " 'voces',\n",
       " 'pingava',\n",
       " 'azuis',\n",
       " 'nuven',\n",
       " 'transtornado',\n",
       " 'sombrinhas',\n",
       " 'tenazes',\n",
       " 'trofeus',\n",
       " 'sopro',\n",
       " 'vendera',\n",
       " 'rapariguinha',\n",
       " 'avistam',\n",
       " 'rido',\n",
       " 'frequentara',\n",
       " 'perturbada',\n",
       " 'sabeis',\n",
       " 'dua',\n",
       " 'horas',\n",
       " 'franzindo',\n",
       " 'garret',\n",
       " 'vira',\n",
       " 'regent',\n",
       " 'educasse',\n",
       " 'enternecidamente',\n",
       " 'namorou',\n",
       " 'chaves',\n",
       " 'renda',\n",
       " 'coisa',\n",
       " 'capitulos',\n",
       " 'postar',\n",
       " 'hesitante',\n",
       " 'veras',\n",
       " 'pecunia',\n",
       " 'carroca',\n",
       " 'tesoura',\n",
       " 'saltou',\n",
       " 'admitiu',\n",
       " 'encharcara',\n",
       " 'dava',\n",
       " 'prior',\n",
       " 'eternidade',\n",
       " 'inclinada',\n",
       " 'pisara',\n",
       " 'delas',\n",
       " 'aproveita',\n",
       " 'clergyman',\n",
       " 'convidativo',\n",
       " 'vibracao',\n",
       " 'entraria',\n",
       " 'saira',\n",
       " 'liquidacao',\n",
       " 'perturbadora',\n",
       " 'etait',\n",
       " 'lui',\n",
       " 'evangelho',\n",
       " 'escuro',\n",
       " 'viesse',\n",
       " 'rir',\n",
       " 'salva',\n",
       " 'dandi',\n",
       " 'seguindo',\n",
       " 'passaste',\n",
       " 'cambaleando',\n",
       " 'cao',\n",
       " 'escreveu',\n",
       " 'medonhamente',\n",
       " 'tabela',\n",
       " 'finorio',\n",
       " 'aguentara',\n",
       " 'liquidado',\n",
       " 'benevolencia',\n",
       " 'escoria',\n",
       " 'pulmao',\n",
       " 'dame',\n",
       " 'demorada',\n",
       " 'atribuindo',\n",
       " 'rato',\n",
       " 'baixelas',\n",
       " 'regras',\n",
       " 'mme',\n",
       " 'esperassem',\n",
       " 'preceptor',\n",
       " 'partidos',\n",
       " 'construira',\n",
       " 'novena',\n",
       " 'atreveria',\n",
       " 'merecemo',\n",
       " 'aturar',\n",
       " 'afundar',\n",
       " 'bucelas',\n",
       " 'escondendo',\n",
       " 'desmaiados',\n",
       " 'casada',\n",
       " 'senti',\n",
       " 'corar',\n",
       " 'risadas',\n",
       " 'numa',\n",
       " 'realeza',\n",
       " 'blonde',\n",
       " 'guerreiros',\n",
       " 'barato',\n",
       " 'romances',\n",
       " 'cordao',\n",
       " 'decidida',\n",
       " 'enfeitado',\n",
       " 'acamaradadas',\n",
       " 'articulares',\n",
       " 'amiguinho',\n",
       " 'descomposta',\n",
       " 'habitar',\n",
       " 'campo',\n",
       " 'silencioso',\n",
       " 'ressuscitar',\n",
       " 'plastrao',\n",
       " 'odiava',\n",
       " 'sustos',\n",
       " 'galanteria',\n",
       " 'rezar',\n",
       " 'friorenta',\n",
       " 'capote',\n",
       " 'formas',\n",
       " 'discutindo',\n",
       " 'goza',\n",
       " 'procurado',\n",
       " 'talagarca',\n",
       " 'resplandecer',\n",
       " 'imposturas',\n",
       " 'faze',\n",
       " 'alugavam',\n",
       " 'mono',\n",
       " 'ha',\n",
       " 'singularmente',\n",
       " 'lance',\n",
       " 'enferrujada',\n",
       " 'partir',\n",
       " 'lentidao',\n",
       " 'anunciando',\n",
       " 'espanhol',\n",
       " 'inteligentes',\n",
       " 'rixa',\n",
       " 'trocara',\n",
       " 'cartao',\n",
       " 'vergou',\n",
       " 'inflamam',\n",
       " 'inevitaveis',\n",
       " 'shlegen',\n",
       " 'alfarrabios',\n",
       " 'confusao',\n",
       " 'sentimentalismo',\n",
       " 'humilhacao',\n",
       " 'rajada',\n",
       " 'morgadinho',\n",
       " 'coleccoe',\n",
       " 'marat',\n",
       " 'cleopatra',\n",
       " 'murcho',\n",
       " 'esplendor',\n",
       " 'paisagen',\n",
       " 'taciturnos',\n",
       " 'patetico',\n",
       " 'pires',\n",
       " 'distraccao',\n",
       " 'desabitada',\n",
       " 'escrevinho',\n",
       " 'ajoelhado',\n",
       " 'maioridade',\n",
       " 'escarrar',\n",
       " 'exigiu',\n",
       " 'influencia',\n",
       " 'voltassem',\n",
       " 'despachado',\n",
       " 'voasse',\n",
       " 'vinham',\n",
       " 'gotica',\n",
       " 'lisongeia',\n",
       " 'danada',\n",
       " 'fofos',\n",
       " 'apostava',\n",
       " 'enviuvou',\n",
       " 'tramada',\n",
       " 'repente',\n",
       " 'distingues',\n",
       " 'surgia',\n",
       " 'mes',\n",
       " 'apanhadas',\n",
       " 'embaciados',\n",
       " 'retendo',\n",
       " 'vasando',\n",
       " 'patacos',\n",
       " 'arrombar',\n",
       " 'providenciar',\n",
       " 'apareciam',\n",
       " 'resende',\n",
       " 'bestialisa',\n",
       " 'descoberta',\n",
       " 'delambida',\n",
       " 'nevoas',\n",
       " 'jupiter',\n",
       " 'convencao',\n",
       " 'rocar',\n",
       " 'metia',\n",
       " 'tenhamos',\n",
       " 'sonhos',\n",
       " 'procuravam',\n",
       " 'frequentes',\n",
       " 'seculo',\n",
       " 'servo',\n",
       " 'almocou',\n",
       " 'falsificados',\n",
       " 'fugisse',\n",
       " 'leitura',\n",
       " 'passe',\n",
       " 'escorracado',\n",
       " 'aia',\n",
       " 'livraria',\n",
       " 'entusiasmou',\n",
       " 'arrebatou',\n",
       " 'derramando',\n",
       " 'hamlet',\n",
       " 'negras',\n",
       " 'viola',\n",
       " 'canalha',\n",
       " 'espessos',\n",
       " 'casara',\n",
       " 'funcionario',\n",
       " 'comecou',\n",
       " 'rolos',\n",
       " 'tosca',\n",
       " 'rosse',\n",
       " 'nitidas',\n",
       " 'embuchou',\n",
       " 'abancado',\n",
       " 'toalha',\n",
       " 'rosa',\n",
       " 'corujazinha',\n",
       " 'rocha',\n",
       " 'bendita',\n",
       " 'asperos',\n",
       " 'traduzida',\n",
       " 'entorpecido',\n",
       " 'manchas',\n",
       " 'houve',\n",
       " 'aromas',\n",
       " 'pasmar',\n",
       " 'simoes',\n",
       " 'trasladado',\n",
       " 'pago',\n",
       " 'constrangido',\n",
       " 'repisar',\n",
       " 'galgando',\n",
       " 'oton',\n",
       " 'luva',\n",
       " 'maravilhada',\n",
       " 'ermida',\n",
       " 'vencer',\n",
       " 'tesouro',\n",
       " 'esgana',\n",
       " 'espreitara',\n",
       " 'fundas',\n",
       " 'mail',\n",
       " 'forra',\n",
       " 'enrolavam',\n",
       " 'libertinagem',\n",
       " 'lixo',\n",
       " 'estocada',\n",
       " 'simpatizara',\n",
       " 'crepitando',\n",
       " 'torre',\n",
       " ...]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df= prepare_data()\n",
    "\n",
    "complete_text = ' '.join(df.sentences.values.tolist())\n",
    "complete_unique_words_list = list(set(complete_text.split(' ')))\n",
    "vocab_len = len(complete_unique_words_list)\n",
    "display(df)\n",
    "display(vocab_len)\n",
    "display(complete_unique_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sent_length\"] = df.sentences.apply(lambda x: len(x.split()))\n",
    "\n",
    "plt.figure()\n",
    "pd.Series(df.sent_length).hist()\n",
    "plt.title(\"Sentence Size for training dataset\")\n",
    "df[df.sent_length>window_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.sent_length>=5]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "outputs = [] \n",
    "count = 0\n",
    "for it, sent in enumerate(df.sentences.tolist()):\n",
    "    sentence_lists = sent.split()\n",
    "    if len(sentence_lists) > window_size:\n",
    "        for i in range(0, len(sentence_lists)-window_size,1):\n",
    "            inputs.append(sentence_lists[i: window_size+i])\n",
    "            outputs.append(sentence_lists[i+1: window_size+i+1])\n",
    "            count +=1\n",
    "    else:\n",
    "        inputs.append(sentence_lists[:-1])\n",
    "        outputs.append(sentence_lists[1:])\n",
    "\n",
    "    if len(inputs)>30000  :\n",
    "        break\n",
    "\n",
    "len(inputs), len(outputs), it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(f\"INPUT {i} : \" + ' '.join(inputs[i]))\n",
    "    print(f\"OUTPUT {i} : \" + ' '.join(outputs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_list=['data scientist tasks', 'data scientist jobs','data scientist job','data scientist, and','data engineer tasks','data engineer jobs']\n",
    "\n",
    "def Minimum_Bayes_Risk(list_of_candidates, field = \"rouge-l\", candidate_reference = None):\n",
    "    candidate_score = {}\n",
    "    max_score = 0\n",
    "    best_score_candidate= ''\n",
    "    best_candidate_index = -1\n",
    "    if candidate_reference == None :\n",
    "        for candidate_ref in list_of_candidates:\n",
    "            \n",
    "            rest_of_candidates = copy.deepcopy(list_of_candidates)\n",
    "            if candidate_ref in rest_of_candidates : rest_of_candidates.remove(candidate_ref)\n",
    "            score = 0\n",
    "\n",
    "            for candidate in rest_of_candidates:\n",
    "                score += ROUGE.get_scores(candidate, candidate_ref)[0][field][\"f\"]\n",
    "            \n",
    "            score = score/len(rest_of_candidates)\n",
    "\n",
    "            candidate_score[candidate_ref] = score\n",
    "\n",
    "            if score >= max_score:\n",
    "                best_score_candidate =candidate_ref\n",
    "                best_candidate_index = list_of_candidates.index(candidate_ref)\n",
    "                max_score = score\n",
    "\n",
    "    else:\n",
    "            \n",
    "            for candidate in list_of_candidates:\n",
    "                score = 0\n",
    "                score = ROUGE.get_scores(candidate, candidate_reference)[0][field][\"f\"]\n",
    "                if score >= max_score:\n",
    "                    best_score_candidate =candidate_ref\n",
    "\n",
    "\n",
    "    return candidate_score, best_score_candidate, best_candidate_index\n",
    "\n",
    "candidate_score, best_score_candidate, best_candidate_index = Minimum_Bayes_Risk(candidate_list)\n",
    "candidate_score, best_score_candidate , best_candidate_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RoBERTa is a BERT based model, which is an encoder transformer. Encoder transformers are usually good for classification, not as good as decoder transformers for text generation. Still, we will use this same model as experience, since these embeddings contains context for every word and sentence being used. The RoBERTa will be used to get the embeddings but the model architecture will be a decoder transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'roberta-base'  # or specify the path to a custom model  \n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "encoder = TFRobertaModel.from_pretrained(model_name)\n",
    "encoder.trainable = False\n",
    "\n",
    "def encode_text(row):\n",
    "    cleaned_review = row[col]\n",
    "\n",
    "    out = roberta_tokenizer.encode_plus(\n",
    "        cleaned_review,\n",
    "        add_special_tokens=True,\n",
    "        max_length=wsize,  # specify the maximum length of your input\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "    return out[\"input_ids\"], out[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df  = pd.DataFrame({\"text\":inputs, \"label\":outputs})\n",
    "train_df.text = train_df.text.apply(lambda x : ' '.join(x))\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wsize = window_size\n",
    "col = \"text\"\n",
    "train_df[[\"input_ids\",\"attention_mask\"]] = train_df.apply(encode_text, axis=1, result_type='expand')\n",
    "\n",
    "wsize = None\n",
    "col = \"label\"\n",
    "train_df[[\"output_ids\",\"output_attention_mask\"]] = train_df.apply(encode_text, axis=1, result_type='expand')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_tfds = tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            (train_ds.input_ids.tolist(), train_ds.attention_mask.tolist()), \n",
    "            (train_ds.output_ids.tolist()), \n",
    "        )\n",
    "    ).batch(batchsize).shuffle(buffer_size=len(train_ds)).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_tfds = tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            (test_ds.input_ids.tolist(), test_ds.attention_mask.tolist()), \n",
    "            test_ds.output_ids.tolist()\n",
    "        )\n",
    "    ).batch(batchsize).shuffle(buffer_size=len(test_ds)).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "for i, o in train_tfds.take(1):\n",
    "    i_id, i_mask = i[0], i[1]\n",
    "    print(i_id)\n",
    "    print(i_mask)\n",
    "    print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def plot_attention(attention, sentence, predicted_sentence, figsize  =(10, 10)):\n",
    "  sentence = sentence.split()\n",
    "  predicted_sentence = predicted_sentence.split() + ['<eos>']\n",
    "  fig = plt.figure(figsize=figsize)\n",
    "  ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "  attention = attention[:len(predicted_sentence), :len(sentence)]\n",
    "\n",
    "  ax.matshow(attention, cmap='viridis', vmin=0.0)\n",
    "\n",
    "  fontdict = {'fontsize': 14}\n",
    "\n",
    "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "  ax.set_xlabel('Input text')\n",
    "  ax.set_ylabel('Output text')\n",
    "  plt.suptitle('Attention weights')\n",
    "  \n",
    "  \n",
    "def plot_weights(res, stop_time = 3, loop = True):\n",
    "\n",
    "    if loop :\n",
    "        for t in range(len(res[\"input_text\"])):\n",
    "        \n",
    "            plt.figure()\n",
    "            attention_array = np.array(res[\"attention\"][t])\n",
    "            _ = plt.bar(range(len(attention_array[0, :])), attention_array[t, :])\n",
    "            \n",
    "            plt.figure()\n",
    "\n",
    "            plot_attention(res['attention'][t],  res[\"output_text\"][t],  res[\"output_text\"][t])#input_text\n",
    "            plt.show()\n",
    "\n",
    "            if t == stop_time:\n",
    "                break\n",
    "    else:\n",
    "        t = -1\n",
    "        attention_array = np.array(res[\"attention\"][t])\n",
    "        plt.figure()\n",
    "\n",
    "        plot_attention(res['attention'][t],  res[\"output_text\"][t],  res[\"output_text\"][t])#input_text\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@keras.saving.register_keras_serializable(package=\"my_custom_metrics\", name=\"masked_sparse_categorical_crossentropy\")\n",
    "def masked_sparse_categorical_crossentropy(y_true, y_pred):\n",
    "\n",
    "    # Calculate the loss for each item in the batch.\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "    # Mask off the losses on padding.\n",
    "    mask = tf.cast(y_true != 0, tf.float32)\n",
    "    loss *= mask\n",
    "\n",
    "    # Return the total.\n",
    "    return tf.reduce_sum(loss)\n",
    "\n",
    "@keras.saving.register_keras_serializable(package=\"my_custom_metrics\", name=\"perplexity\")\n",
    "def perplexity(labels, logits):\n",
    "    #Calculates perplexity metric = 2^(entropy) or e^(entropy)\n",
    "\n",
    "    mask = tf.cast(labels != 0, tf.float32)\n",
    "    loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    return pow(2, loss(y_true=labels, y_pred=logits)* mask)\n",
    "\n",
    "\n",
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_len  dense_units_list = [32], dropout_units_list= [0.1], n_dense_layer = 1, n_lstm_layers = 1, lstm_units = [128], model_name = 'roberta-base'):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.num_layers = n_dense_layer\n",
    "        self.dense_layers = [Dense(dense_units_list[i], activation='relu') for i in range(self.num_layers)]\n",
    "\n",
    "        self.lstm_units = lstm_units\n",
    "        self.n_lstm_layers = n_lstm_layers\n",
    "\n",
    "        for i in range(self.n_lstm_layers):\n",
    "            self.lstm_layers = [LSTM(lstm_units[i], return_sequences=False, return_state = False, statefull = True) for i in range(self.n_lstm_layers)]\n",
    "           \n",
    "        self.dropout_layers =[]\n",
    "        if self.num_layers > 1:\n",
    "            self.dropout_layers = [Dropout(dropout_units_list[i]) for i in range(self.num_layers-1)]\n",
    "            self.batch_norms = [BatchNormalization() for i in range(self.num_layers-1)]\n",
    "        \n",
    "        self.encoder = TFRobertaModel.from_pretrained(model_name)\n",
    "        self.encoder.trainable = False\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.output_dense = Dense(vocab_len, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Get mean and standard deviation from the policy network\n",
    "        input_ids, attention_mask = inputs[0], inputs[1]\n",
    "        x = self.encoder((input_ids, attention_mask))[0]\n",
    "\n",
    "        for i in range(self.n_lstm_layers):\n",
    "            x   = self.lstm_layers[i](x, last_state)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dense_layers[i](x)\n",
    "            \n",
    "            if i < (self.num_layers - 1) and  self.num_layers > 1:\n",
    "                x = self.batch_norms[i](x)\n",
    "                x = self.dropout_layers[i](x)\n",
    "\n",
    "        next_tokens  = self.output_dense(x)\n",
    "\n",
    "        return next_tokens\n",
    "\n",
    "\n",
    "generate_text_model = TextGenerator(vocab_len)\n",
    "generate_text_model.compile(loss=masked_sparse_categorical_crossentropy, optimizer=Adam(learning_rate=0.0005), metrics=[tf.metrics.SparseCategoricalAccuracy(), perplexity])\n",
    "\n",
    "for i, o in dataset.take(1):\n",
    "    print(i.shape, o.shape)\n",
    "    generate_text_model(i)\n",
    "\n",
    "generate_text_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaaaaaaaaaaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_sentence(model_inst, input_text, max_length=10, temperature=0.5, number_of_candidates = 20, ):\n",
    "    \n",
    "    \n",
    "    attention_per_candidate = []\n",
    "    result_text_per_candidate = []\n",
    "    input_text_per_candidate = []\n",
    "    output_text_per_candidate = []\n",
    "\n",
    "    for c in range(number_of_candidates):\n",
    "\n",
    "        last_state= None\n",
    "        last_activation = None\n",
    "        input_mask = None\n",
    "        last_attention = None\n",
    "\n",
    "        result_tokens = []\n",
    "    \n",
    "        attention = []\n",
    "\n",
    "        x = input_tokens\n",
    "        input_text= []\n",
    "        output_text = []\n",
    "        result_tokens = [w.numpy() for w in  x[0] if w != 0 ]\n",
    "        for k in range(max_length):\n",
    "\n",
    "            input_text.append(' '.join([tokenizer.index_word[w.numpy()] for w in  x[0] if w != 0 ]))\n",
    "            x = pad_sequences(x, padding = \"post\", maxlen = window_size)#\n",
    "\n",
    "            input_mask = x!= 0\n",
    "            t = np.count_nonzero(input_mask)\n",
    "\n",
    "            if no_state: last_state= None\n",
    "\n",
    "            predictions, last_state , attention_weights, last_activation, last_attention  =model_inst(x, last_attention, last_state,  last_activation)\n",
    "\n",
    "            if k > 0 and attention_weights is not None:\n",
    "                attention.append(attention_weights[0])\n",
    "\n",
    "            predicted_logits  = predictions[:,t-1,:]/temperature\n",
    "            predicted_logits = tf.where(token_mask, -np.inf, predicted_logits)\n",
    "            predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "            new_tokens = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "            if loop:\n",
    "            \n",
    "                x = new_tokens.numpy()\n",
    "            else : \n",
    "\n",
    "                x = list(x[0])\n",
    "                x = [i for i in x if i != 0]\n",
    "                x.append(new_tokens.numpy()[0])\n",
    "                \n",
    "\n",
    "            output_text.append(' '.join([tokenizer.index_word[w] for w in  x if w != 0 ][1:]))\n",
    "            x = tf.reshape(tf.convert_to_tensor(x), (len(input_tokens),len(x)))\n",
    "            result_tokens.append(new_tokens.numpy()[0])\n",
    "                \n",
    "            if tokenizer.word_index['<eos>'] == new_tokens.numpy()[0]:\n",
    "                break\n",
    "\n",
    "\n",
    "        result_text =   ' '.join([tokenizer.index_word[w] for w in  result_tokens if w != 0 ])\n",
    "        \n",
    "        attention_per_candidate.append(attention)\n",
    "        result_text_per_candidate.append(result_text)\n",
    "        input_text_per_candidate.append(input_text)\n",
    "        output_text_per_candidate.append(output_text)\n",
    "\n",
    "    candidate_score, best_score_candidate, best_candidate_index = Minimum_Bayes_Risk(result_text_per_candidate)\n",
    "\n",
    "    attention_stack = attention_per_candidate[best_candidate_index]\n",
    "    result_text = result_text_per_candidate[best_candidate_index]\n",
    "    input_text = input_text_per_candidate[best_candidate_index]\n",
    "    output_text = output_text_per_candidate[best_candidate_index]\n",
    "    \n",
    "    return {'generated_text': result_text, 'attention': attention_stack, \"input_text\" : input_text, \"output_text\" : output_text, \"candidates\": result_text_per_candidate}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class TextGeneratorAttention(tf.keras.Model):\n",
    "#     def __init__(self, n_words  dense_units_list = [32], dropout_units_list= [0.1], n_dense_layer = 1, n_lstm_layers = 1, lstm_units = [128], model_name = 'roberta-base'):\n",
    "#         super(TextGeneratorAttention, self).__init__()\n",
    "        \n",
    "#         self.num_layers = n_dense_layer\n",
    "#         self.dense_layers = [Dense(dense_units_list[i], activation='relu') for i in range(self.num_layers)]\n",
    "\n",
    "#         self.lstm_units = lstm_units\n",
    "#         self.n_lstm_layers = n_lstm_layers\n",
    "           \n",
    "#         self.dropout_layers =[]\n",
    "#         if self.num_layers > 1:\n",
    "#             self.dropout_layers = [Dropout(dropout_units_list[i]) for i in range(self.num_layers-1)]\n",
    "#             self.batch_norms = [BatchNormalization() for i in range(self.num_layers-1)]\n",
    "        \n",
    "#         self.encoder = TFRobertaModel.from_pretrained(model_name)\n",
    "#         self.encoder.trainable = False\n",
    "#         self.flatten = tf.keras.layers.Flatten()\n",
    "#         self.output_dense = Dense(n_words, activation='softmax')\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         # Get mean and standard deviation from the policy network\n",
    "#         input_ids, attention_mask = inputs[0], inputs[1]\n",
    "#         x = self.encoder((input_ids, attention_mask))[0]\n",
    "\n",
    "#         for i in range(self.num_layers):\n",
    "#             x  = self.lstm_layers[i](x)\n",
    "\n",
    "#         for i in range(self.num_layers):\n",
    "#             x = self.dense_layers[i](x)\n",
    "            \n",
    "#             if i < (self.num_layers - 1) and  self.num_layers > 1:\n",
    "#                 x = self.batch_norms[i](x)\n",
    "#                 x = self.dropout_layers[i](x)\n",
    "\n",
    "#         next_token  = self.output_dense(x)\n",
    "\n",
    "#         return next_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ModelClass(tf.keras.Model):\n",
    "    def __init__(self, vocab_len, model_type= 1, causal_mask_enabled= 0 , blstm_enable = False, loop= True, use_attention= True):\n",
    "        super(ModelClass, self).__init__()\n",
    "        self.vocab_size = vocab_len\n",
    "        self.loop = loop\n",
    "        self.textModel = TextModel(vocab_len, emb_dim,embeddings_matrix, causal_mask_enabled, blstm_enable, use_attention)\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        if self.use_tf_function:\n",
    "            return self._tf_train_step(inputs)\n",
    "        else:\n",
    "            if self.loop:\n",
    "                return self._train_step_loop(inputs)\n",
    "            else:\n",
    "                return self._train_step(inputs)\n",
    "\n",
    "\n",
    "    def _train_step(self, inputs):\n",
    "        inp, targ = inputs  \n",
    "        target_mask = targ != 0     \n",
    "        last_state= None\n",
    "        last_activation = None\n",
    "        last_attention = None\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = tf.constant(0.0)\n",
    "            acc = []\n",
    "            x = inp\n",
    "            input_mask = x != 0\n",
    "            y =  targ\n",
    "            \n",
    "            pred, last_state , attention_weights, last_activation, last_attention  = self.textModel(x, last_attention, last_state, input_mask, last_activation)\n",
    "            \n",
    "            loss += self.loss(y, pred)\n",
    "            self.compiled_metrics.update_state(y, pred)\n",
    "            acc.append(self.metrics[0].result())\n",
    "\n",
    "            average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))  \n",
    "            average_acc  = tf.reduce_mean(tf.convert_to_tensor(acc) )\n",
    "            average_perplexity  = tf.reduce_mean(tf.convert_to_tensor(perplexity(y, pred) ))\n",
    "            \n",
    "        variables = self.trainable_variables \n",
    "        gradients = tape.gradient(average_loss, variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "        m = {m.name: average_acc   for m in self.metrics}\n",
    "        m[\"loss\"] = average_loss\n",
    "        m[\"perplexity\"] = average_perplexity\n",
    "    \n",
    "        return m\n",
    "    \n",
    "    def call(self, inputs, last_attention = None, last_state = None,  last_activation  = None):\n",
    "        input_mask = inputs != 0\n",
    "        pred, last_state , attention_weights, last_activation, last_attention  = self.textModel(inputs, last_attention, last_state, input_mask, last_activation)\n",
    "\n",
    "        return pred, last_state , attention_weights, last_activation, last_attention\n",
    "\n",
    "    def _train_step_loop(self, inputs):\n",
    "        inp, targ = inputs  \n",
    "        \n",
    "        target_mask = targ != 0     \n",
    "\n",
    "        last_state= None\n",
    "        last_activation = None\n",
    "        last_attention = None\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = tf.constant(0.0)\n",
    "            acc = []\n",
    "            x = inp\n",
    "            perplexity_list = []\n",
    "            y =  targ\n",
    "            for t in range(0, targ.shape[1]):\n",
    "                \n",
    "                dec_input = tf.expand_dims(x[:, t], 1)## Teacher forcing - feeding the target as the next input\n",
    "                input_mask = dec_input != 0\n",
    "                \n",
    "                pred, last_state , attention_weights, last_activation, last_attention = self.textModel(dec_input,  last_attention, last_state, input_mask, last_activation)\n",
    "                \n",
    "                tar = tf.expand_dims(y[:, t], 1)\n",
    "                loss += self.loss(tar, pred)\n",
    "\n",
    "                self.compiled_metrics.update_state(tar, pred)\n",
    "                acc.append(self.metrics[0].result())\n",
    "                perplexity_list.append(perplexity(tar, pred))\n",
    "\n",
    "\n",
    "            average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))  \n",
    "            average_acc  = tf.reduce_mean(tf.convert_to_tensor(acc) )\n",
    "            \n",
    "            average_perplexity  = tf.reduce_mean(tf.convert_to_tensor(perplexity_list))\n",
    "            \n",
    "        variables = self.trainable_variables \n",
    "        gradients = tape.gradient(average_loss, variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "        m = {m.name: average_acc   for m in self.metrics}\n",
    "        m[\"loss\"] = average_loss\n",
    "        m[\"perplexity\"] = average_perplexity\n",
    "    \n",
    "        return m\n",
    "            \n",
    "\n",
    "    @tf.function(input_signature=[[tf.TensorSpec(dtype=tf.int32, shape=[None, window_size]),\n",
    "                                tf.TensorSpec(dtype=tf.int32, shape=[None, window_size])]])#\n",
    "                                \n",
    "    def _tf_train_step(self, inputs):\n",
    "        if self.loop:\n",
    "            return self._train_step_loop(inputs)\n",
    "        else:\n",
    "            return self._train_step(inputs)\n",
    "\n",
    "\n",
    "    def test_step(self, inputs):\n",
    "        if self.use_tf_function:\n",
    "            return self._tf_test_step(inputs)\n",
    "        else:\n",
    "            if self.loop:\n",
    "                return self._test_step_loop(inputs)\n",
    "            else:\n",
    "                return self._test_step(inputs)\n",
    "\n",
    "    \n",
    "    def _test_step(self, inputs):\n",
    "        inp, targ = inputs  \n",
    "        \n",
    "        target_mask = targ != 0\n",
    "        input_mask = None\n",
    "\n",
    "        loss_val = tf.constant(0.0)\n",
    "        acc = []\n",
    "\n",
    "        last_state= None\n",
    "        last_activation = None\n",
    "        last_attention = None\n",
    "\n",
    "        x = inp\n",
    "        y =  targ\n",
    "        input_mask = x != 0\n",
    "        \n",
    "        pred, last_state , attention_weights, last_activation, last_attention  = self.textModel(x, last_attention, last_state, input_mask, last_activation)\n",
    "        \n",
    "        loss_val += self.loss(y, pred)\n",
    "\n",
    "        self.compiled_metrics.update_state(y, pred)\n",
    "        acc.append(self.metrics[0].result())\n",
    "        \n",
    "\n",
    "        average_loss = loss_val / tf.reduce_sum(tf.cast(target_mask, tf.float32))  \n",
    "        average_acc  = tf.reduce_mean(tf.convert_to_tensor(acc) )\n",
    "        average_perplexity  = tf.reduce_mean(tf.convert_to_tensor(perplexity(y, pred) ))\n",
    "\n",
    "        m = {m.name: average_acc  for m in self.metrics}\n",
    "        m[\"loss\"] = average_loss\n",
    "        m[\"perplexity\"] = average_perplexity\n",
    "        \n",
    "        return m\n",
    "\n",
    "    def _test_step_loop(self, inputs):\n",
    "        inp, targ = inputs  \n",
    "        \n",
    "        target_mask = targ != 0\n",
    "        input_mask = None\n",
    "\n",
    "        loss_val = tf.constant(0.0)\n",
    "        acc = []\n",
    "\n",
    "        last_state= None\n",
    "        last_activation = None\n",
    "        last_attention = None\n",
    "\n",
    "        x = inp\n",
    "        perplexity_list = []\n",
    "        y =  targ\n",
    "        \n",
    "        for t in range(0, targ.shape[1]):\n",
    "            \n",
    "            dec_input = tf.expand_dims(x[:, t], 1)## Teacher forcing - feeding the target as the next input\n",
    "            input_mask = dec_input != 0\n",
    "\n",
    "            pred, last_state , attention_weights, last_activation, last_attention = self.textModel(dec_input,  last_attention, last_state, input_mask, last_activation)\n",
    "            \n",
    "            tar = tf.expand_dims(y[:, t], 1)\n",
    "            loss_val += self.loss(tar, pred)\n",
    "            self.compiled_metrics.update_state(tar, pred)\n",
    "            acc.append(self.metrics[0].result())\n",
    "            perplexity_list.append(perplexity(tar, pred))\n",
    "            acc.append(self.metrics[0].result())\n",
    "\n",
    "        average_loss = loss_val / tf.reduce_sum(tf.cast(target_mask, tf.float32))  \n",
    "        average_acc  = tf.reduce_mean(tf.convert_to_tensor(acc) )\n",
    "        average_perplexity  = tf.reduce_mean(tf.convert_to_tensor(perplexity_list))\n",
    "\n",
    "        m = {m.name: average_acc  for m in self.metrics}\n",
    "        m[\"loss\"] = average_loss\n",
    "        m[\"perplexity\"] = average_perplexity\n",
    "        \n",
    "        return m\n",
    "\n",
    "\n",
    "    @tf.function(input_signature=[[tf.TensorSpec(dtype=tf.int32, shape=[None, window_size]),\n",
    "                                tf.TensorSpec(dtype=tf.int32, shape=[None, window_size])\n",
    "                                ]])\n",
    "    def _tf_test_step(self, inputs):\n",
    "        if self.loop:\n",
    "            return self._test_step_loop(inputs)\n",
    "        else:\n",
    "            return self._test_step(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_AND_SAVE:\n",
    "    text_model = TextModel(vocab_len, emb_dim, embeddings_matrix)\n",
    "    for i, o in dataset.take(1):\n",
    "        print(i.shape, o.shape)\n",
    "        text_model(i)\n",
    "\n",
    "    text_model = TextModel(vocab_len, emb_dim, embeddings_matrix, causal_mask_enabled= 1)\n",
    "    for i, o in dataset.take(1):\n",
    "        text_model(i)\n",
    "\n",
    "\n",
    "    text_model = TextModel(vocab_len, emb_dim, embeddings_matrix, causal_mask_enabled= 2)\n",
    "    for i, o in dataset.take(1):\n",
    "        text_model(i)\n",
    "\n",
    "    text_model = TextModel(vocab_len, emb_dim, embeddings_matrix, blstm_enable= True)\n",
    "    for i, o in dataset.take(1):\n",
    "        text_model(i)\n",
    "\n",
    "    text_model = TextModel(vocab_len, emb_dim, embeddings_matrix, causal_mask_enabled= 1,  blstm_enable= True)\n",
    "    for i, o in dataset.take(1):\n",
    "        text_model(i)\n",
    "\n",
    "    text_model = TextModel(vocab_len, emb_dim, embeddings_matrix, causal_mask_enabled= 2,  blstm_enable= True)\n",
    "    for i, o in dataset.take(1):\n",
    "        text_model(i)\n",
    "\n",
    "    text_model = TextModel(vocab_len, emb_dim, embeddings_matrix, causal_mask_enabled= 2,  blstm_enable= True, use_attention = False)\n",
    "    for i, o in dataset.take(1):\n",
    "        text_model(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, o in dataset.take(1):\n",
    "    print(i)\n",
    "    \n",
    "emb_layer = Embedding(input_dim=vocab_len, input_length = window_size, output_dim=emb_dim, weights=[embeddings_matrix] ,name =\"emb_layer\", trainable=False, mask_zero = True)\n",
    "\n",
    "attention_layer = SelfMaskedDotProductAttention()\n",
    "attention_layer_causal1 = SelfMaskedDotProductAttention()\n",
    "#attention_layer_causal2 = SelfMaskedDotProductAttention(causal_mask_enabled=2)\n",
    "\n",
    "emb_layer = Embedding(input_dim=vocab_len, input_length = window_size, output_dim=emb_dim, weights=[embeddings_matrix] ,name =\"emb_layer\", trainable=False, mask_zero = True)\n",
    "query = emb_layer(i)\n",
    "values = emb_layer(i)\n",
    "\n",
    "context_vector, attention_weights = attention_layer(query,values,values, (i != 0))\n",
    "context_vector1, attention_weights1 = attention_layer_causal1(query,values,values, (i != 0))\n",
    "context_vector1, attention_weights_causal = attention_layer_causal1(query,values,values, (i != 0), (1 - tf.linalg.band_part(tf.ones((i.shape[1], i.shape[1])), -1, 0)))\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.subplot(3, 2, 1)\n",
    "plt.pcolormesh(attention_weights[:, 0, :])\n",
    "plt.title('Attention weights - Not causal')\n",
    "\n",
    "plt.subplot(3, 2, 2)\n",
    "plt.pcolormesh(i != 0)\n",
    "plt.title('Mask')\n",
    "\n",
    "plt.subplot(3, 2, 3)\n",
    "plt.pcolormesh(attention_weights1[:, 0, :])\n",
    "plt.title('Attention weights 1')\n",
    "\n",
    "plt.subplot(3, 2, 4)\n",
    "plt.pcolormesh(attention_weights[0])\n",
    "plt.title('Attention weights')\n",
    "\n",
    "plt.subplot(3, 2, 5)\n",
    "plt.pcolormesh(attention_weights1[0])\n",
    "plt.title('Attention weights 1')\n",
    "\n",
    "plt.subplot(3, 2, 5)\n",
    "plt.pcolormesh(attention_weights_causal[0])\n",
    "plt.title('Attention weights causal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blstm_enable = False\n",
    "\n",
    "if TRAIN_AND_SAVE:\n",
    "    model = ModelClass(vocab_len, model_type=1, causal_mask_enabled= 0, blstm_enable=blstm_enable, use_attention=False)\n",
    "    model.compile(loss=MaskedLossCustom(), optimizer=Adam(learning_rate=0.0005), metrics=[tf.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "    model_causal1 = ModelClass(vocab_len, model_type=1, causal_mask_enabled= 1, blstm_enable=blstm_enable, loop= False)\n",
    "    model_causal1.compile(loss=MaskedLossCustom(), optimizer=Adam(learning_rate=0.0005), metrics=[tf.metrics.SparseCategoricalAccuracy()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = vocab_len\n",
    "token_mask_ids = np.array([tokenizer.word_index[x] for x in  [ '<oov>', '<eos>' ]])[:, None]#\n",
    "token_mask = np.zeros([vocab_size], dtype=np.bool)\n",
    "token_mask[np.array(token_mask_ids)] = True\n",
    "\n",
    "print(\"token_mask\", token_mask)\n",
    "\n",
    "\n",
    "def generate_sentence(model_inst, input_text, max_length=10, temperature=0.5, number_of_candidates = 20, ):\n",
    "    \n",
    "    \n",
    "    attention_per_candidate = []\n",
    "    result_text_per_candidate = []\n",
    "    input_text_per_candidate = []\n",
    "    output_text_per_candidate = []\n",
    "\n",
    "    for c in range(number_of_candidates):\n",
    "\n",
    "        last_state= None\n",
    "        last_activation = None\n",
    "        input_mask = None\n",
    "        last_attention = None\n",
    "\n",
    "        result_tokens = []\n",
    "    \n",
    "        attention = []\n",
    "\n",
    "        x = input_tokens\n",
    "        input_text= []\n",
    "        output_text = []\n",
    "        result_tokens = [w.numpy() for w in  x[0] if w != 0 ]\n",
    "        for k in range(max_length):\n",
    "\n",
    "            input_text.append(' '.join([tokenizer.index_word[w.numpy()] for w in  x[0] if w != 0 ]))\n",
    "            x = pad_sequences(x, padding = \"post\", maxlen = window_size)#\n",
    "\n",
    "            input_mask = x!= 0\n",
    "            t = np.count_nonzero(input_mask)\n",
    "\n",
    "            if no_state: last_state= None\n",
    "\n",
    "            predictions, last_state , attention_weights, last_activation, last_attention  =model_inst(x, last_attention, last_state,  last_activation)\n",
    "\n",
    "            if k > 0 and attention_weights is not None:\n",
    "                attention.append(attention_weights[0])\n",
    "\n",
    "            predicted_logits  = predictions[:,t-1,:]/temperature\n",
    "            predicted_logits = tf.where(token_mask, -np.inf, predicted_logits)\n",
    "            predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "            new_tokens = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "            if loop:\n",
    "            \n",
    "                x = new_tokens.numpy()\n",
    "            else : \n",
    "\n",
    "                x = list(x[0])\n",
    "                x = [i for i in x if i != 0]\n",
    "                x.append(new_tokens.numpy()[0])\n",
    "                \n",
    "\n",
    "            output_text.append(' '.join([tokenizer.index_word[w] for w in  x if w != 0 ][1:]))\n",
    "            x = tf.reshape(tf.convert_to_tensor(x), (len(input_tokens),len(x)))\n",
    "            result_tokens.append(new_tokens.numpy()[0])\n",
    "                \n",
    "            if tokenizer.word_index['<eos>'] == new_tokens.numpy()[0]:\n",
    "                break\n",
    "\n",
    "\n",
    "        result_text =   ' '.join([tokenizer.index_word[w] for w in  result_tokens if w != 0 ])\n",
    "        \n",
    "        attention_per_candidate.append(attention)\n",
    "        result_text_per_candidate.append(result_text)\n",
    "        input_text_per_candidate.append(input_text)\n",
    "        output_text_per_candidate.append(output_text)\n",
    "\n",
    "    candidate_score, best_score_candidate, best_candidate_index = Minimum_Bayes_Risk(result_text_per_candidate)\n",
    "\n",
    "    attention_stack = attention_per_candidate[best_candidate_index]\n",
    "    result_text = result_text_per_candidate[best_candidate_index]\n",
    "    input_text = input_text_per_candidate[best_candidate_index]\n",
    "    output_text = output_text_per_candidate[best_candidate_index]\n",
    "    \n",
    "    return {'generated_text': result_text, 'attention': attention_stack, \"input_text\" : input_text, \"output_text\" : output_text, \"candidates\": result_text_per_candidate}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_AND_SAVE:\n",
    "    input_sent = \"<sos> eu quero \"\n",
    "    inp_seq = np.array([tokenizer.word_index[x] for x in input_sent.split()])\n",
    "    print(\"inp_seq -->\",inp_seq)\n",
    "\n",
    "    result = generate_sentence(model_causal1, input_tokens = tf.expand_dims(inp_seq,0),max_length= 5)\n",
    "    print(\"RESULT : \",result[\"generated_text\"])\n",
    "    plot_weights(result, stop_time = 3, loop=True)\n",
    "\n",
    "    result = generate_sentence(model_causal1, input_tokens = tf.expand_dims(inp_seq,0),max_length= 5, loop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_AND_SAVE:\n",
    "    model.use_tf_function = False\n",
    "    for i, o in dataset.take(2):\n",
    "        print(model.train_step([i, o]))\n",
    "        print(model.test_step([i, o]))\n",
    "    print()\n",
    "\n",
    "    model_causal1.use_tf_function = False\n",
    "    for i, o in dataset.take(2):\n",
    "        print(model_causal1.train_step([i, o]))\n",
    "        print(model_causal1.test_step([i, o]))\n",
    "    print()\n",
    "\n",
    "    print(\"\\n---------------------\\n\")\n",
    "\n",
    "    model.use_tf_function = True\n",
    "    for i, o in dataset.take(2):\n",
    "        print(model.train_step([i, o]))\n",
    "        print(model.test_step([i, o]))\n",
    "    print()\n",
    "\n",
    "    model_causal1.use_tf_function = True\n",
    "    for i, o in dataset.take(2):\n",
    "        print(model_causal1.train_step([i, o]))\n",
    "        print(model_causal1.test_step([i, o]))\n",
    "\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_AND_SAVE:\n",
    "    \n",
    "    history = model.fit(dataset, epochs=epochs, shuffle= True,#6000\n",
    "                        steps_per_epoch = train_size, callbacks=[PlotLossesCallback()], validation_data=val_dataset, validation_steps=10)\n",
    "    model.save_weights(\"./checkpoints/model_ckpt\")\n",
    "\n",
    "    with open('./checkpoints/history.pickle', 'wb') as handle:\n",
    "        pickle.dump(history.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "else :\n",
    "    print(\"Loading saved data ...\")\n",
    "    model = ModelClass(vocab_len, model_type=1, causal_mask_enabled= 0, blstm_enable=blstm_enable, use_attention=False)\n",
    "    model.load_weights(\"./checkpoints/model_ckpt\")\n",
    "\n",
    "    with open('./checkpoints/history.pickle', 'rb') as handle:\n",
    "        history = pickle.load(handle)\n",
    "        plt.figure()\n",
    "        plt.plot(history['sparse_categorical_accuracy'])\n",
    "        plt.plot(history['val_sparse_categorical_accuracy'])\n",
    "        plt.title('Model accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(history['loss'])\n",
    "        plt.plot(history['val_loss'])\n",
    "        plt.title('Model training loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(history['perplexity'])\n",
    "        plt.plot(history['val_perplexity'])\n",
    "        plt.title('Model perplexity ')\n",
    "        plt.ylabel('Perplexity')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_AND_SAVE:\n",
    "    \n",
    "    history_causal1 = model_causal1.fit(dataset, epochs=epochs+100, shuffle= True, #6000\n",
    "                        steps_per_epoch = train_size, callbacks=[PlotLossesCallback()], validation_data=val_dataset, validation_steps=10)\n",
    "    model_causal1.save_weights(\"./checkpoints/model_causal1_ckpt\")\n",
    "\n",
    "    with open('./checkpoints/history_causal1.pickle', 'wb') as handle:\n",
    "        pickle.dump(history_causal1.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "else :\n",
    "    print(\"Loading saved data ...\")\n",
    "    model_causal1 = ModelClass(vocab_len, model_type=1, causal_mask_enabled= 1, blstm_enable=blstm_enable, loop= False)\n",
    "\n",
    "    model_causal1.load_weights(\"./checkpoints/model_causal1_ckpt\")\n",
    "\n",
    "    with open('./checkpoints/history_causal1.pickle', 'rb') as handle:\n",
    "        history_causal1 = pickle.load(handle)\n",
    "        plt.figure()\n",
    "        plt.plot(history_causal1['sparse_categorical_accuracy'])\n",
    "        plt.plot(history_causal1['val_sparse_categorical_accuracy'])\n",
    "        plt.title('Model accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(history_causal1['loss'])\n",
    "        plt.plot(history_causal1['val_loss'])\n",
    "        plt.title('Model training loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(history_causal1['perplexity'])\n",
    "        plt.plot(history_causal1['val_perplexity'])\n",
    "        plt.title('Model perplexity ')\n",
    "        plt.ylabel('Perplexity')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, padding_mask= None, look_ahead_mask= None):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead)\n",
    "  but it must be broadcastable for addition.\n",
    "\n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable\n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "  if padding_mask is not None:\n",
    "    scaled_attention_logits += (padding_mask * -1e9)\n",
    "\n",
    "  if look_ahead_mask is not None:\n",
    "    scaled_attention_logits += (look_ahead_mask * -1e9)\n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights\n",
    "\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self,*, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.wq = Dense(d_model)\n",
    "    self.wk = Dense(d_model)\n",
    "    self.wv = Dense(d_model)\n",
    "\n",
    "    self.dense = Dense(d_model)\n",
    "\n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, v, k, q, padding_mask, look_ahead_mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "\n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention( q, k, v, padding_mask, look_ahead_mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention,(batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "n_vectors = 150\n",
    "d_model = 256\n",
    "\n",
    "query = tf.random.uniform((batch_size, n_vectors, d_model), dtype=tf.float32)\n",
    "key = tf.random.uniform((batch_size, n_vectors, d_model), dtype=tf.float32)\n",
    "value = tf.random.uniform((batch_size, n_vectors, d_model), dtype=tf.float32)\n",
    "query.shape, key.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dot product attention\n",
    "x,_ = scaled_dot_product_attention(query, key, key, None)\n",
    "print(f\"Output from dot product attention: {x.shape}\")\n",
    "\n",
    "att = SelfMaskedDotProductAttention()\n",
    "x1, _ = att(query, key,key)\n",
    "#x = tf.concat(x, -1)\n",
    "print(f\"Output from dot product attention: {x1.shape}\")\n",
    "np.where((x-x1) > 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multi-head attention\n",
    "mh_layer = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "x, _ = mh_layer(query, key, value, None, None)\n",
    "print(f\"Output from multi-head attention: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff): #Point wise feed forward network consists of two fully-connected layers with a ReLU activation in between.\n",
    "  return tf.keras.Sequential([\n",
    "      Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])\n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self,*, d_model, num_heads, dff, rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    self.dropout1 = Dropout(rate)\n",
    "    self.dropout2 = Dropout(rate)\n",
    "\n",
    "  def call(self, x, training, padding_mask= None, look_ahead_mask = None):\n",
    "    attn_output, attn_weights_block1  = self.mha(x, x, x, padding_mask, look_ahead_mask)  # (batch_size, input_seq_len, d_model)\n",
    "    attn_output = self.dropout1(attn_output, training=training)\n",
    "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    return out2, attn_weights_block1\n",
    "\n",
    "\n",
    "sample_encoder_layer = DecoderLayer(d_model=512, num_heads=8, dff=batch_size)\n",
    "sample_encoder_layer_output,attn_weights_block1 = sample_encoder_layer(tf.random.uniform((64, 43, 512)), False, None)\n",
    "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "  pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "n, d = 2048, vocab_len\n",
    "pos_encoding = positional_encoding(n, d)\n",
    "print(pos_encoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self,*, num_layers, d_model, embeddings_matrix, window_size, num_heads, dff, vocab_len,\n",
    "               rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.emb_layer = Embedding(input_dim=vocab_len, input_length = window_size, output_dim=d_model, weights=[embeddings_matrix] ,name =\"emb_layer2\", trainable=False)#, mask_zero = True\n",
    "    self.pos_encoding = positional_encoding(window_size, self.d_model)\n",
    "\n",
    "    self.dec_layers = [DecoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate) for _ in range(num_layers)]\n",
    "\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "  def call(self, x, training, padding_mask =None, look_ahead_mask= None):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    attention_weights = {}\n",
    "\n",
    "    x = self.emb_layer(x)  # (batch_size, input_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "    x = self.dropout(x, training=training)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x, block1 = self.dec_layers[i](x, training, padding_mask, look_ahead_mask)\n",
    "      attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "\n",
    "\n",
    "    return x ,attention_weights # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "sample_decoder = Decoder(num_layers=2, d_model=emb_dim, embeddings_matrix = embeddings_matrix, window_size= window_size, num_heads=10,\n",
    "                         dff=2048, vocab_len=vocab_len)\n",
    "\n",
    "for i, o in dataset.take(1):\n",
    "  print(i.shape, o.shape)\n",
    "  output, attn = sample_decoder(i, training=True, padding_mask =None, look_ahead_mask= None)\n",
    "  \n",
    "output.shape, attn['decoder_layer2_block1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(tf.keras.losses.Loss):\n",
    "  def __init__(self):\n",
    "    self.name = 'masked_loss_transformer_custom'\n",
    "    self.loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "  def __call__(self, real, pred):\n",
    "      \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = self.loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "  real = tf.cast(real, dtype=tf.int32)\n",
    "  accuracies = tf.equal(real, tf.cast(tf.argmax(pred, axis=2), dtype=tf.int32))\n",
    "\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  accuracies = tf.math.logical_and(mask, accuracies)\n",
    "\n",
    "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "  mask = tf.cast(mask, dtype=tf.float32)\n",
    "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)\n",
    "  \n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.xlabel('Train Step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,*, num_layers, d_model,embeddings_matrix,window_size, num_heads, dff,  vocab_len,rate=0.1, use_tf_function= False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.decoder = Decoder(num_layers=num_layers, d_model=d_model, embeddings_matrix = embeddings_matrix,\n",
    "                            window_size= window_size, num_heads=num_heads, dff=dff, vocab_len=vocab_len, rate=rate)\n",
    "\n",
    "        self.final_layer = Dense(vocab_len)\n",
    "        self.use_tf_function = use_tf_function\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        # Keras models prefer if you pass all your inputs in the first argument\n",
    "        inp = inputs\n",
    "\n",
    "        padding_mask, look_ahead_mask = self.create_masks(inp)\n",
    "        \n",
    "        # dec_output.shape == (batch_size, inp_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(inp, training, padding_mask, look_ahead_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, vocab_len)\n",
    "\n",
    "        return final_output, attention_weights\n",
    "\n",
    "    def create_masks(self, inp):\n",
    "        # Encoder padding mask (Used in the 2nd attention block in the decoder too.)\n",
    "        \n",
    "        padding_mask = self.create_padding_mask(inp)\n",
    "\n",
    "        # Used in the 1st attention block in the decoder.\n",
    "        # It is used to pad and mask future tokens in the input received by\n",
    "        # the decoder.\n",
    "        look_ahead_mask = self.create_look_ahead_mask(tf.shape(inp)[1])\n",
    "\n",
    "        return padding_mask, look_ahead_mask\n",
    "\n",
    "    def create_padding_mask(self,seq):\n",
    "        seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "        # add extra dimensions to add the padding\n",
    "        # to the attention logits.\n",
    "        return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "    def create_look_ahead_mask(self, size):\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "        return mask  # (seq_len, seq_len)\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        if self.use_tf_function:\n",
    "            return self._tf_train_step(inputs)\n",
    "        else:\n",
    "            return self._train_step(inputs)\n",
    "\n",
    "    @tf.function(input_signature=[[tf.TensorSpec(dtype=tf.int32, shape=[None, window_size]),\n",
    "                                tf.TensorSpec(dtype=tf.int32, shape=[None, window_size])]])\n",
    "                                \n",
    "    def _tf_train_step(self, inputs):\n",
    "        return self._train_step(inputs)\n",
    "\n",
    "\n",
    "\n",
    "    def test_step(self, inputs):\n",
    "        if self.use_tf_function:\n",
    "            return self._tf_test_step(inputs)\n",
    "        else:\n",
    "            return self._test_step(inputs)\n",
    "\n",
    "\n",
    "    @tf.function(input_signature=[[tf.TensorSpec(dtype=tf.int32, shape=[None, window_size]),\n",
    "                                tf.TensorSpec(dtype=tf.int32, shape=[None, window_size])]])\n",
    "    def _tf_test_step(self, inputs):\n",
    "        return self._test_step(inputs)\n",
    "\n",
    "\n",
    "    def _train_step(self, inputs):\n",
    "        inp, tar = inputs\n",
    "        target_mask = tar != 0     \n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            acc = []\n",
    "            loss = tf.constant(0.0)\n",
    "\n",
    "            predictions, _ = self.call(inp,training = True)\n",
    "            loss += self.loss(tar, predictions)\n",
    "            self.compiled_metrics.update_state(tar, predictions)\n",
    "\n",
    "            acc.append(self.metrics[0].result())\n",
    "\n",
    "            average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))  \n",
    "            average_acc  = tf.reduce_mean(tf.convert_to_tensor(acc) )\n",
    "            average_perplexity  = tf.reduce_mean(tf.convert_to_tensor(perplexity(tar, predictions) ))\n",
    "            average_cc  = tf.reduce_mean(tf.convert_to_tensor(accuracy_function(tf.cast(tar, tf.int32), predictions) ))\n",
    "\n",
    "        variables = self.trainable_variables \n",
    "        gradients = tape.gradient(average_loss, variables)\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "        m = {m.name: average_acc  for m in self.metrics} #.update({'loss': batch_loss})m.result() \n",
    "        m[\"loss\"] = average_loss\n",
    "        m[\"perplexity\"] = average_perplexity\n",
    "        m[\"custom_acc\"] = average_cc\n",
    "\n",
    "        return m\n",
    "\n",
    "    def _test_step(self, inputs):\n",
    "        inp, tar = inputs\n",
    "        target_mask = tar != 0     \n",
    "\n",
    "        acc = []\n",
    "        loss = tf.constant(0.0)\n",
    "\n",
    "        predictions, _ = self.call(inp,training = True)\n",
    "        loss += self.loss(tar, predictions)\n",
    "        self.compiled_metrics.update_state(tar, predictions)\n",
    "\n",
    "        acc.append(self.metrics[0].result())\n",
    "\n",
    "        average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))  \n",
    "        average_acc  = tf.reduce_mean(tf.convert_to_tensor(acc) )\n",
    "        average_perplexity  = tf.reduce_mean(tf.convert_to_tensor(perplexity(tar, predictions) ))\n",
    "        average_cc  = tf.reduce_mean(tf.convert_to_tensor(accuracy_function(tf.cast(tar, tf.int32), predictions) ))\n",
    "\n",
    "        m = {m.name: average_acc  for m in self.metrics} #.update({'loss': batch_loss})m.result() \n",
    "        m[\"loss\"] = average_loss\n",
    "        m[\"perplexity\"] = average_perplexity\n",
    "        m[\"custom_acc\"] = average_cc\n",
    "\n",
    "        return m\n",
    "\n",
    "\n",
    "transformer = Transformer(\n",
    "    num_layers=4,\n",
    "    d_model=emb_dim,\n",
    "    embeddings_matrix = embeddings_matrix,\n",
    "    window_size = window_size,\n",
    "    num_heads=10,\n",
    "    dff=256,\n",
    "    vocab_len=vocab_len,\n",
    "    rate=0.1)\n",
    "\n",
    "for i, o in dataset.take(1):\n",
    "    print(i.shape, o.shape)\n",
    "    fn_out, _ = transformer(i, training=False)\n",
    "\n",
    "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "transformer.compile(loss=CustomLoss(), optimizer=optimizer, metrics=[tf.metrics.SparseCategoricalAccuracy()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_sentence_for_transformer(transformer_inst, input_tokens, max_length=10, temperature=1, number_of_candidates = 20, loop= False):\n",
    "    \n",
    "    attention_per_candidate = []\n",
    "    result_text_per_candidate = []\n",
    "    input_text_per_candidate = []\n",
    "    output_text_per_candidate = []\n",
    "\n",
    "    for c in range(number_of_candidates):\n",
    "\n",
    "        input_mask = None\n",
    "        result_tokens = []\n",
    "        attention = []\n",
    "\n",
    "        x = input_tokens\n",
    "        input_text= []\n",
    "        output_text = []\n",
    "        result_tokens = [w.numpy() for w in  x[0] if w != 0 ]\n",
    "        for k in range(max_length):\n",
    "\n",
    "            input_text.append(' '.join([tokenizer.index_word[w.numpy()] for w in  x[0] if w != 0 ]))\n",
    "            x = pad_sequences(x, padding = \"post\", maxlen = window_size)#window_size\n",
    "\n",
    "            input_mask = x!= 0\n",
    "            t = np.count_nonzero(input_mask)\n",
    "\n",
    "            predictions, attention_weights  =transformer_inst(x, training=False)\n",
    "\n",
    "            if k > 0 and attention_weights is not None:\n",
    "                attention.append(attention_weights)\n",
    "\n",
    "            predicted_logits  = predictions[:,t-1,:]/temperature\n",
    "            predicted_logits = tf.where(token_mask, -np.inf, predicted_logits)\n",
    "            predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "            new_tokens = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "            if loop:\n",
    "                x = new_tokens.numpy()\n",
    "            else : \n",
    "\n",
    "                x = list(x[0])\n",
    "                x = [i for i in x if i != 0]\n",
    "                x.append(new_tokens.numpy()[0])\n",
    "                \n",
    "\n",
    "            output_text.append(' '.join([tokenizer.index_word[w] for w in  x if w != 0 ][1:]))\n",
    "            x = tf.reshape(tf.convert_to_tensor(x), (len(input_tokens),len(x)))\n",
    "            result_tokens.append(new_tokens.numpy()[0])\n",
    "                \n",
    "            if tokenizer.word_index['<eos>'] == new_tokens.numpy()[0]:\n",
    "                break\n",
    "\n",
    "        result_text =   ' '.join([tokenizer.index_word[w] for w in  result_tokens if w != 0 ])\n",
    "        \n",
    "        attention_per_candidate.append(attention)\n",
    "        result_text_per_candidate.append(result_text)\n",
    "        input_text_per_candidate.append(input_text)\n",
    "        output_text_per_candidate.append(output_text)\n",
    "        \n",
    "\n",
    "\n",
    "    candidate_score, best_score_candidate, best_candidate_index = Minimum_Bayes_Risk(result_text_per_candidate)\n",
    "\n",
    "    attention_stack = attention_per_candidate[best_candidate_index]\n",
    "    result_text = result_text_per_candidate[best_candidate_index]\n",
    "    input_text = input_text_per_candidate[best_candidate_index]\n",
    "    output_text = output_text_per_candidate[best_candidate_index]\n",
    "    \n",
    "    return {'generated_text': result_text, 'attention': attention_stack, \"input_text\" : input_text, \"output_text\" : output_text, \"candidates\": result_text_per_candidate}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_AND_SAVE_TRANFORMER:\n",
    "    transformer.use_tf_function = False\n",
    "    for i, o in dataset.take(2):\n",
    "        print(transformer.train_step([i, o]))\n",
    "        print(transformer.test_step([i, o]))\n",
    "    print()\n",
    "\n",
    "\n",
    "    transformer.use_tf_function = True\n",
    "    for i, o in dataset.take(2):\n",
    "        print(transformer.train_step([i, o]))\n",
    "        print(transformer.test_step([i, o]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_AND_SAVE_TRANFORMER:\n",
    "    history_transformer = transformer.fit(dataset, epochs=epochs+50, shuffle= True,#6000\n",
    "                            steps_per_epoch = train_size, callbacks=[PlotLossesCallback()], validation_data=val_dataset, validation_steps=10)\n",
    "    transformer.save_weights(\"./checkpoints/transformer_ckpt\")\n",
    "\n",
    "    with open('./checkpoints/history_transformer.pickle', 'wb') as handle:\n",
    "        pickle.dump(history_transformer.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    print(\"Loading saved data ...\")\n",
    "    transformer = Transformer( num_layers=4, d_model=emb_dim, embeddings_matrix = embeddings_matrix, window_size = window_size, num_heads=10, dff=256, vocab_len=vocab_len, rate=0.1)\n",
    "    transformer.load_weights(\"./checkpoints/transformer_ckpt\")\n",
    "\n",
    "    with open('./checkpoints/history_transformer.pickle', 'rb') as handle:\n",
    "        history_transformer = pickle.load(handle)\n",
    "        plt.figure()\n",
    "        plt.plot(history_transformer['sparse_categorical_accuracy'])\n",
    "        plt.plot(history_transformer['val_sparse_categorical_accuracy'])\n",
    "        plt.title('Model accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(history_transformer['loss'])\n",
    "        plt.plot(history_transformer['val_loss'])\n",
    "        plt.title('Model training loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(history_transformer['perplexity'])\n",
    "        plt.plot(history_transformer['val_perplexity'])\n",
    "        plt.title('Model perplexity ')\n",
    "        plt.ylabel('Perplexity')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(history_transformer['custom_acc'])\n",
    "        plt.plot(history_transformer['val_custom_acc'])\n",
    "        plt.title('Model Custom acc')\n",
    "        plt.ylabel('Custom acc')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, o in val_dataset.take(1):\n",
    "    print(i[0])\n",
    "    print(' '.join([tokenizer.index_word[x.numpy()] for x in i[0] if x.numpy() != 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sent = \"<sos> no inverno , as familias necessitadas\"\n",
    "\n",
    "#tokenizer.texts_to_sequences(\"<sos> o rato roeu a roupa do rei de roma \")\n",
    "#inp_seq = np.array([tokenizer.word_index[x] for x in input_sent.split()])\n",
    "\n",
    "inp_seq = np.array([ w[0] for w in  tokenizer.texts_to_sequences(input_sent.split(\" \")) if len(w)>0])\n",
    "temp = 0.9\n",
    "candidates_n = 50\n",
    "max_len = 30\n",
    "\n",
    "inp_seq#, tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE_TEXT:\n",
    "    result1 = generate_sentence(model, input_tokens = tf.expand_dims(inp_seq,0), max_length= max_len, temperature= temp , number_of_candidates=candidates_n, no_state = False)\n",
    "    result2 = generate_sentence(model, input_tokens = tf.expand_dims(inp_seq,0), max_length= max_len, temperature= temp , number_of_candidates=candidates_n , loop= True)\n",
    "\n",
    "    print(\"RESULT WITH STATE : \",result1[\"generated_text\"],\"\\n\")\n",
    "    print(\"RESULT WITH STATE AND LOOP : \",result2[\"generated_text\"],\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention with causal mask on upper triangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE_TEXT:\n",
    "    result = generate_sentence(model_causal1, input_tokens = tf.expand_dims(inp_seq,0), max_length= max_len, temperature= temp , number_of_candidates=candidates_n, no_state = True)\n",
    "    result2 = generate_sentence(model_causal1, input_tokens = tf.expand_dims(inp_seq,0), max_length= max_len, temperature= temp , number_of_candidates=candidates_n , loop= True)\n",
    "\n",
    "    plot_weights(result, stop_time = 3, loop = False)\n",
    "    print(\"RESULT NO STATE : \",result[\"generated_text\"],\"\\n\")\n",
    "    print(\"RESULT WITH STATE AND LOOP : \",result2[\"generated_text\"],\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer with causal mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE_TEXT:\n",
    "    result = generate_sentence_for_transformer(transformer, input_tokens = tf.expand_dims(inp_seq,0), max_length= max_len, temperature= temp , number_of_candidates=candidates_n, )\n",
    "    result1 = generate_sentence_for_transformer(transformer, input_tokens = tf.expand_dims(inp_seq,0), max_length= max_len, temperature= temp , number_of_candidates=candidates_n , loop= True)\n",
    "    print(\"RESULT  : \",result[\"generated_text\"],\"\\n\")\n",
    "    print(\"RESULT WITH  LOOP : \",result1[\"generated_text\"],\"\\n\")\n",
    "\n",
    "    t = -1\n",
    "    plt.figure()\n",
    "    plot_attention(result['attention'][t][\"decoder_layer4_block1\"][0][-1],  result[\"output_text\"][t],  result[\"output_text\"][t])#input_text\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "TextGenerator-In progress.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "dffee67a034c262da22ecd866efe5fa1174a6876e1e96ed512a9acb5833498b8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('tf_cpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
