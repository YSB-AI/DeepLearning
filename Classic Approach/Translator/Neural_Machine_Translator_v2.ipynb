{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7V0bu-DHrN_4",
        "outputId": "7fa863a4-058c-411e-a0a6-03a28a6e04c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.7/dist-packages (0.9.2)\n",
            "Requirement already satisfied: livelossplot in /usr/local/lib/python3.7/dist-packages (0.5.5)\n",
            "Requirement already satisfied: googletrans in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
            "Requirement already satisfied: Rouge in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: keras_tuner in /usr/local/lib/python3.7/dist-packages (1.1.3)\n",
            "Requirement already satisfied: talos in /usr/local/lib/python3.7/dist-packages (1.3)\n",
            "Requirement already satisfied: bayesian-optimization in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext) (2.10.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.21.6)\n",
            "Requirement already satisfied: ipython==7.* in /usr/local/lib/python3.7/dist-packages (from livelossplot) (7.9.0)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.7/dist-packages (from livelossplot) (2.3.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from livelossplot) (3.2.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (4.4.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (0.2.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (2.6.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (4.8.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (0.18.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (2.0.10)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython==7.*->livelossplot) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython==7.*->livelossplot) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython==7.*->livelossplot) (1.15.0)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.7/dist-packages (from googletrans) (0.13.3)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans) (2022.9.1)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans) (1.5.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans) (1.3.0)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans) (2.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans) (2022.6.15)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans) (0.9.1)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans) (3.0.4)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.7/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans) (3.2.0)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.7/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans) (0.9.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.7/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans) (3.0.0)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.7/dist-packages (from keras_tuner) (1.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras_tuner) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras_tuner) (21.3)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from keras_tuner) (2.8.0)\n",
            "Requirement already satisfied: astetik in /usr/local/lib/python3.7/dist-packages (from talos) (1.13)\n",
            "Requirement already satisfied: chances in /usr/local/lib/python3.7/dist-packages (from talos) (0.1.9)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from talos) (0.0)\n",
            "Requirement already satisfied: wrangle in /usr/local/lib/python3.7/dist-packages (from talos) (0.7.2)\n",
            "Requirement already satisfied: tensorflow>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from talos) (2.8.2+zzzcolab20220719082949)\n",
            "Requirement already satisfied: kerasplotlib in /usr/local/lib/python3.7/dist-packages (from talos) (1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from talos) (1.3.5)\n",
            "Requirement already satisfied: statsmodels>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from talos) (0.12.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from talos) (4.64.1)\n",
            "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.7/dist-packages (from statsmodels>=0.11.0->talos) (0.5.2)\n",
            "Requirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.7/dist-packages (from statsmodels>=0.11.0->talos) (1.7.3)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->talos) (2022.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->talos) (2.8.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (1.1.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (14.0.6)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (2.0.7)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (2.8.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (1.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (1.1.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (3.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (0.26.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (1.48.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (3.3.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (0.5.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (4.1.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow>=2.0.0->talos) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.0.0->talos) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (3.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (1.35.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras_tuner) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras_tuner) (4.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras_tuner) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras_tuner) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->keras_tuner) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras_tuner) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->keras_tuner) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras_tuner) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras_tuner) (3.2.0)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.1.0)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (21.10.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0)\n",
            "Requirement already satisfied: geonamescache in /usr/local/lib/python3.7/dist-packages (from astetik->talos) (1.5.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from astetik->talos) (0.11.2)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (2.11.3)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (7.1.2)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (5.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.9->bokeh->livelossplot) (2.0.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras_tuner) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot) (0.11.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython==7.*->livelossplot) (0.7.0)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "!pip install fasttext livelossplot googletrans Rouge keras_tuner talos bayesian-optimization scikit-optimize\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJEHjTpouS8J",
        "outputId": "44817f31-ba92-43ad-a151-5d5d287431ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on TPU  ['10.97.125.26:8470']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xhXWJ-JegY9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcJJofzv5ebp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, SpatialDropout1D, Reshape,Softmax, Reshape, Activation, Dropout, Input, LSTM, Embedding,GRU, AdditiveAttention, Flatten,Bidirectional, TimeDistributed,RepeatVector,Permute,Concatenate,Multiply,BatchNormalization,Dot, LayerNormalization\n",
        "from tensorflow.keras.models import load_model, Model,Sequential, model_from_json\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from random import seed , choice\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "import time\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk import word_tokenize,pos_tag\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from collections import defaultdict        \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from pathlib import Path\n",
        "\n",
        "import fasttext.util\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "import pickle\n",
        "from livelossplot.inputs.tf_keras import PlotLossesCallback\n",
        "from googletrans import Translator\n",
        "translator = Translator(service_urls=['translate.googleapis.com'])\n",
        "from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors\n",
        "from scipy import spatial\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import time\n",
        "import copy\n",
        "from google.colab import drive\n",
        "import keras_tuner as kt\n",
        "import talos as ta\n",
        "from bayes_opt import BayesianOptimization, UtilityFunction\n",
        "from skopt.space import Integer, Categorical, Real\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VfkY7Ja3fMGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEM7AYvnpGjM"
      },
      "outputs": [],
      "source": [
        "PROCESS_DATA = False\n",
        "SHOW_DATASET_INFO = True\n",
        "PROCESS_TRAIN_DS = True\n",
        "GET_WORD_DICT = False \n",
        "TRAIN_MODEL = True\n",
        "RUN_NAIVE_APPROACH = False\n",
        "units=1024#256\n",
        "lr = 0.001 #0.0005\n",
        "batch_size = 128#128 best until now\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "batch_size = batch_size * tpu_strategy.num_replicas_in_sync\n",
        "\n",
        "emb_dim = 300\n",
        "ds_len = 1000000\n",
        "ds_train_size = 1000000\n",
        "trainset_percentage = 0.97\n",
        "training_epochs = 2000\n",
        "\n",
        "#np.random.seed(0)\n",
        "#tf.random.set_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poxUineRvVsL"
      },
      "outputs": [],
      "source": [
        "def get_embeddings_fb(ft,vocab):\n",
        "    embeddings_matrix = np.zeros((len(vocab)+1,emb_dim))\n",
        "    for word in vocab:\n",
        "        embeddings_matrix[vocab[word],:] = [float(i) for i in ft.get_word_vector(word)]\n",
        "            \n",
        "    return embeddings_matrix\n",
        "\n",
        "def get_naive_embeddings(vocab,tokenizer_lang,vocab_len,fname ):\n",
        "    embeddings_matrix = np.zeros((vocab_len,emb_dim))\n",
        "    f= open(fname, encoding=\"utf8\")\n",
        "    word_found = 0\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        if(values[0] in vocab):\n",
        "            word = values[0]\n",
        "            \n",
        "            try:\n",
        "                coefs = [float(i) for i in values[1:]]\n",
        "                #word = np.expand_dims(word, -1)[0]\n",
        "                word_index = tokenizer_lang[word]\n",
        "\n",
        "                embeddings_matrix[word_index,:] = coefs\n",
        "\n",
        "                if word_found <10:\n",
        "                    print(word,\" Found / Index :\",word_index)\n",
        "                    word_found +=1\n",
        "            except:\n",
        "                print(\"word : \",values[0])\n",
        "                continue\n",
        "    f.close()\n",
        "    \n",
        "    return embeddings_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnxGzMxirN_8"
      },
      "source": [
        "## Create dataset, Analyse and saved it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_9uNZCFrN_9"
      },
      "outputs": [],
      "source": [
        "\n",
        "Emoji_Dict ={\n",
        "    \":p\" : \"<>\" ,\n",
        "    \":)\" :\"<SMILE_SIMPLE>\",\n",
        "    \":(\":\"<SAD>\",\n",
        "    \":'(\":\"<CRY>\",\n",
        "    \":')\":\"<BLINK_EYE>\",\n",
        "    \":d\" : \"<BIG_SMILE>\", \n",
        "    \":-)\":\"<SMILE_HAPPY>\",\n",
        "    \";-)\":\"<SMILE_A_LOT>\",\n",
        "    \":<})\":\"<MOUSTACHE_SMILE>\",\n",
        "    \":-||\":\"<MAD>\",\n",
        "    \":-(\":\"<SAD>\", \n",
        "    \":'-(\":\"<CRY>\",\n",
        "    \":-))\":\"<HAPPY>\",\n",
        "    \":-*\": \"<KISS>\",\n",
        "    \":-P~\":\"<LICK>\", \n",
        "    \":-o\" :\"<SURPRISED>\",\n",
        "    \":-|\":\"<GRIM>\",\n",
        "    \":-/\":\"<PERPLEXED>\",\n",
        "    \"=:O\"  :\"<FRIGHTNED>\",\n",
        "    \"<3\": \"<LOVE>\",\n",
        "    \"*****\": \"<5_STARS>\",\n",
        "    \"****\": \"<4_STARS>\",\n",
        "    \"***\": \"<3_STARS>\",\n",
        "    \"**\": \"<2_STARS>\",\n",
        "    \"*\": \"<1_STARS>\",\n",
        "    \".org\" : \"<>\"\n",
        "    }\n",
        "\n",
        "translationTable = str.maketrans(\"áéíóúàèìòùâêîôûãõç\", \"aeiouaeiouaeiouaoc\")\n",
        "string_check= re.compile('[^a-zA-Z.,?!\\'<>]')#0-9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rq06C00RlhAH",
        "outputId": "f2f6ed66-5e55-4c79-9720-c2b53a19ff1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ],
      "source": [
        "language_model = fasttext.load_model('/content/drive/My Drive/Colab Notebooks/Translator/lid.176.ftz')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwTXhMB7rN_9"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def get_sentence_separated(text, naive_approach=False):\n",
        "    text_review = []\n",
        "\n",
        "    text = (text.replace(\"?\",\" ? \").replace(\"!\",\" ! \").replace(\".\",\" . \"))\n",
        "    text = text.replace(\"back to text\", \" \").replace(\" s.\", \" saint\").replace(\" s.a.\", \" sa\").replace(\"monterosso farmhouses\", \" \").replace(\" prof.\", \" prof\")\n",
        "    \n",
        "    return [ \"<SOS> \" +text+ \" <EOS> \" ]\n",
        "    \n",
        "def process_dataset(line):\n",
        "   \n",
        "    line = line.translate(translationTable)\n",
        "    line = ' '.join(line.split('\\t'))\n",
        "\n",
        "    if line.find(\".org\") != -1:\n",
        "        site = line.split(\".org\")[0].split()[-1]\n",
        "        line = line.replace(site+\".org\",\"<URL>\")\n",
        "    elif line.find(\".com\") != -1:\n",
        "        if len(line.split(\".com\"))> 0:\n",
        "            try:\n",
        "                site = line.split(\".com\")[0].split()[-1]\n",
        "                line = line.replace(site+\".com\",\"<URL>\")\n",
        "            except:\n",
        "                pass\n",
        "    \n",
        "    line = line.replace(\" st.\",\" saint\")\n",
        "    line = (re.sub('\\.+', \" . \", line).replace(\"<br />\",\" \")).replace(\"?\",\" ? \").replace(\" ! \",\" ! \")\n",
        "\n",
        "    for em in list(Emoji_Dict.keys()):\n",
        "        line = line.replace(em,Emoji_Dict[em])\n",
        "\n",
        "    stripped =[]\n",
        "    tmp = nltk.word_tokenize(line)\n",
        "    for w in tmp:\n",
        "        #if  w.isdigit():\n",
        "        #    stripped.append(\"<NUMBER>\")\n",
        "        #else:\n",
        "        stripped.append(w)\n",
        "\n",
        "    line = ' '.join(stripped)\n",
        "    line = re.sub(string_check, ' ', (line)\n",
        "                    .replace(\"-\",\" \")\n",
        "                    .replace(\"´\",\"'\")\n",
        "                    .replace(\"`\",\"'\")\n",
        "                    .replace(\"'nt\",\" not\")\n",
        "                    .replace(\"i'm\",\"i am\")\n",
        "                    .replace(\"i 'm\",\"i am\")\n",
        "                    #.replace(\"'s\",\" \")\n",
        "                    .replace(\"don't\",\"do not\")\n",
        "                    .replace(\"can't\",\"can not\")\n",
        "                    .replace(\"n't\",\" not\")\n",
        "                    .replace(\"'re\",\" are\")\n",
        "                    .replace(\"'d\",\" would\")\n",
        "                    .replace(\"'ve\",\" have\")\n",
        "                    .replace(\"'ll\",\" will\")\n",
        "                    .replace(\"'till\",\" until\")\n",
        "                    .replace(\"''\",\" \")\n",
        "                    .replace(\" b \",\" \")\n",
        "                    .replace(\" c \",\" \")\n",
        "                    .replace(\" d \",\" \")\n",
        "                    .replace(\" f \",\" \")\n",
        "                    .replace(\" g \",\" \")\n",
        "                    .replace(\" h \",\" \")\n",
        "                    .replace(\" j \",\" \")\n",
        "                    .replace(\" k \",\" \")\n",
        "                    .replace(\" l \",\" \")\n",
        "                    .replace(\" m \",\" \")\n",
        "                    .replace(\" n \",\" \")\n",
        "                    .replace(\" p \",\" \")\n",
        "                    .replace(\" q \",\" \")\n",
        "                    .replace(\" r \",\" \")\n",
        "                    .replace(\" s \",\" \")\n",
        "                    .replace(\" t \",\" \")\n",
        "                    .replace(\" v \",\" \")\n",
        "                    .replace(\" w \",\" \")\n",
        "                    )\n",
        "        \n",
        "    return  ' '.join(line.split())\n",
        "\n",
        "    \n",
        "def read_dataset():\n",
        "    \n",
        "    english_list = []\n",
        "    english_size = []\n",
        "\n",
        "    portuguese_list = []\n",
        "    portuguese_size = []\n",
        "\n",
        "    portuguese_naive_list = []\n",
        "    english_naive_list = []\n",
        "\n",
        "    path = \"/content/drive/My Drive/Colab Notebooks/Translator/en-pt.bicleaner07.txt\"\n",
        "\n",
        "    f=open(path,'r', encoding=\"utf8\")\n",
        "    Lines = f.readlines()\n",
        "    Lines = Lines[0:ds_len]\n",
        "    ds_size = len(Lines)\n",
        "    print(\"Progress show at every \",int(ds_size*0.05),\" lines\")\n",
        "\n",
        "    for i, vec in enumerate(Lines):\n",
        "        vec = str(vec).lower()\n",
        "        vec = vec.split('\\t')\n",
        "\n",
        "        eng_line = vec[0]\n",
        "        pt_line = vec[1]\n",
        "\n",
        "        if i% int(ds_size*0.05) == 0 and i>0:\n",
        "            print(\"Progress : \",ds_size,\"/\",i)\n",
        "            break\n",
        "\n",
        "        english_list.append(process_dataset(eng_line))\n",
        "        portuguese_list.append(process_dataset(pt_line))\n",
        "        \n",
        "    f.close()\n",
        "\n",
        "    english_list_review = []\n",
        "    pt_list_review = []\n",
        "    eng_expected_lang= []\n",
        "    pt_expected_lang = []\n",
        "    for i , sent in enumerate(zip(english_list, portuguese_list)):\n",
        "        en_txt, pt_txt = sent\n",
        "\n",
        "        eng_lang = language_model.predict(en_txt, k=1)[0][0]\n",
        "        pt_lang = language_model.predict(pt_txt, k=1)[0][0]\n",
        "\n",
        "        en = get_sentence_separated(en_txt, naive_approach=False)\n",
        "        pt = get_sentence_separated(pt_txt, naive_approach=False)\n",
        "        if len(en) == len(pt):\n",
        "            for s in en:\n",
        "                english_size.append(len(s.split()))\n",
        "                english_list_review.append(s)\n",
        "                english_naive_list.append(s.replace(\".\",\" \").replace(\"'\",\" \"))\n",
        "                eng_expected_lang.append(eng_lang)\n",
        "            for s in pt:\n",
        "                portuguese_size.append(len(s.split()))\n",
        "                pt_list_review.append(s)\n",
        "                s_naive = s.replace(\".\",\" \").replace(\"'\",\" \")\n",
        "                portuguese_naive_list.append(s_naive)\n",
        "                pt_expected_lang.append(pt_lang)\n",
        "\n",
        "              \n",
        "\n",
        "    cols = ['eng','pt','eng_sentence_size', 'pt_sentence_size']\n",
        "    df_tmp= pd.DataFrame(columns=cols)\n",
        "\n",
        "    df_tmp[\"eng\"] =english_list_review\n",
        "    df_tmp[\"pt\"] =pt_list_review\n",
        "    df_tmp[\"eng_expected_lang\"] =eng_expected_lang\n",
        "    df_tmp[\"pt_expected_lang\"] =pt_expected_lang\n",
        "    df_tmp[\"english_naive\"] =english_naive_list\n",
        "    df_tmp[\"portuguese_naive\"] =portuguese_naive_list\n",
        "    df_tmp[\"eng_sentence_size\"] =english_size\n",
        "    df_tmp[\"pt_sentence_size\"] =portuguese_size\n",
        "\n",
        "    return  df_tmp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4Tj8Ry2rN_-"
      },
      "outputs": [],
      "source": [
        "#https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
        "\n",
        "def describre_dataset(dataset):\n",
        "    plt.figure()\n",
        "    pd.Series(dataset.eng_sentence_size).hist()\n",
        "    plt.title(\"Sentence Size for training dataset\")\n",
        "\n",
        "    plt.show()\n",
        "    print(pd.Series(dataset.pt_sentence_size).describe())\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "def create_trainingdev(eng_sequence, pt_sequence, df,maxlen, pt_max_len, en_max_len):\n",
        "    \n",
        "    y = pad_sequences(eng_sequence, padding='post', maxlen=en_max_len)\n",
        "    x = pad_sequences(pt_sequence, padding='post', maxlen=pt_max_len)\n",
        "    \n",
        "    x_train, x_dev, y_train, y_dev = train_test_split(x, y, test_size=(1-trainset_percentage), random_state=0)\n",
        "    print(\"x_in shape\", x_train.shape)\n",
        "    print(\"y_in shape\", y_train.shape)\n",
        "\n",
        "    \"\"\"train_elements_index = np.random.choice(len(x), int(len(x)*trainset_percentage), replace=False) \n",
        "    print(\"train_elements_index length : \",len(train_elements_index))\n",
        "    \n",
        "    x_train = np.take(x, train_elements_index, 0)\n",
        "    y_train = np.take(y, train_elements_index, 0)\n",
        "    print(\"x_in shape\", x_train.shape)\n",
        "    print(\"y_in shape\", y_train.shape)\n",
        "\n",
        "\n",
        "    x_dev= np.delete(x, train_elements_index, 0)\n",
        "    y_dev= np.delete(y, train_elements_index, 0)\"\"\"\n",
        "    print(\"x_dev shape\", x_dev.shape)\n",
        "    print(\"y_test shape\", y_dev.shape)\n",
        "\n",
        "    return x_train, y_train, x_dev, y_dev \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "sk-IxYY9rN__",
        "outputId": "b0f8ee2c-c0fc-4e9d-a9bb-b4b9b5f29896"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-c827d6a63d19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-c85340c548f5>\u001b[0m in \u001b[0;36mread_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mLines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0mLines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mds_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mds_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "df = read_dataset()\n",
        "df = df.drop_duplicates()\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nos79M4uu5wj"
      },
      "source": [
        "# Check text languages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbG7I3p9oTID"
      },
      "outputs": [],
      "source": [
        "# eng_expected_lang =  language_model.predict(df.eng.tolist(), k=1)[0]\n",
        "# pt_expected_lang=  language_model.predict(df.pt.tolist(), k=1)[0]\n",
        "\n",
        "# eng_expected_lang = [x[0] for x in eng_expected_lang]\n",
        "# pt_expected_lang = [x[0] for x in pt_expected_lang]\n",
        "# df[\"eng_expected_lang\"] =  eng_expected_lang\n",
        "# df[\"pt_expected_lang\"] =  pt_expected_lang\n",
        "# df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4mz3Ta00xoG"
      },
      "outputs": [],
      "source": [
        "df[\"eng_expected_lang\"].unique(), df[\"pt_expected_lang\"].unique()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WG3lGr0p1YFf"
      },
      "outputs": [],
      "source": [
        "df[\"eng_expected_lang\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKQwq-Wlj-uT"
      },
      "source": [
        "As there are many different languages in the data, let's fix this and include only sentences with portuguese or english based languages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qpRmq5W1dKG"
      },
      "outputs": [],
      "source": [
        "df[df.eng_expected_lang != \"__label__en\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0m8zB_jE5FUA"
      },
      "outputs": [],
      "source": [
        "df[\"pt_expected_lang\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHg9IlDH5AK5"
      },
      "outputs": [],
      "source": [
        "df[df.eng_expected_lang != \"__label__pt\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AWYw0dT5AOg"
      },
      "outputs": [],
      "source": [
        "df = df[df.eng_expected_lang.isin([\"__label__en\",\"__label__uk\"])]\n",
        "df = df[df.pt_expected_lang.isin([\"__label__pt\",\"__label__br\"])]\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4rSWhLgu_4s"
      },
      "source": [
        "# Remove sentence outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ow4oSX88otw"
      },
      "outputs": [],
      "source": [
        "sns.boxplot(x=df['eng_sentence_size'])\n",
        "df.eng_sentence_size.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lyvEVvd89Xb"
      },
      "outputs": [],
      "source": [
        "sns.boxplot(x=df['pt_sentence_size'])\n",
        "df.pt_sentence_size.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqnKSdTkkK_m"
      },
      "source": [
        "Tukeys' rule approach to fix outliers, considering a factor of 1.5 (not extreme outliers, as we have a huge dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7bURYS-9feN"
      },
      "outputs": [],
      "source": [
        "Q1_en = df.eng_sentence_size.quantile(0.25)\n",
        "Q3_en = df.eng_sentence_size.quantile(0.75)\n",
        "IQR_en = Q3_en - Q1_en\n",
        "factor = 1.5\n",
        "IQR_en, (Q1_en - factor * IQR_en),(Q3_en + factor * IQR_en), len(df[ df.eng_sentence_size <= (Q3_en + factor * IQR_en)]), len(df[ df.eng_sentence_size > (Q3_en + factor * IQR_en)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HJRXs5_9sly"
      },
      "outputs": [],
      "source": [
        "Q1_pt = df.pt_sentence_size.quantile(0.25)\n",
        "Q3_pt = df.pt_sentence_size.quantile(0.75)\n",
        "IQR_pt = Q3_pt - Q1_pt\n",
        "IQR_pt, (Q1_pt - 1.5 * IQR_pt),(Q3_pt + 1.5 * IQR_pt), len(df[ df.pt_sentence_size <= (Q3_pt + 1.5 * IQR_pt)]), len(df[ df.pt_sentence_size > (Q3_pt + 1.5 * IQR_pt)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQfSxUGr_ZJ6"
      },
      "outputs": [],
      "source": [
        "IQR_total = np.minimum((Q3_pt + 1.5 * IQR_pt),(Q3_en + 1.5 * IQR_en))\n",
        "IQR_total "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NL7YAPdrN__"
      },
      "outputs": [],
      "source": [
        "if SHOW_DATASET_INFO :\n",
        "    describre_dataset(df)\n",
        "    #IQR_total = 120 # REVIEW LATER\n",
        "    df = df[df.eng_sentence_size<IQR_total]\n",
        "    df = df[df.eng_sentence_size> 0]#6\n",
        "\n",
        "    if ds_train_size < ds_len:\n",
        "      df = df.sample(n=ds_train_size, random_state=1).reset_index().drop([\"index\"], axis  =1)\n",
        "    describre_dataset(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWUr9tX7kzRI"
      },
      "outputs": [],
      "source": [
        "df.duplicated().any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcugABXqleif"
      },
      "outputs": [],
      "source": [
        "df.isna().any()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tf1hCkJ3rOAA"
      },
      "source": [
        "## Create, save and read train dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaOKPAU1rOAA"
      },
      "outputs": [],
      "source": [
        "if PROCESS_DATA :\n",
        "    #df = read_dataset()\n",
        "    \n",
        "    eng_tokenizer = Tokenizer(filters='\"#$%&()*+-/:;=@[\\\\]^_´`«»{|}~\\t\\n',oov_token='<oov>')# sequence_size = 500 ~= mean+ std sentence size\n",
        "    eng_tokenizer.fit_on_texts(df.eng.values.tolist())\n",
        "\n",
        "    pt_tokenizer = Tokenizer(filters='\"#$%&()*+-/:;=@[\\\\]^_´`«»{|}~\\t\\n',oov_token='<oov>')# sequence_size = 500 ~= mean+ std sentence size\n",
        "    pt_tokenizer.fit_on_texts(df.pt.values.tolist())\n",
        "\n",
        "    with open('/content/drive/My Drive/Colab Notebooks/Translator/eng_tokenizer.pkl', 'wb') as f:\n",
        "            pickle.dump(eng_tokenizer, f)\n",
        "    \n",
        "    with open('/content/drive/My Drive/Colab Notebooks/Translator/pt_tokenizer.pkl', 'wb') as f:\n",
        "            pickle.dump(pt_tokenizer, f)\n",
        "\n",
        "    df.to_pickle(\"/content/drive/My Drive/Colab Notebooks/Translator//df_init.pkl\")  \n",
        "else:\n",
        "\n",
        "    df = pd.read_pickle(\"/content/drive/My Drive/Colab Notebooks/Translator/df_init.pkl\")\n",
        "    \n",
        "    with open('/content/drive/My Drive/Colab Notebooks/Translator/eng_tokenizer.pkl', 'rb') as f:\n",
        "        eng_tokenizer = pickle.load(f)\n",
        "\n",
        "    with open('/content/drive/My Drive/Colab Notebooks/Translator/pt_tokenizer.pkl', 'rb') as f:\n",
        "        pt_tokenizer = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zy9e30Tu1Gy"
      },
      "source": [
        "# Check words frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9I2MNEIt927"
      },
      "outputs": [],
      "source": [
        "def check_word_frequency(tokenizer_temp):\n",
        "    word_frequency = {}\n",
        "    for i, (word, count) in enumerate(tokenizer_temp.word_counts.items()):\n",
        "        word_frequency[word]=count\n",
        "            \n",
        "    return word_frequency\n",
        "\n",
        "eng_frequency = check_word_frequency(eng_tokenizer)\n",
        "pt_frequency = check_word_frequency(pt_tokenizer)\n",
        "\n",
        "eng_frequency = {k: v for k, v in sorted(eng_frequency.items(), key=lambda item: item[1], reverse=True)}\n",
        "pt_frequency = {k: v for k, v in sorted(pt_frequency.items(), key=lambda item: item[1],reverse=True)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5ov31g-zB51"
      },
      "outputs": [],
      "source": [
        "eng_frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMMZEBFqzMyi"
      },
      "outputs": [],
      "source": [
        "pt_frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gELdPRIAxydn"
      },
      "outputs": [],
      "source": [
        "eng_frequency_tmp = {k:v for (k,v) in eng_frequency.items() if v > 5000}\n",
        "plt.bar(*zip(*eng_frequency_tmp.items()))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSqOHafhyZi0"
      },
      "outputs": [],
      "source": [
        "pt_frequency_tmp = {k:v for (k,v) in pt_frequency.items() if v > 5000}\n",
        "plt.bar(*zip(*pt_frequency_tmp.items()))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Gw8qZWNkjmm"
      },
      "source": [
        "As seem, there are multiple words/characters with a really huge frequency, which can bias the model. But since we are translating them, I guess we can't just remove these stop words and special characters like , . the, etc..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snzt8cktuf5m"
      },
      "outputs": [],
      "source": [
        "# plt.bar(*zip(*pt_frequency.items()))\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgGwcoV_oyFj"
      },
      "outputs": [],
      "source": [
        "eng_sequence = eng_tokenizer.texts_to_sequences(df.eng.tolist())\n",
        "pt_sequence = pt_tokenizer.texts_to_sequences(df.pt.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvbjbA7zkzPZ"
      },
      "source": [
        "Let's debug our sequence, to understand if we are setting the right english/portuguese pair as input and output "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1Iol6MOmbKH"
      },
      "outputs": [],
      "source": [
        "for en_sent,pt_sent,ptss,engss in zip(eng_sequence[10000:10100],pt_sequence[10000:10100],df.pt[10000:10100].tolist(),df.eng[10000:10100].tolist()):\n",
        "  #for x in pt_sent:\n",
        "    #print(x,\"---->\",pt_tokenizer.index_word[x])\n",
        "  print(\"ORIGINAL : \",ptss,\"--------->\",engss)\n",
        "  pts = ' '.join([pt_tokenizer.index_word[x] for x in pt_sent])\n",
        "  ens = ' '.join([eng_tokenizer.index_word[x] for x in en_sent])\n",
        "  print(pts,\"--------->\",ens)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIKEeQWwk9nw"
      },
      "source": [
        "# Getting Embeddings Matrices for our sequences, both Portuguese and English"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIxABOwPrOAA"
      },
      "outputs": [],
      "source": [
        "eng_vocab_size = len(pt_tokenizer.word_index) +1\n",
        "pt_vocab_size = len(eng_tokenizer.word_index) +1\n",
        "\n",
        "ft = fasttext.load_model('/content/drive/My Drive/Colab Notebooks/Translator/cc.en.300.bin')\n",
        "fasttext.util.reduce_model(ft, emb_dim)\n",
        "embeddings_eng_matrix = get_embeddings_fb(ft,eng_tokenizer.word_index)\n",
        "del ft\n",
        "\n",
        "ft = fasttext.load_model('/content/drive/My Drive/Colab Notebooks/Translator/cc.pt.300.bin')\n",
        "fasttext.util.reduce_model(ft, emb_dim)\n",
        "embeddings_pt_matrix = get_embeddings_fb(ft,pt_tokenizer.word_index)\n",
        "\n",
        "sequence_size = np.maximum(df.pt_sentence_size.max(),df.eng_sentence_size.max())\n",
        "\n",
        "embeddings_eng_matrix, embeddings_pt_matrix, sequence_size\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZUC1bW1lIx1"
      },
      "source": [
        "# Word translation for Naive Approach\n",
        "\n",
        "Setting Word dictionary containing every portuguese/english word pair translation. Naive Approach focus in words, not sentences. So, instead of having the sentence pair of translation, we need the word pair of translation. For this, we need to use the google translator api to translate each word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVtipxS_rOAA"
      },
      "outputs": [],
      "source": [
        "# TO use in the Naive Approach\n",
        "try:\n",
        "  if GET_WORD_DICT:\n",
        "      eng_vocab_list = list(eng_tokenizer.word_index.keys())\n",
        "      eng_vocab_list.remove(\"<oov>\")\n",
        "      eng_vocab_list.remove(\".\")\n",
        "      eng_vocab_list.remove(\"<eos>\")\n",
        "      eng_vocab_list.remove(\"<number>\")\n",
        "      \n",
        "      pt_vocab_list = []\n",
        "      google_batch = 1000\n",
        "      for i in range(0, len(eng_vocab_list), google_batch):\n",
        "          if (i+google_batch) <  len(eng_vocab_list):\n",
        "              aux = eng_vocab_list[i:i+google_batch]\n",
        "          else :\n",
        "              aux = eng_vocab_list[i:]\n",
        "\n",
        "          print(i,\"/\",len(eng_vocab_list))\n",
        "          translations = translator.translate(aux, src='en', dest='pt')\n",
        "          for t in translations:\n",
        "              pt_vocab_list.append(t.text)\n",
        "\n",
        "\n",
        "\n",
        "      words_dict = pd.DataFrame(columns=[\"eng_words\",\"pt_words\"])\n",
        "      words_dict[\"eng_words\"] = eng_vocab_list\n",
        "      words_dict[\"pt_words\"] = pt_vocab_list\n",
        "\n",
        "      words_dict.pt_words =  words_dict.pt_words.apply(lambda line : re.sub(string_check, ' ',line.lower().translate(translationTable)))\n",
        "\n",
        "      words_dict.loc[words_dict.eng_words==\"th\",\"pt_words\"]=\"nd\"\n",
        "      words_dict.pt_words =  words_dict.pt_words.apply(lambda x : ' '.join(x.split()) if len(x.split()) > 1 else x.split()[0])\n",
        "\n",
        "      words_dict.to_pickle(\"./words_dict.pkl\")  \n",
        "\n",
        "      pt_tokenizer_naive = pt_tokenizer\n",
        "      pt_tokenizer_naive.fit_on_texts(words_dict.pt_words.tolist())\n",
        "\n",
        "      with open('pt_tokenizer_naive.pkl', 'wb') as f:\n",
        "          pickle.dump(pt_tokenizer_naive, f)\n",
        "\n",
        "  else:\n",
        "\n",
        "      words_dict = pd.read_pickle(\"/content/drive/My Drive/Colab Notebooks/Translator/words_dict.pkl\")\n",
        "\n",
        "      with open('/content/drive/My Drive/Colab Notebooks/pt_tokenizer_naive.pkl', 'rb') as f:\n",
        "          pt_tokenizer_naive = pickle.load(f)\n",
        "except:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wMYbAuKrOAB"
      },
      "source": [
        "# Naive Approach -  Word translation \n",
        "\n",
        "We need to have a matrix R which will transform X into Y \n",
        "\n",
        "## 2.1 Translation as linear transformation of embeddings\n",
        "\n",
        "\n",
        "Given dictionaries of English and French word embeddings you will create a transformation matrix `R`\n",
        "* Given an English word embedding, $\\mathbf{e}$, you can multiply $\\mathbf{eR}$ to get a new word embedding $\\mathbf{f}$.\n",
        "    * Both $\\mathbf{e}$ and $\\mathbf{f}$ are [row vectors](https://en.wikipedia.org/wiki/Row_and_column_vectors).\n",
        "* You can then compute the nearest neighbors to `f` in the french embeddings and recommend the word that is most similar to the transformed word embedding.\n",
        "\n",
        "\n",
        "### Describing translation as the minimization problem\n",
        "\n",
        "Find a matrix `R` that minimizes the following equation. \n",
        "\n",
        "$$\\arg \\min _{\\mathbf{R}}\\| \\mathbf{X R} - \\mathbf{Y}\\|_{F}\\tag{1} $$\n",
        "\n",
        "### Frobenius norm\n",
        "\n",
        "The Frobenius norm of a matrix $A$ (assuming it is of dimension $m,n$) is defined as the square root of the sum of the absolute squares of its elements:\n",
        "\n",
        "$$\\|\\mathbf{A}\\|_{F} \\equiv \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n}\\left|a_{i j}\\right|^{2}}\\tag{2}$$\n",
        "\n",
        "\n",
        "### Actual loss function\n",
        "In the real world applications, the Frobenius norm loss:\n",
        "\n",
        "$$\\| \\mathbf{XR} - \\mathbf{Y}\\|_{F}$$\n",
        "\n",
        "is often replaced by it's squared value divided by $m$:\n",
        "\n",
        "$$ \\frac{1}{m} \\|  \\mathbf{X R} - \\mathbf{Y} \\|_{F}^{2}$$\n",
        "\n",
        "where $m$ is the number of examples (rows in $\\mathbf{X}$).\n",
        "\n",
        "* The same R is found when using this loss function versus the original Frobenius norm.\n",
        "* The reason for taking the square is that it's easier to compute the gradient of the squared Frobenius.\n",
        "* The reason for dividing by $m$ is that we're more interested in the average loss per embedding than the  loss for the entire training set.\n",
        "    * The loss for all training set increases with more words (training examples),\n",
        "    so taking the average helps us to track the average loss regardless of the size of the training set.\n",
        "\n",
        "\n",
        "### *********************************************************************************************************************************************\n",
        "\n",
        "#### 1: Computing the loss\n",
        "* The loss function will be squared Frobenoius norm of the difference between\n",
        "matrix and its approximation, divided by the number of training examples $m$.\n",
        "* Its formula is:\n",
        "$$ L(X, Y, R)=\\frac{1}{m}\\sum_{i=1}^{m} \\sum_{j=1}^{n}\\left( a_{i j} \\right)^{2}$$\n",
        "\n",
        "where $a_{i j}$ is value in $i$th row and $j$th column of the matrix $\\mathbf{XR}-\\mathbf{Y}$.\n",
        "\n",
        "\n",
        "### 2: Computing the gradient of loss in respect to transform matrix R\n",
        "\n",
        "* Calculate the gradient of the loss with respect to transform matrix `R`.\n",
        "* The gradient is a matrix that encodes how much a small change in `R`\n",
        "affect the change in the loss function.\n",
        "* The gradient gives us the direction in which we should decrease `R`\n",
        "to minimize the loss.\n",
        "* $m$ is the number of training examples (number of rows in $X$).\n",
        "* The formula for the gradient of the loss function $𝐿(𝑋,𝑌,𝑅)$ is:\n",
        "\n",
        "$$\\frac{d}{dR}𝐿(𝑋,𝑌,𝑅)=\\frac{d}{dR}\\Big(\\frac{1}{m}\\| X R -Y\\|_{F}^{2}\\Big) = \\frac{2}{m}X^{T} (X R - Y)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfRPMV3QrOAB"
      },
      "outputs": [],
      "source": [
        "def get_matrices(word_dict_df,eng_tokenizer,pt_tokenizer_naive,emb_dim, sequence_size, embeddings_eng_matrix_naive,embeddings_pt_matrix_naive):\n",
        "\n",
        "    eng_embedding_layer = Embedding(input_dim = len(eng_tokenizer.word_index)+1,\n",
        "        output_dim = emb_dim,\n",
        "        input_length = sequence_size ,\n",
        "        name =\"eng_embedding_layer\", \n",
        "        weights=[embeddings_eng_matrix_naive], \n",
        "        trainable=False, mask_zero=True\n",
        "        )\n",
        "\n",
        "    pt_embedding_layer = Embedding(input_dim = len(pt_tokenizer_naive.word_index)+1,\n",
        "        output_dim = emb_dim,\n",
        "        input_length = sequence_size ,\n",
        "        name =\"pt_embedding_layer\", \n",
        "        weights=[embeddings_pt_matrix_naive], \n",
        "        trainable=False, mask_zero=True\n",
        "        )\n",
        "        \n",
        "    X_l = list()\n",
        "    Y_l = list()\n",
        "\n",
        "    for en_word, pt_word in zip(word_dict_df.eng_words.tolist(), word_dict_df.pt_words.tolist()):\n",
        "\n",
        "        if len(pt_word.split())>1 :\n",
        "            pt_vec = [0]*emb_dim\n",
        "            try : \n",
        "                for  w in pt_word.split():\n",
        "                    pt_vec += pt_embedding_layer(pt_tokenizer_naive.word_index[w])\n",
        "                #pt_vec = pt_vec/\n",
        "            except:\n",
        "                print(f\"Failed to get complete sentence'{pt_word}'\" )\n",
        "                continue\n",
        "        else :\n",
        "            try :\n",
        "                pt_vec = pt_embedding_layer(pt_tokenizer_naive.word_index[pt_word])\n",
        "            except:\n",
        "                print(f\"Failed to get complete '{pt_word}'\" )\n",
        "                continue\n",
        "\n",
        "        en_vec = eng_embedding_layer(eng_tokenizer.word_index[en_word])\n",
        "\n",
        "        X_l.append(en_vec)\n",
        "        Y_l.append(pt_vec)\n",
        "\n",
        "    X_train = np.vstack(X_l)\n",
        "    Y_train = np.vstack(Y_l)\n",
        "\n",
        "    X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "    return eng_embedding_layer, pt_embedding_layer, X_train, Y_train, X_val, Y_val\n",
        "\n",
        "def compute_loss(X, Y, R):\n",
        "    '''\n",
        "    Inputs: \n",
        "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
        "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
        "        R: a matrix of dimension (n,n) - transformation matrix from English to French vector space embeddings.\n",
        "    Outputs:\n",
        "        L: a matrix of dimension (m,n) - the value of the loss function for given X, Y and R.\n",
        "    '''\n",
        "    m = len(X)\n",
        "    diff = np.dot(X,R) - Y\n",
        "    diff_squared = diff**2\n",
        "    sum_diff_squared = np.sum(diff_squared)\n",
        "    loss = sum_diff_squared/m\n",
        "    return loss\n",
        "\n",
        "def compute_gradient(X, Y, R):\n",
        "    '''\n",
        "    Inputs: \n",
        "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
        "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
        "        R: a matrix of dimension (n,n) - transformation matrix from English to French vector space embeddings.\n",
        "    Outputs:\n",
        "        g: a scalar value - gradient of the loss function L for given X, Y and R.\n",
        "    '''\n",
        "    m = len(X)\n",
        "    gradient = (2/m)*np.dot((X.T), (np.dot(X,R)-Y))\n",
        "    return gradient\n",
        "\n",
        "\n",
        "def align_embeddings(X, Y, train_steps=100, learning_rate=0.0003, verbose=True, compute_loss=compute_loss, compute_gradient=compute_gradient):\n",
        "    '''\n",
        "    Inputs:\n",
        "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
        "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
        "        train_steps: positive int - describes how many steps will gradient descent algorithm do.\n",
        "        learning_rate: positive float - describes how big steps will  gradient descent algorithm do.\n",
        "    Outputs:\n",
        "        R: a matrix of dimension (n,n) - the projection matrix that minimizes the F norm ||X R -Y||^2\n",
        "    '''\n",
        "    np.random.seed(129)\n",
        "\n",
        "    # the number of columns in X is the number of dimensions for a word vector (e.g. 300)\n",
        "    # R is a square matrix with length equal to the number of dimensions in th  word embedding\n",
        "    R = np.random.rand(X.shape[1], X.shape[1])\n",
        "\n",
        "    for i in range(train_steps):\n",
        "        if verbose and i % 1000 == 0:\n",
        "            print(f\"loss at iteration {i} is: {compute_loss(X, Y, R):.4f}\")\n",
        "        gradient = compute_gradient(X, Y, R)\n",
        "        # update R by subtracting the learning rate times gradient\n",
        "        R -= learning_rate * gradient\n",
        "\n",
        "    return R\n",
        "\n",
        "\n",
        "def nearest_neighbor_adapted(v, candidates, k=1):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "      - v, the vector you are going find the nearest neighbor for\n",
        "      - candidates: a set of vectors where we will find the neighbors\n",
        "      - k: top k nearest neighbors to find\n",
        "    Output:\n",
        "      - k_idx: the indices of the top k closest vectors in sorted form\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ###\n",
        "    similarity_l = []\n",
        "\n",
        "    # for each candidate vector...\n",
        "    v_array = v.reshape((1,candidates.shape[1]))\n",
        "\n",
        "    cos_similarity = cosine_similarity(v_array, candidates)\n",
        "    similarity_l = list(cos_similarity[0,:])\n",
        "    \n",
        "    # sort the similarity list and get the indices of the sorted list    \n",
        "    sorted_ids = np.argsort(similarity_l)  \n",
        "    # Reverse the order of the sorted_ids array\n",
        "    sorted_ids = np.argsort(similarity_l)[::-1]\n",
        "    # get the indices of the k most similar candidate vectors\n",
        "    k_idx = sorted_ids[:k]\n",
        "    ### END CODE HERE ###\n",
        "    return k_idx\n",
        "\n",
        "    \n",
        "def test_vocabulary(X, Y, R, nearest_neighbor=nearest_neighbor_adapted):\n",
        "    '''\n",
        "    Input:\n",
        "        X: a matrix where the columns are the English embeddings.\n",
        "        Y: a matrix where the columns correspong to the French embeddings.\n",
        "        R: the transform matrix which translates word embeddings from\n",
        "        English to French word vector space.\n",
        "    Output:\n",
        "        accuracy: for the English to French capitals\n",
        "    '''\n",
        "\n",
        "    pred = np.dot(X,R)\n",
        "\n",
        "    # initialize the number correct to zero\n",
        "    num_correct = 0\n",
        "\n",
        "    # loop through each row in pred (each transformed embedding)\n",
        "    for i in range(len(pred)):\n",
        "        # get the index of the nearest neighbor of pred at row 'i'; also pass in the candidates in Y\n",
        "        pred_idx = nearest_neighbor(pred[i],Y)[0]\n",
        "        # if the index of the nearest neighbor equals the row of i... \\\n",
        "        if pred_idx == i:\n",
        "            # increment the number correct by 1.\n",
        "            num_correct += 1\n",
        "\n",
        "    # accuracy is the number correct divided by the number of rows in 'pred' (also number of rows in X)\n",
        "    accuracy = num_correct/len(pred)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yk1gLlK7rOAC"
      },
      "outputs": [],
      "source": [
        "RUN_NAIVE_APPROACH = False\n",
        "if RUN_NAIVE_APPROACH:\n",
        "\n",
        "    embeddings_eng_matrix_naive =  get_embeddings_fb(ft,eng_tokenizer.word_index)\n",
        "    embeddings_pt_matrix_naive = get_embeddings_fb(ft,pt_tokenizer_naive.word_index)\n",
        "    eng_embedding_layer, pt_embedding_layer,  X_train, Y_train, X_val, Y_val = get_matrices(words_dict, eng_tokenizer,pt_tokenizer_naive,emb_dim, sequence_size, embeddings_eng_matrix_naive, embeddings_pt_matrix_naive)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0s_EsZPrOAC"
      },
      "outputs": [],
      "source": [
        "if RUN_NAIVE_APPROACH:\n",
        "    #Calculate transformation matrix R\n",
        "    R_train = align_embeddings(X_train, Y_train, train_steps=200000, learning_rate=0.01) #1982.9182\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AulOue7ltdK"
      },
      "source": [
        "loss at iteration 0 is: 32.9974\n",
        "\n",
        "loss at iteration 1000 is: 14.2506\n",
        "\n",
        "loss at iteration 2000 is: 9.6578\n",
        "\n",
        "loss at iteration 3000 is: 7.4478\n",
        "\n",
        "loss at iteration 4000 is: 6.0210\n",
        "\n",
        "loss at iteration 5000 is: 4.9834\n",
        "\n",
        "loss at iteration 6000 is: 4.1927\n",
        "\n",
        "loss at iteration 7000 is: 3.5761\n",
        "\n",
        "loss at iteration 8000 is: 3.0881\n",
        "\n",
        "loss at iteration 9000 is: 2.6975\n",
        "\n",
        "loss at iteration 10000 is: 2.3817\n",
        "\n",
        "loss at iteration 11000 is: 2.1240\n",
        "\n",
        "loss at iteration 12000 is: 1.9121\n",
        "\n",
        "loss at iteration 13000 is: 1.7363\n",
        "\n",
        "loss at iteration 14000 is: 1.5894\n",
        "\n",
        "loss at iteration 15000 is: 1.4658\n",
        "\n",
        "loss at iteration 16000 is: 1.3610\n",
        "\n",
        "loss at iteration 17000 is: 1.2717\n",
        "\n",
        "loss at iteration 18000 is: 1.1949\n",
        "\n",
        "loss at iteration 19000 is: 1.1286\n",
        "\n",
        "loss at iteration 20000 is: 1.0709\n",
        "\n",
        "loss at iteration 21000 is: 1.0205\n",
        "\n",
        "loss at iteration 22000 is: 0.9762\n",
        "\n",
        "loss at iteration 23000 is: 0.9370\n",
        "\n",
        "loss at iteration 24000 is: 0.9022\n",
        "\n",
        "loss at iteration 25000 is: 0.8712\n",
        "\n",
        "loss at iteration 26000 is: 0.8434\n",
        "\n",
        "loss at iteration 27000 is: 0.8184\n",
        "\n",
        "loss at iteration 28000 is: 0.7958\n",
        "\n",
        "loss at iteration 29000 is: 0.7753\n",
        "\n",
        "loss at iteration 30000 is: 0.7567\n",
        "\n",
        "loss at iteration 31000 is: 0.7397\n",
        "\n",
        "loss at iteration 32000 is: 0.7241\n",
        "\n",
        "loss at iteration 33000 is: 0.7098\n",
        "\n",
        "loss at iteration 34000 is: 0.6967\n",
        "\n",
        "loss at iteration 35000 is: 0.6846\n",
        "\n",
        "loss at iteration 36000 is: 0.6734\n",
        "\n",
        "loss at iteration 37000 is: 0.6630\n",
        "\n",
        "loss at iteration 38000 is: 0.6534\n",
        "\n",
        "loss at iteration 39000 is: 0.6445\n",
        "\n",
        "loss at iteration 40000 is: 0.6361\n",
        "\n",
        "loss at iteration 41000 is: 0.6284\n",
        "\n",
        "loss at iteration 42000 is: 0.6211\n",
        "\n",
        "loss at iteration 43000 is: 0.6144\n",
        "\n",
        "loss at iteration 44000 is: 0.6081\n",
        "\n",
        "loss at iteration 45000 is: 0.6021\n",
        "\n",
        "loss at iteration 46000 is: 0.5966\n",
        "\n",
        "loss at iteration 47000 is: 0.5914\n",
        "\n",
        "loss at iteration 48000 is: 0.5865\n",
        "\n",
        "loss at iteration 49000 is: 0.5819\n",
        "\n",
        "loss at iteration 50000 is: 0.5775\n",
        "\n",
        "loss at iteration 51000 is: 0.5734\n",
        "\n",
        "loss at iteration 52000 is: 0.5696\n",
        "\n",
        "loss at iteration 53000 is: 0.5660\n",
        "\n",
        "loss at iteration 54000 is: 0.5626\n",
        "\n",
        "...\n",
        "loss at iteration 172000 is: 0.4993\n",
        "\n",
        "loss at iteration 173000 is: 0.4993"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zefKiwYrOAC"
      },
      "outputs": [],
      "source": [
        "if RUN_NAIVE_APPROACH:\n",
        "    acc = test_vocabulary(X_val, Y_val, R_train)  # this might take a minute or two\n",
        "    print(f\"accuracy on test set is {acc:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyWXP6VsrOAC"
      },
      "outputs": [],
      "source": [
        "if RUN_NAIVE_APPROACH:\n",
        "    v = embeddings_eng_matrix_naive[eng_tokenizer.word_index[\"you\"]]\n",
        "    candidates = embeddings_eng_matrix_naive\n",
        "    indices = nearest_neighbor_adapted(v, candidates, 5)\n",
        "    print(indices,\"-->\", [pt_tokenizer_naive.index_word[x] for x in indices])#, \"-->\",candidates[indices])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PF00HUzhrOAC"
      },
      "outputs": [],
      "source": [
        "if RUN_NAIVE_APPROACH:\n",
        "    #Calculate transformation matrix R\n",
        "    R_train = align_embeddings(X_train[:500], Y_train[:500], train_steps=10000, learning_rate=0.1) #1982.9182\n",
        "\n",
        "    acc = test_vocabulary(X_val[:300], Y_val[:300], R_train)  # this might take a minute or two\n",
        "    print(f\"accuracy on test set is {acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uot92qLtrOAC"
      },
      "source": [
        "# Deep Learning Approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRdkiHB8mEd9"
      },
      "source": [
        "Create the dataset for deep learning approach, as long os the data parameters such as decoder and encoder size, training size, etc.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRaI3BE6rOAD"
      },
      "outputs": [],
      "source": [
        "\n",
        "pt_vocab_size = len(pt_tokenizer.word_index) +1\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) +1\n",
        "\n",
        "sequence_size = np.maximum(df.pt_sentence_size.max(),df.eng_sentence_size.max())\n",
        "pt_sequence_size = df.pt_sentence_size.max() # sequence_size\n",
        "en_sequence_size = df.eng_sentence_size.max() #sequence_size \n",
        "eng_sequence = eng_tokenizer.texts_to_sequences(df.eng.tolist())\n",
        "pt_sequence = pt_tokenizer.texts_to_sequences(df.pt.tolist())\n",
        "x, y, x_dev, y_dev, = create_trainingdev(eng_sequence,pt_sequence, df,sequence_size,pt_sequence_size,en_sequence_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvqAJyl_rOAD"
      },
      "outputs": [],
      "source": [
        "dataset_fit = tf.data.Dataset.from_tensor_slices((x,y))\n",
        "dataset_fit = dataset_fit.shuffle(buffer_size = 10000).batch(batch_size=batch_size, drop_remainder=True)\n",
        "dataset_fit_dev = tf.data.Dataset.from_tensor_slices((x_dev,y_dev))\n",
        "dataset_fit_dev = dataset_fit_dev.shuffle(buffer_size = 10000).batch(batch_size=batch_size, drop_remainder=True)\n",
        "\n",
        "dataset_fit_hp = tf.data.Dataset.from_tensor_slices((x,y))\n",
        "dataset_fit_hp =dataset_fit_hp.shuffle(buffer_size = 10000).batch(batch_size=batch_size, drop_remainder=True)\n",
        "dataset_fit_dev_hp = tf.data.Dataset.from_tensor_slices((x_dev,y_dev))\n",
        "dataset_fit_dev_hp = dataset_fit_dev_hp.shuffle(buffer_size = 10000).batch(batch_size=batch_size, drop_remainder=True)\n",
        "\n",
        "train_size = len(list(dataset_fit))\n",
        "\n",
        "dataset_fit = dataset_fit.repeat()\n",
        "dataset_fit_dev = dataset_fit_dev.repeat()\n",
        "\n",
        "del ft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPpUQaLBmSY-"
      },
      "outputs": [],
      "source": [
        "decoder_vocab_size = len(eng_tokenizer.word_index) +1\n",
        "encoder_vocab_size = len(pt_tokenizer.word_index) +1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfPecrUgmUjF"
      },
      "source": [
        "Double check the translation pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2dKphgYrOAD"
      },
      "outputs": [],
      "source": [
        "\n",
        "for xi,yi in dataset_fit.take(5):\n",
        "  print(' '.join([pt_tokenizer.index_word[i] for i in xi[0].numpy() if i != 0]))\n",
        "  print(' '.join([eng_tokenizer.index_word[i] for i in yi[0].numpy() if i != 0]), \"\\n ------------------------------- \\n\\n\")\n",
        "\n",
        "for xi_batch, yi_batch in dataset_fit_dev.take(1):\n",
        "    print(' '.join([pt_tokenizer.index_word[i] for i in xi[0].numpy() if i != 0]))\n",
        "    print(' '.join([eng_tokenizer.index_word[i] for i in yi[0].numpy() if i != 0]), \"\\n ------------------------------- \\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDFop2zhme_4"
      },
      "source": [
        "### Defining the Encoder, Decoder and Attention classes for models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnY15bszrOAD"
      },
      "outputs": [],
      "source": [
        "import matplotlib.ticker as ticker\n",
        "\n",
        "def plot_attention(attention, sentence, predicted_sentence,pltsize=(10, 10)):\n",
        "  sentence = sentence.split()\n",
        "  predicted_sentence = predicted_sentence.split()\n",
        "  fig = plt.figure(figsize=pltsize)\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "  attention = attention[:len(predicted_sentence), :len(sentence)]\n",
        "\n",
        "  ax.matshow(attention, cmap='viridis', vmin=0.0)\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  ax.set_xlabel('Input text')\n",
        "  ax.set_ylabel('Output text')\n",
        "  plt.suptitle('Attention weights')\n",
        "\n",
        "  \n",
        "class EncoderTranslator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, emb_dim,units, enc_embedding_layer, drop = 0.5, reg = 0.01):  \n",
        "        super(EncoderTranslator, self).__init__()\n",
        "        self.emb_dim = emb_dim\n",
        "        self.enc_vocab_size = vocab_size\n",
        "        self.enc_embedding_layer = enc_embedding_layer\n",
        "        self.enc_units = units\n",
        "\n",
        "        self.embedding = Embedding(input_dim=self.enc_vocab_size, input_length = pt_sequence_size, output_dim=self.emb_dim, weights=[self.enc_embedding_layer], trainable=False, mask_zero=True )\n",
        "        self.lstm = LSTM(self.enc_units, return_sequences=True, return_state=True, dropout=drop, recurrent_dropout=drop, recurrent_regularizer = tf.keras.regularizers.L1(l1=reg))\n",
        "        # self.spatial = SpatialDropout1D(0.5)\n",
        "\n",
        "    def call(self, inp, state=None):\n",
        "        x = self.embedding(inp)\n",
        "        enc_mask = self.embedding.compute_mask(inp)\n",
        "\n",
        "        state = self.lstm.get_initial_state(x)\n",
        "\n",
        "        output, h_state, c_state = self.lstm(x, initial_state=state)\n",
        "        #output = self.spatial(output)\n",
        "        \n",
        "        return output, (h_state, c_state), enc_mask\n",
        "\n",
        "\n",
        "class SelfMaskedDotProductAttention(tf.keras.Model): \n",
        "    def __init__(self):\n",
        "        super(SelfMaskedDotProductAttention, self).__init__()\n",
        "\n",
        "\n",
        "    def call(self, q, k, v, padding_mask= None, look_ahead_mask= None, q_mask= None):\n",
        "        \n",
        "        PRINT_SHAPE= False\n",
        "                \n",
        "        matmul_qk = tf.matmul(q, k, transpose_b=True) \n",
        "\n",
        "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "        if PRINT_SHAPE : print(\"weight shape : \", scaled_attention_logits.shape,\" - [batch_size, Tq, Tv]\")\n",
        "        \n",
        "        if padding_mask is not None:\n",
        "            padding_mask = tf.expand_dims(padding_mask, 1)\n",
        "            scaled_attention_logits = tf.where(padding_mask, scaled_attention_logits, tf.experimental.numpy.full_like(scaled_attention_logits,  -1e9))\n",
        "\n",
        "        # if q_mask is not None:\n",
        "        #     q_mask = tf.expand_dims(q_mask, 2)\n",
        "        #     scaled_attention_logits = tf.where(q_mask, scaled_attention_logits, tf.experimental.numpy.full_like(scaled_attention_logits,  -1e9))\n",
        "\n",
        "        if look_ahead_mask is not None:\n",
        "            scaled_attention_logits += (look_ahead_mask * -1e9)\n",
        "            \n",
        "\n",
        "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1) \n",
        "        if PRINT_SHAPE : print(\"attention_weights / values : \",attention_weights.shape, v.shape)\n",
        "\n",
        "        context_vector = tf.matmul(attention_weights, v)\n",
        "\n",
        "        if PRINT_SHAPE : print(\"context_vector shape : \", context_vector.shape)\n",
        "\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "\n",
        "class DecoderTranslator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, emb_dim, units,  embeddings_dec_layer, causal_mask_enabled= True, use_attention = False, drop = 0.5, reg = 0.01):\n",
        "        super(DecoderTranslator, self).__init__()\n",
        "\n",
        "        self.use_attention = use_attention\n",
        "        self.causal_mask_enabled = causal_mask_enabled\n",
        "        self.dec_vocab_size = vocab_size\n",
        "        self.emb_dim = emb_dim\n",
        "        self.embeddings_dec_layer = embeddings_dec_layer\n",
        "        self.units= units\n",
        "\n",
        "        self.embedding = Embedding(input_dim=self.dec_vocab_size, input_length = en_sequence_size, output_dim=self.emb_dim,  weights=[self.embeddings_dec_layer], trainable=False  , mask_zero=True)#embedding_dim #\n",
        "        #self.spatial = SpatialDropout1D(drop_spat)\n",
        "        self.lstm = LSTM(self.units,return_sequences=True, return_state=True, dropout=drop, recurrent_dropout=drop, recurrent_regularizer = tf.keras.regularizers.L1(l1=reg))\n",
        "        self.drop = Dropout(drop)\n",
        "        # self.batchNorm0 =BatchNormalization()\n",
        "        # self.layernorm0 = LayerNormalization(epsilon=layernorm_epsi)\n",
        "        #self.layernorm1 = LayerNormalization(epsilon=layernorm_epsi)\n",
        "        self.d0 = Dense(self.units, activation='relu')\n",
        "        # self.drop1 = Dropout(0.5)\n",
        "        # self.batchNorm1 =BatchNormalization()\n",
        "        # self.d1 = Dense(self.units, activation='relu')\n",
        "        self.d = Dense(self.dec_vocab_size)\n",
        "        self.dot_attention = SelfMaskedDotProductAttention()\n",
        "        \n",
        "\n",
        "    def create_look_ahead_mask(self, size, size_input):\n",
        "        mask = 1 - tf.linalg.band_part(tf.ones((size, size_input)), -1, 0)\n",
        "        return mask  \n",
        "\n",
        "    def call(self, decoder_input, dec_hidden,  enc_output,  inp_mask= None, target_mask= None, training= True):\n",
        "\n",
        "        #target_mask = self.embedding.compute_mask(decoder_input)\n",
        "\n",
        "        x = self.embedding(decoder_input)\n",
        "        #x = self.spatial(x)\n",
        "        x, state_h, state_c = self.lstm(x, initial_state = dec_hidden )\n",
        "\n",
        "        if self.causal_mask_enabled :\n",
        "            look_ahead_mask = self.create_look_ahead_mask(tf.shape(decoder_input)[1], tf.shape(enc_output)[1])\n",
        "        else:\n",
        "            look_ahead_mask = None\n",
        "\n",
        "        if self.use_attention:\n",
        "            #tf.expand_dims(dec_hidden[0], axis = 1)\n",
        "            context_vector, attention_weights = self.dot_attention(x, enc_output, enc_output, padding_mask = inp_mask, look_ahead_mask =look_ahead_mask, q_mask = target_mask)#dec_hidden[0]\n",
        "            x =  tf.concat([context_vector, x], axis=-1)\n",
        "            #x = tf.reduce_sum(context_vector, axis = 1)\n",
        "        else:\n",
        "            context_vector = None\n",
        "            attention_weights = None\n",
        "\n",
        "        #lstm_out, state_h, state_c = self.lstm(x, initial_state = dec_hidden )\n",
        "        #x = lstm_out\n",
        "        x = self.drop(x)#, training = training\n",
        "        # x = self.layernorm0(x)\n",
        "        x = self.d0(x)\n",
        "        #x = self.layernorm1(x)\n",
        "        # x = self.drop1(x)\n",
        "        # x = self.batchNorm1(x)\n",
        "        # x = self.d1(x)\n",
        "\n",
        "        output = self.d(x)\n",
        "\n",
        "        return output, (state_h, state_c), attention_weights\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROtVgsEeml3l"
      },
      "source": [
        "#### Check the classes structures\n",
        "\n",
        "* Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fC9i4JjrrOAD"
      },
      "outputs": [],
      "source": [
        "for i, o in dataset_fit.take(1):\n",
        "    print(i)\n",
        "\n",
        "i_batch = i\n",
        "i = i[0:1]\n",
        "o = o[0:1]\n",
        "encoder = EncoderTranslator(encoder_vocab_size, emb_dim, units, embeddings_pt_matrix)\n",
        "enc_output, enc_state, mask_enc = encoder(i)\n",
        "mask = (i != 0) \n",
        "\n",
        "print(\"mask : \",mask)\n",
        "print(\"mask_enc : \",mask_enc)\n",
        "print(f'Input batch, shape (batch): {i.shape}')\n",
        "print(f'Output batch, shape (batch): {o.shape}')\n",
        "\n",
        "print(f'Encoder output, shape (batch, s, units): {enc_output.shape}')\n",
        "print(f'Encoder hidden state, shape (batch, units): {enc_state[0].shape}')\n",
        "print(f'Encoder cell state, shape (batch, units): {enc_state[0].shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No9qot8Gmz81"
      },
      "source": [
        "* Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLFBnjql80LT"
      },
      "outputs": [],
      "source": [
        "decoder = DecoderTranslator(decoder_vocab_size, emb_dim, units, embeddings_eng_matrix, causal_mask_enabled= True)\n",
        "qmask = (o != 0) \n",
        "dec_output, dec_state, attention_weights = decoder(o, enc_state, enc_output,  inp_mask = mask,  target_mask = qmask)\n",
        "\n",
        "\n",
        "print(f'Target batch, shape (batch): {o.shape}')\n",
        "print(f'Target batch tokens, shape (batch, s): {yi.shape}')\n",
        "print(f'Decoder output, shape (batch, s, units): {dec_output.shape}')\n",
        "print(f'Decoder hidden state, shape (batch, units): {dec_state[0].shape}')\n",
        "print(f'Decoder cell state, shape (batch, units): {dec_state[1].shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiJrrgmqm1S0"
      },
      "source": [
        "* Masked Dot Product Attention - This one was chosed because there are no major layers to be learned on it, which implies simplicity, and it has been used for transformers. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "tEccNZgyrOAE"
      },
      "outputs": [],
      "source": [
        "#@title Default title text\n",
        "attention_layer = SelfMaskedDotProductAttention()\n",
        "\n",
        "print(\"i -->\",i[0])\n",
        "\n",
        "query= enc_output\n",
        "qmask = (i != 0) \n",
        "values = enc_output\n",
        "\n",
        "print(\"Query shape : \",query.shape)\n",
        "print(\"Values shape : \", values.shape)\n",
        "attention_context, attention_weights_pad_mask = attention_layer(query,  values, values, padding_mask = mask)\n",
        "attention_context, attention_weights_with_qmask = attention_layer(query,  values, values, q_mask = qmask)\n",
        "attention_context, attention_weights_nomask = attention_layer(query,  values, values)\n",
        "attention_context, attention_weights_with_causal= attention_layer(query,  values, values, look_ahead_mask= (1 - tf.linalg.band_part(tf.ones((i.shape[1], i.shape[1])), -1, 0)) )\n",
        "attention_context, attention_weights = attention_layer(query,  values, values, look_ahead_mask= (1 - tf.linalg.band_part(tf.ones((i.shape[1], i.shape[1])), -1, 0)) , padding_mask = mask, q_mask = qmask)\n",
        "attention_context, attention_weights_with_qmask_padding = attention_layer(query,  values, values, padding_mask = mask, q_mask = qmask)\n",
        "\n",
        "print(\"Attention result shape: (batch size, s, units) {}\".format(attention_context.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, sequence_length) {}\".format(attention_weights.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONemHSEZW_Am"
      },
      "outputs": [],
      "source": [
        "\n",
        "max_limit = np.count_nonzero((i!= 0), axis=1).max()\n",
        "plt.figure(figsize=(30,20))\n",
        "plt.subplot(3, 3, 1)\n",
        "plt.imshow(attention_weights_nomask[0][ :max_limit,:max_limit])#[ :max_limit,:max_limit]\n",
        "plt.title('Attention weights - No mask')\n",
        "\n",
        "plt.subplot(3, 3, 2)\n",
        "plt.imshow(attention_weights_pad_mask[0][ :max_limit,:max_limit])#[ :max_limit,:max_limit]\n",
        "plt.title('Attention weights - Padding mask')\n",
        "\n",
        "plt.subplot(3, 3, 3)\n",
        "plt.imshow(attention_weights_with_qmask[0][ :max_limit,:max_limit])#[ :max_limit,:max_limit]\n",
        "plt.title('Attention weights with query mask')\n",
        "\n",
        "plt.subplot(3, 3, 4)\n",
        "plt.imshow(attention_weights_with_qmask_padding[0][ :max_limit,:max_limit])#[ :max_limit,:max_limit]\n",
        "plt.title('Attention weights with query and padding mask')\n",
        "\n",
        "plt.subplot(3, 3, 5)\n",
        "plt.imshow(attention_weights_with_causal[0][ :max_limit,:max_limit])#[ :max_limit,:max_limit]\n",
        "plt.title('Attention weights with causal mask')\n",
        "\n",
        "plt.subplot(3, 3, 6)\n",
        "plt.imshow(attention_weights[0][ :max_limit,:max_limit])#[ :max_limit,:max_limit]\n",
        "plt.title('Attention weights with all masks')\n",
        "\n",
        "\n",
        "plt.subplot(3, 3, 7)\n",
        "plt.pcolormesh(i_batch != 0)\n",
        "plt.title('Mask')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvKNvFh4nI5u"
      },
      "source": [
        "* Translator model class creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DijVP6KrOAE"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MaskedLossCustom(tf.keras.losses.Loss):\n",
        "  def __init__(self):\n",
        "    self.name = 'masked_loss_transformer_custom'\n",
        "    self.loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "  def __call__(self, real, pred):\n",
        "      \n",
        "    loss = self.loss_object(real, pred)\n",
        "\n",
        "    # Mask off the losses on padding.\n",
        "    mask = tf.cast(real != 0, tf.float32)\n",
        "    loss *= mask\n",
        "\n",
        "    # Return the total.\n",
        "    return tf.reduce_sum(loss)/tf.reduce_sum(mask) \n",
        "    \n",
        "# class MaskedLossCustom(tf.keras.losses.Loss):\n",
        "#   def __init__(self):\n",
        "#     self.name = 'masked_loss_transformer_custom'\n",
        "#     self.loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "#   def __call__(self, real, pred):\n",
        "      \n",
        "#     mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "#     loss_ = self.loss_object(real, pred)\n",
        "\n",
        "#     mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "#     loss_ *= mask\n",
        "\n",
        "#     return tf.reduce_sum(loss_)/tf.reduce_sum(mask) \n",
        "\n",
        "  \n",
        "class TrainCustomTranslator(tf.keras.Model):\n",
        "    def __init__(self, emb_dim, units,encoder_vocab_size,decoder_vocab_size,embeddings_dec_matrix,embeddings_enc_matrix,dec_tokenizer, dec_sequence_size, enc_sequence_size, use_tf_function=True,\n",
        "                 enc_drop = 0.5, enc_reg = 1e-5,  dec_drop = 0.5, dec_reg = 0.01, causal_mask_enabled= False, use_attention = False):\n",
        "        super().__init__()\n",
        "\n",
        "         \n",
        "        self.encoder = EncoderTranslator(encoder_vocab_size, emb_dim, units, embeddings_enc_matrix, drop = enc_drop, reg = enc_reg)\n",
        "        self.decoder = DecoderTranslator(decoder_vocab_size, emb_dim, units, embeddings_dec_matrix, causal_mask_enabled= causal_mask_enabled, use_attention = use_attention, drop = dec_drop, reg = dec_reg)\n",
        "        self.decoder_vocab_size = decoder_vocab_size\n",
        "        self.dec_tokenizer = dec_tokenizer\n",
        "        self.use_tf_function = use_tf_function\n",
        "        self.enc_sequence_size = enc_sequence_size\n",
        "        self.dec_sequence_size = dec_sequence_size\n",
        "\n",
        "  \n",
        "    def accuracy_function(self, real, pred):\n",
        "        real = tf.cast(real, dtype=tf.int32)\n",
        "        accuracies = tf.equal(real, tf.cast(tf.argmax(pred, axis=2), dtype=tf.int32))\n",
        "\n",
        "        mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "        accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "        accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "        mask = tf.cast(mask, dtype=tf.float32)\n",
        "        return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)\n",
        "\n",
        "    def call(self, inputs):\n",
        "      inp, targ = inputs  \n",
        "      input_mask = inp != 0\n",
        "\n",
        "      enc_output, dec_hidden  = self.encoder(inp, None)\n",
        "      last_dec= None\n",
        "      dec_input =targ[:, :-1] ## Teacher forcing - feeding the target as the next input\n",
        "      target_mask = None#dec_input != 0\n",
        "\n",
        "      predictions, dec_hidden, context_vector, last_dec = self.decoder(dec_input,  dec_hidden, enc_output, last_dec, input_mask, target_mask)\n",
        "      \n",
        "      return predictions\n",
        "\n",
        "    def train_step(self, inputs):\n",
        "        if self.use_tf_function:\n",
        "            return self._tf_train_step(inputs)\n",
        "        else:\n",
        "            return self._train_step(inputs)\n",
        "\n",
        "    def _train_step(self, inputs):\n",
        "        inp, targ = inputs  \n",
        "        \n",
        "        # input_mask = inp != 0\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            \n",
        "            enc_output, dec_hidden, input_mask  = self.encoder(inp, None)\n",
        "\n",
        "            acc = []\n",
        "            last_dec= None\n",
        "            loss = tf.constant(0.0)\n",
        "\n",
        "            # Pass enc_output to the decoder\n",
        "            dec_input =targ[:, :-1] ## Teacher forcing - feeding the target as the next input\n",
        "            # target_mask = dec_input != 0\n",
        "\n",
        "            predictions, dec_hidden, context_vector = self.decoder(dec_input,  dec_hidden, enc_output,  input_mask)\n",
        "            tar = targ[:, 1:]\n",
        "            loss = self.loss(tar, predictions)\n",
        "\n",
        "            self.compiled_metrics.update_state(tar, predictions)\n",
        "            acc.append(self.metrics[0].result())\n",
        "            average_acc  = tf.reduce_sum(tf.convert_to_tensor(acc) )\n",
        "            acc_m = self.accuracy_function(tar, predictions)\n",
        "\n",
        "        variables = self.trainable_variables \n",
        "        gradients = tape.gradient(loss, variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "        m = {m.name: average_acc for m in self.metrics}\n",
        "        m[\"loss\"] = loss\n",
        "        m[\"masked_acc\"] = acc_m\n",
        "\n",
        "        return m\n",
        "\n",
        "\n",
        "    # def _train_step(self, inputs):\n",
        "    #     inp, targ = inputs  \n",
        "        \n",
        "    #     input_mask = inp != 0\n",
        "    #     target_mask = targ != 0\n",
        "    #     max_target_length = tf.shape(targ)[1]\n",
        "\n",
        "    #     with tf.GradientTape() as tape:\n",
        "            \n",
        "    #         enc_output, dec_hidden  = self.encoder(inp, None)\n",
        "    #         dec_state = dec_hidden\n",
        "            \n",
        "    #         loss = tf.constant(0.0)\n",
        "    #         acc = tf.constant(0.0)\n",
        "    #         acc_m = tf.constant(0.0)\n",
        "    #         predictions_list = []\n",
        "    #         res = []\n",
        "\n",
        "    #         for t in tf.range(max_target_length-1):\n",
        "\n",
        "    #           new_tokens = targ[:, t:t+2]\n",
        "    #           input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\n",
        "\n",
        "    #           predictions, dec_state, context_vector = self.decoder(input_token,  dec_state, enc_output, input_mask, target_mask)\n",
        "\n",
        "    #           y = target_token\n",
        "    #           y_pred = predictions\n",
        "\n",
        "    #           loss += self.loss(y, y_pred)\n",
        "          \n",
        "    #           self.compiled_metrics.update_state(target_token, predictions)\n",
        "    #           acc += self.metrics[0].result()\n",
        "\n",
        "    #           predictions_list.append(y_pred)\n",
        "    #           res.append(y)\n",
        "            \n",
        "    #         acc_m = self.accuracy_function(tf.convert_to_tensor(res), tf.convert_to_tensor(predictions_list))\n",
        "    #         average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
        "    #         average_acc = acc / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
        "\n",
        "\n",
        "    #     variables = self.trainable_variables \n",
        "    #     gradients = tape.gradient(loss, variables)\n",
        "    #     self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    #     m = {m.name: average_acc for m in self.metrics}\n",
        "    #     m[\"loss\"] = average_loss\n",
        "    #     m[\"masked_acc\"] = acc_m\n",
        "\n",
        "    #     return m\n",
        "\n",
        "    @tf.function(input_signature=[[tf.TensorSpec(dtype=tf.int32, shape=[None, pt_sequence_size]),tf.TensorSpec(dtype=tf.int32, shape=[None, en_sequence_size])]])\n",
        "    def _tf_train_step(self, inputs):\n",
        "        return self._train_step(inputs)\n",
        "\n",
        "\n",
        "    def test_step(self, inputs):\n",
        "        return self._test_step(inputs)\n",
        "\n",
        "    def _test_step(self, inputs):\n",
        "        inp, targ = inputs   \n",
        "        input_mask = inp != 0\n",
        "\n",
        "        enc_output, dec_hidden, input_mask  = self.encoder(inp, None)\n",
        "\n",
        "        acc = []\n",
        "        last_dec= None\n",
        "        loss = tf.constant(0.0)\n",
        "\n",
        "        # Pass enc_output to the decoder\n",
        "        dec_input =targ[:, :-1] ## Teacher forcing - feeding the target as the next input\n",
        "        # target_mask = dec_input != 0\n",
        "        \n",
        "        predictions, dec_hidden, context_vector = self.decoder(dec_input,  dec_hidden, enc_output, input_mask, training= False)\n",
        "\n",
        "        tar = targ[:, 1:]\n",
        "        loss += self.loss(tar, predictions)\n",
        "\n",
        "        self.compiled_metrics.update_state(tar, predictions)\n",
        "        acc.append(self.metrics[0].result())\n",
        "        average_acc  = tf.reduce_mean(tf.convert_to_tensor(acc) )\n",
        "\n",
        "        m = {m.name: average_acc for m in self.metrics}#.update({'loss': batch_loss})\n",
        "        m[\"loss\"] = loss\n",
        "        m[\"masked_acc\"] = self.accuracy_function(tar, predictions)\n",
        "\n",
        "        return m\n",
        "\n",
        "    # def _test_step(self, inputs):\n",
        "    #     inp, targ = inputs  \n",
        "        \n",
        "    #     input_mask = inp != 0\n",
        "    #     target_mask = targ != 0\n",
        "    #     max_target_length = tf.shape(targ)[1]\n",
        "    \n",
        "    #     enc_output, dec_hidden  = self.encoder(inp, None)\n",
        "    #     dec_state = dec_hidden\n",
        "\n",
        "    #     loss = tf.constant(0.0)\n",
        "    #     acc = tf.constant(0.0)\n",
        "    #     acc_m = tf.constant(0.0)\n",
        "\n",
        "    #     predictions_list = []\n",
        "    #     res = []\n",
        "    #     for t in tf.range(max_target_length-1):\n",
        "\n",
        "    #       new_tokens = targ[:, t:t+2]\n",
        "    #       input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\n",
        "          \n",
        "    #       predictions, dec_state, context_vector = self.decoder(input_token,  dec_state, enc_output, input_mask, target_mask, training= False)\n",
        "\n",
        "    #       y = target_token\n",
        "    #       y_pred = predictions\n",
        "\n",
        "    #       loss += self.loss(y, y_pred)\n",
        "      \n",
        "    #       self.compiled_metrics.update_state(target_token, predictions)\n",
        "    #       acc += self.metrics[0].result()\n",
        "\n",
        "    #       predictions_list.append(y_pred)\n",
        "    #       res.append(y)\n",
        "            \n",
        "    #     acc_m = self.accuracy_function(tf.convert_to_tensor(res), tf.convert_to_tensor(predictions_list))\n",
        "    #     average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
        "    #     average_acc = acc / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
        "\n",
        "\n",
        "    #     m = {m.name: average_acc for m in self.metrics}\n",
        "    #     m[\"loss\"] = average_loss\n",
        "    #     m[\"masked_acc\"] = acc_m\n",
        "\n",
        "    #     return m\n",
        "\n",
        "    # @tf.function(input_signature=[[tf.TensorSpec(dtype=tf.int32, shape=[None, pt_sequence_size]),\n",
        "    #                             tf.TensorSpec(dtype=tf.int32, shape=[None, en_sequence_size])\n",
        "    #                             ]])\n",
        "    # def _tf_test_step(self, inputs):\n",
        "    #     return self._test_step(inputs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UqbvpWLk82n"
      },
      "outputs": [],
      "source": [
        "# with tpu_strategy.scope():\n",
        "\n",
        "#   custom_translator = TrainCustomTranslator(\n",
        "#         emb_dim= emb_dim,\n",
        "#         units = units,\n",
        "#         encoder_vocab_size= encoder_vocab_size,\n",
        "#         decoder_vocab_size= decoder_vocab_size,\n",
        "#         embeddings_dec_matrix = embeddings_eng_matrix,\n",
        "#         embeddings_enc_matrix = embeddings_pt_matrix,\n",
        "#         dec_tokenizer = eng_tokenizer,\n",
        "#         dec_sequence_size = en_sequence_size, \n",
        "#         enc_sequence_size = pt_sequence_size,\n",
        "#         use_tf_function=True,\n",
        "#         causal_mask_enabled= True, use_attention = True)\n",
        "\n",
        "\n",
        "#   # Configure the loss and optimizer\n",
        "#   custom_translator.compile(\n",
        "#       optimizer=tf.optimizers.Adam(0.001),\n",
        "#       loss= MaskedLossCustom(),\n",
        "#       metrics = [tf.metrics.SparseCategoricalAccuracy()] #[\"tf.metrics.SparseCategoricalAccuracy()\"] AccuracyCustom()\n",
        "#   )\n",
        "\n",
        "#   history = custom_translator.fit(dataset_fit,  epochs=epochs, callbacks=[PlotLossesCallback()], steps_per_epoch = train_size,  shuffle= True,  validation_data = dataset_fit_dev, validation_steps=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwsorYLVly1A"
      },
      "source": [
        "# Performing Hyperparam tunning manual  + TPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9fD0OP3mILA"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pprint\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "MANUAL_TUNING = False\n",
        "from datetime import datetime\n",
        "\n",
        "def get_manual_tunning(train_ds, val_ds, hp_units, hp_enc_drop, hp_enc_reg, hp_dec_drop, hp_dec_reg,lr):\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    with tpu_strategy.scope():\n",
        "        custom_translator = TrainCustomTranslator(\n",
        "          emb_dim= emb_dim,\n",
        "          units = hp_units,\n",
        "          encoder_vocab_size= encoder_vocab_size,\n",
        "          decoder_vocab_size= decoder_vocab_size,\n",
        "          embeddings_dec_matrix = embeddings_eng_matrix,\n",
        "          embeddings_enc_matrix = embeddings_pt_matrix,\n",
        "          dec_tokenizer = eng_tokenizer,\n",
        "          dec_sequence_size = en_sequence_size, \n",
        "          enc_sequence_size = pt_sequence_size,\n",
        "          use_tf_function=True,\n",
        "\n",
        "          enc_drop =hp_enc_drop,\n",
        "          enc_reg = hp_enc_reg, \n",
        "          dec_drop = hp_dec_drop, \n",
        "          dec_reg = hp_dec_reg,\n",
        "          \n",
        "          causal_mask_enabled= True, use_attention = True)\n",
        "\n",
        "      \n",
        "        custom_translator.compile(\n",
        "          optimizer=tf.optimizers.Adam(lr),\n",
        "          loss=MaskedLossCustom(),\n",
        "          metrics = [tf.metrics.SparseCategoricalAccuracy()] )\n",
        "\n",
        "        # Step 4: Train the model on TPU with fixed batch size.\n",
        "        val_metric = 'val_masked_acc'\n",
        "        stop_early = tf.keras.callbacks.EarlyStopping(monitor=val_metric, patience=100)\n",
        "        out = custom_translator.fit(train_ds,  epochs=300,   shuffle= True, validation_data=val_ds, verbose =0, callbacks=[stop_early]) #callbacks=[PlotLossesCallback()],\n",
        "\n",
        "    \n",
        "    return out\n",
        "\n",
        "if MANUAL_TUNING:\n",
        "\n",
        "  L1L2_reg = [ 0.1, 0.01, 0.001, 0.0001]\n",
        "  drop_reg = [ 0.1, 0.3, 0.5]\n",
        "\n",
        "  para = {\n",
        "      \"hp_units\": list(range(32,512,32)),\n",
        "      \"hp_enc_drop\": drop_reg,\n",
        "      \"hp_enc_reg\": L1L2_reg,\n",
        "      \"hp_dec_drop\": drop_reg,\n",
        "      \"hp_dec_reg\": L1L2_reg,\n",
        "      \"learning_rate\": [  0.001, 0.0001]\n",
        "      }\n",
        "\n",
        "  Trial_Dict = {}\n",
        "  trials = 0\n",
        "  best_trial = -1\n",
        "  best_trial_value = 0\n",
        "  HP_tunning_summary = {}\n",
        "  last_trial = -1\n",
        "  LOAD_MANUAL_TUNNING = True\n",
        "\n",
        "  if LOAD_MANUAL_TUNNING:\n",
        "    try:\n",
        "      with open(\"/content/drive/My Drive/Colab Notebooks/Translator/Trial_Dict.json\") as fp:\n",
        "        Trial_Dict= json.load(fp)\n",
        "        print(\"Trial_Dict->\",Trial_Dict)\n",
        "\n",
        "      with open(\"/content/drive/My Drive/Colab Notebooks/Translator/HP_tunning_summary.json\") as fp:\n",
        "        HP_tunning_summary= json.load(fp)\n",
        "      #print(\"HP_tunning_summary->\",Trial_Dict)\n",
        "      best_trial = HP_tunning_summary[\"Best_Trial\"]\n",
        "      best_trial_value = HP_tunning_summary[\"Best_Trial_Value\"]\n",
        "      last_trial = HP_tunning_summary[\"Trial_counter\"]\n",
        "\n",
        "      print(f\"{best_trial} is the best trial with val_masked_acc = {best_trial_value} \\n -----------------------------------\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"Failed do read the json files . {e}\")\n",
        "\n",
        "  for hp_units in para[\"hp_units\"]:\n",
        "    for hp_enc_drop in para[\"hp_enc_drop\"]:\n",
        "        for hp_enc_reg in para[\"hp_enc_reg\"]:\n",
        "            for hp_dec_drop in para[\"hp_dec_drop\"]:\n",
        "                for hp_dec_reg in para[\"hp_dec_reg\"]:\n",
        "                  for learning_rate in para[\"learning_rate\"]:\n",
        "\n",
        "                    starting_point = datetime.now()\n",
        "\n",
        "                    data = {\n",
        "                      \"hp_units\": hp_units,\n",
        "                      \"hp_enc_drop\": hp_enc_drop,\n",
        "                      \"hp_enc_reg\": hp_enc_reg,\n",
        "                      \"hp_dec_drop\": hp_dec_drop,\n",
        "                      \"hp_dec_reg\": hp_dec_reg,\n",
        "                      \"learning_rate\": learning_rate\n",
        "                      }\n",
        "                      \n",
        "                    print(f\"Trial {trials}\\n Current architecture : \")\n",
        "                    pp.pprint(data)\n",
        "                    \n",
        "                    if trials <= last_trial:\n",
        "                      print(f\"Current val_masked_acc : {Trial_Dict[str(trials)]['validation_score']} \\n {best_trial} is the best trial with val_masked_acc = {best_trial_value} \\n -----------------------------------\\n\")\n",
        "                      trials = trials+1\n",
        "\n",
        "                    else:\n",
        "\n",
        "                      hist = get_manual_tunning(dataset_fit_hp ,dataset_fit_dev_hp,hp_units, hp_enc_drop,hp_enc_reg,hp_dec_drop,hp_dec_reg,learning_rate)\n",
        "\n",
        "                      Trial_Dict[trials] = {\n",
        "                          \"data\" : data,\n",
        "                          \"history\" : hist.history,\n",
        "                          \"validation_score\" : hist.history[\"val_masked_acc\"][-1],\n",
        "                      }\n",
        "                      \n",
        "                      if hist.history[\"val_masked_acc\"][-1] > best_trial_value : \n",
        "                        best_trial_value = hist.history[\"val_masked_acc\"][-1]\n",
        "                        best_trial = trials\n",
        "\n",
        "                      print(f\"Current val_masked_acc : {hist.history['val_masked_acc'][-1]} \\n {best_trial} is the best trial with val_masked_acc = {best_trial_value}\")\n",
        "                      \n",
        "                      \n",
        "                      duration = (datetime.now() - starting_point).total_seconds()\n",
        "\n",
        "                      HP_tunning_summary = {\n",
        "                          \"Trials\" : Trial_Dict,\n",
        "                          \"Best_Trial\": best_trial,\n",
        "                          \"Best_Trial_Value\" : best_trial_value,\n",
        "                          \"Trial_counter\" : trials,\n",
        "                          \"Timer\" : str(duration) +\" seconds\"\n",
        "\n",
        "                      }\n",
        "                      print(f\"It took {str(duration)} seconds  \\n -----------------------------------\\n\")\n",
        "\n",
        "                      with open(\"/content/drive/My Drive/Colab Notebooks/Translator/Trial_Dict.json\", \"w\") as fp:\n",
        "                        json.dump(Trial_Dict,fp) \n",
        "\n",
        "                      with open(\"/content/drive/My Drive/Colab Notebooks/Translator/HP_tunning_summary.json\", \"w\") as fp:\n",
        "                        json.dump(HP_tunning_summary,fp) \n",
        "\n",
        "                      trials = trials+1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMQbZnLHv1_q"
      },
      "source": [
        "# Performing Hyperparam tunning with Bayesian Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEP0RZ14v01j"
      },
      "outputs": [],
      "source": [
        "#https://medium.com/analytics-vidhya/how-to-do-hyper-parameters-search-with-bayesian-optimization-for-keras-model-6b9918caa0b1\n",
        "from bayes_opt.logger import JSONLogger\n",
        "from bayes_opt.event import Events\n",
        "from bayes_opt.util import load_logs\n",
        "\n",
        "def get_bayesian_tunning(hp_units, hp_enc_drop, hp_dec_drop, hp_enc_reg, hp_dec_reg,lr):\n",
        "\n",
        "    # tf.keras.backend.clear_session()\n",
        "    with tpu_strategy.scope():\n",
        "        hp_units = int(hp_units)\n",
        "        custom_translator = TrainCustomTranslator(\n",
        "          emb_dim= emb_dim,\n",
        "          units = hp_units,\n",
        "          encoder_vocab_size= encoder_vocab_size,\n",
        "          decoder_vocab_size= decoder_vocab_size,\n",
        "          embeddings_dec_matrix = embeddings_eng_matrix,\n",
        "          embeddings_enc_matrix = embeddings_pt_matrix,\n",
        "          dec_tokenizer = eng_tokenizer,\n",
        "          dec_sequence_size = en_sequence_size, \n",
        "          enc_sequence_size = pt_sequence_size,\n",
        "          use_tf_function=True,\n",
        "\n",
        "          enc_drop = hp_enc_drop, enc_reg = hp_enc_reg,  dec_drop = hp_dec_drop, dec_reg = hp_dec_reg,\n",
        "          causal_mask_enabled= True, use_attention = True)\n",
        "\n",
        "        \n",
        "        custom_translator.compile(\n",
        "          optimizer=tf.optimizers.Adam(lr),\n",
        "          loss=MaskedLossCustom(),\n",
        "          metrics = [tf.metrics.SparseCategoricalAccuracy()] )\n",
        "\n",
        "        # Step 4: Train the model on TPU with fixed batch size.\n",
        "        val_metric = 'val_masked_acc'#val_sparse_categorical_accuracy\n",
        "        stop_early = tf.keras.callbacks.EarlyStopping(monitor=val_metric, patience=100)\n",
        "        out = custom_translator.fit(dataset_fit_hp ,  epochs=500,   shuffle= True, validation_data=dataset_fit_dev_hp, verbose =0, callbacks=[stop_early]) #callbacks=[PlotLossesCallback()],\n",
        "\n",
        " \n",
        "\n",
        "    return out.history[val_metric][-1]#val_sparse_categorical_accuracy\n",
        "\n",
        "\n",
        "L1L2_reg = ( 0.0001, 0.1)\n",
        "drop_reg = ( 0.1, 0.5)\n",
        "\n",
        "para = {\n",
        "    \"hp_units\": (32,1024),\n",
        "     \"hp_enc_drop\": drop_reg,\n",
        "     \"hp_dec_drop\": drop_reg,\n",
        "     \"hp_enc_reg\": L1L2_reg,\n",
        "     \"hp_dec_reg\": L1L2_reg,\n",
        "     \"lr\": (0.00001, 0.001)\n",
        "     }\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "fit_with_partial = partial(get_bayesian_tunning)\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "    f = fit_with_partial,\n",
        "    pbounds =para,\n",
        "    verbose=2,  # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
        "    random_state=1\n",
        ")\n",
        "para\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEyxJebdlqNR"
      },
      "outputs": [],
      "source": [
        "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UC4Sko66DosI"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  load_logs(optimizer, logs=[\"/content/drive/My Drive/Colab Notebooks/Translator/bayesian_logs.json\"]);\n",
        "except :\n",
        "  print(\"File bayesian_logs.json does not exists\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAG9vRbP2eOK"
      },
      "outputs": [],
      "source": [
        "logger = JSONLogger(path=\"/content/drive/My Drive/Colab Notebooks/Translator/bayesian_logs.json\")\n",
        "optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "El0E2UhRXcZ3"
      },
      "outputs": [],
      "source": [
        "optimizer.maximize(init_points=20, n_iter=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Koz0cFpQv1Su"
      },
      "outputs": [],
      "source": [
        "for i, res in enumerate(optimizer.res):\n",
        "    print(\"Iteration {}: \\n\\t{}\".format(i, res))\n",
        "\n",
        "print(optimizer.max)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMMF2l9EztOc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHYwMBu_bayi"
      },
      "source": [
        "# Performing Hyperparam tunning Using Talo  + TPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0kTLsFibZlK"
      },
      "outputs": [],
      "source": [
        "\n",
        "with tpu_strategy.scope():\n",
        "\n",
        "  custom_translator = TrainCustomTranslator(\n",
        "      emb_dim= emb_dim,\n",
        "      units = 900,\n",
        "      encoder_vocab_size= encoder_vocab_size,\n",
        "      decoder_vocab_size= decoder_vocab_size,\n",
        "      embeddings_dec_matrix = embeddings_eng_matrix,\n",
        "      embeddings_enc_matrix = embeddings_pt_matrix,\n",
        "      dec_tokenizer = eng_tokenizer,\n",
        "      dec_sequence_size = en_sequence_size, \n",
        "      enc_sequence_size = pt_sequence_size,\n",
        "      use_tf_function=True,\n",
        "      enc_drop =0.5,\n",
        "      enc_rec_drop = 0.5, \n",
        "      enc_reg = 0.001,\n",
        "      dec_drop = 0.5, \n",
        "      dec_rec_drop = 0.5, \n",
        "      dec_reg = 0.001\n",
        "      )\n",
        "\n",
        "  lr = 0.0005\n",
        "  # Configure the loss and optimizer\n",
        "  custom_translator.compile(\n",
        "      optimizer=tf.optimizers.Adam(lr),\n",
        "      loss=MaskedLossCustom(),\n",
        "      metrics = [tf.metrics.SparseCategoricalAccuracy()] #[\"tf.metrics.SparseCategoricalAccuracy()\"] AccuracyCustom()\n",
        "  )\n",
        "\n",
        "  if TRAIN_MODEL:    \n",
        "\n",
        "    history = custom_translator.fit(dataset_fit,  epochs=training_epochs, callbacks=[PlotLossesCallback()], steps_per_epoch = train_size,  shuffle= True,  validation_data = dataset_fit_dev, validation_steps=10)\n",
        "    custom_translator.save_weights(\"custom_translator\")\n",
        "  else : \n",
        "      custom_translator.load_weights(\"custom_translator\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdMlevkMrOAF"
      },
      "outputs": [],
      "source": [
        "from rouge import Rouge #https://towardsdatascience.com/introduction-to-text-summarization-with-rouge-scores-84140c64b471\n",
        "ROUGE = Rouge()\n",
        "\n",
        "def Minimum_Bayes_Risk(list_of_candidates, field = \"rouge-l\", candidate_reference = None):\n",
        "    candidate_score = {}\n",
        "    max_score = 0\n",
        "    best_score_candidate= ''\n",
        "    best_candidate_index = -1\n",
        "    if candidate_reference == None :\n",
        "        for candidate_ref in list_of_candidates:\n",
        "            \n",
        "            rest_of_candidates = copy.deepcopy(list_of_candidates)\n",
        "            if candidate_ref in rest_of_candidates : rest_of_candidates.remove(candidate_ref)\n",
        "            score = 0\n",
        "\n",
        "            for candidate in rest_of_candidates:\n",
        "                score += ROUGE.get_scores(candidate, candidate_ref)[0][field][\"f\"]\n",
        "            \n",
        "            score = score/len(rest_of_candidates)\n",
        "\n",
        "            candidate_score[candidate_ref] = score\n",
        "\n",
        "            if score >= max_score:\n",
        "                best_score_candidate =candidate_ref\n",
        "                best_candidate_index = list_of_candidates.index(candidate_ref)\n",
        "                max_score = score\n",
        "\n",
        "    else:\n",
        "            \n",
        "            for candidate in list_of_candidates:\n",
        "                score = 0\n",
        "                score = ROUGE.get_scores(candidate, candidate_reference)[0][field][\"f\"]\n",
        "                if score >= max_score:\n",
        "                    best_score_candidate =candidate\n",
        "\n",
        "\n",
        "    return candidate_score, best_score_candidate, best_candidate_index\n",
        "\n",
        "\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(30,30))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeSeb5ZvrOAF"
      },
      "outputs": [],
      "source": [
        "vocab_size = decoder_vocab_size\n",
        "token_mask_ids = np.array([eng_tokenizer.word_index[x] for x in  [ '<oov>' ]])[:, None]#\n",
        "token_mask = np.zeros([vocab_size], dtype=np.bool)\n",
        "token_mask[np.array(token_mask_ids)] = True\n",
        "\n",
        "print(\"token_mask\", token_mask)\n",
        "\n",
        "def translate_sequence(input_tokens, temperature=0.5, number_of_candidates = 10, loop= False):\n",
        "\n",
        "  attention_per_candidate = []\n",
        "  result_text_per_candidate = []\n",
        "  input_text_per_candidate = []\n",
        "  output_text_per_candidate = []\n",
        "  \n",
        "\n",
        "  for c in range(number_of_candidates):\n",
        "    \n",
        "    batch_size_tmp = len(input_tokens)\n",
        "\n",
        "    last_dec = None\n",
        "    \n",
        "    max_length = input_tokens.shape[1] #np.count_nonzero(input_mask)\n",
        "    \n",
        "    enc_output,  enc_hidden, enc_cell = custom_translator.encoder(input_tokens)\n",
        "    dec_state = ( enc_hidden, enc_cell)\n",
        "\n",
        "    x = tf.expand_dims([eng_tokenizer.word_index['<sos>']]*batch_size_tmp , 1)\n",
        "    result_tokens = []\n",
        "    input_text=[]\n",
        "    output_text=[]\n",
        "    attention = np.zeros((pt_sequence_size, en_sequence_size))\n",
        "\n",
        "    for t in range(pt_sequence_size):\n",
        "\n",
        "      input_text.append(' '.join([pt_tokenizer.index_word[w.numpy()] for w in  input_tokens[0] if w != 0 ]))\n",
        "\n",
        "      input_mask = x!= 0\n",
        "\n",
        "      predictions, dec_state, attention_weights, last_dec = custom_translator.decoder(x, dec_state, enc_output, last_dec, input_mask)\n",
        "\n",
        "      attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "      attention[t] = attention_weights.numpy()\n",
        "\n",
        "      predicted_logits  = predictions[:,-1,:]/temperature\n",
        "      predicted_logits = tf.where(token_mask, -np.inf, predicted_logits)\n",
        "      predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "      new_tokens = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "      if loop:\n",
        "          x = new_tokens.numpy()\n",
        "      else : \n",
        "          x = list(x[0])\n",
        "          x = [i for i in x if i != 0]\n",
        "          x.append(new_tokens.numpy()[0])\n",
        "\n",
        "      output_text.append(' '.join([eng_tokenizer.index_word[w] for w in  x if w != 0 ]))\n",
        "      x = tf.reshape(tf.convert_to_tensor(x), (len(input_tokens),len(x)))\n",
        "\n",
        "      result_tokens.append(new_tokens.numpy()[0])\n",
        "\n",
        "      if eng_tokenizer.word_index['<eos>'] == new_tokens.numpy()[0]:\n",
        "          break\n",
        "      \n",
        "          \n",
        "    result_text = ' '.join([eng_tokenizer.index_word[w] for w in  result_tokens if w != 0])\n",
        "\n",
        "    \n",
        "    attention_per_candidate.append(attention)\n",
        "    result_text_per_candidate.append(result_text)\n",
        "    input_text_per_candidate.append(input_text)\n",
        "    output_text_per_candidate.append(output_text)\n",
        "\n",
        "  candidate_score, best_score_candidate, best_candidate_index = Minimum_Bayes_Risk(result_text_per_candidate)\n",
        "\n",
        "  attention_stack = attention_per_candidate[best_candidate_index]\n",
        "  result_text = result_text_per_candidate[best_candidate_index]\n",
        "  input_text = input_text_per_candidate[best_candidate_index]\n",
        "  output_text = output_text_per_candidate[best_candidate_index]\n",
        "\n",
        "\n",
        "  return {'pt_sentence': result_text, 'attention': attention_stack, \"input_text\" : input_text, \"output_text\" : output_text, \"candidates\": result_text_per_candidate}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RyltbIMUIcp"
      },
      "outputs": [],
      "source": [
        "decoder_vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2S_sAO2DrOAF"
      },
      "outputs": [],
      "source": [
        "print(' '.join([pt_tokenizer.index_word[i] for i in x[0] if i != 0]),\" ---> \",  ' '.join([eng_tokenizer.index_word[i] for i in y[0] if i != 0]))\n",
        "\n",
        "sentences = ' '.join([pt_tokenizer.index_word[i] for i in x[0] if i != 0])\n",
        "result = translate_sequence(input_tokens = tf.expand_dims(x[0],0), temperature=0.5, number_of_candidates = 2, loop= True)\n",
        "\n",
        "attention_plot, res= result[\"attention\"], result[\"pt_sentence\"]\n",
        "print(sentences,\" ---> \",res)\n",
        "\n",
        "attention_plot = attention_plot[:len(res.split(' ')), :len(sentences.split(' '))]\n",
        "plot_attention(attention_plot, sentences.split(' '), res.split(' '))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiQ1sSD2rOAG"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "\n",
        "def word_for_id(integer, tokenizer):\n",
        "    # map an integer to a word\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None\n",
        "\n",
        "def predict_seq(model, tokenizer, source):\n",
        "    # generate target from a source sequence\n",
        "    prediction = model.predict(source, verbose=0)[0]\n",
        "    integers = [np.argmax(vector) for vector in prediction]\n",
        "    target = list()\n",
        "    for i in integers:\n",
        "        word = word_for_id(i, tokenizer)\n",
        "        if word is None:\n",
        "            break\n",
        "        target.append(word)\n",
        "    return ' '.join(target)\n",
        "\n",
        "def bleu_score(model, tokenizer, sources, raw_dataset):\n",
        "    # Get the bleu score of a model\n",
        "    actual, predicted = [], []\n",
        "    for i, source in enumerate(sources):\n",
        "        # translate encoded source text\n",
        "        source = source.reshape((1, source.shape[0]))\n",
        "        translation = predict_seq(model, tokenizer, source)\n",
        "        raw_target, raw_src = raw_dataset[i]\n",
        "        actual.append([raw_target.split()])\n",
        "        predicted.append(translation.split())\n",
        "        \n",
        "    bleu_dic = {}\n",
        "    bleu_dic['1-grams'] = corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0))\n",
        "    bleu_dic['1-2-grams'] = corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0))\n",
        "    bleu_dic['1-3-grams'] = corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0))\n",
        "    bleu_dic['1-4-grams'] = corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "    \n",
        "    return bleu_dic\n",
        "\n",
        "bleu_train = bleu_score(model, pt_tokenizer, x_dev, train)\n",
        "bleu_test = bleu_score(model, pt_tokenizer, x_dev, test)\n",
        "\n",
        "plt.figure()\n",
        "plt.bar(x = bleu_train.keys(), height = bleu_train.values())\n",
        "plt.title(\"BLEU Score with the training set\")\n",
        "plt.ylim((0,1))\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.bar(x = bleu_test.keys(), height = bleu_test.values())\n",
        "plt.title(\"BLEU Score with the test set\")\n",
        "plt.ylim((0,1))\n",
        "plt.show()\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwglrK-krOAG"
      },
      "outputs": [],
      "source": [
        "for i, o in dataset_fit_dev.take(1):\n",
        "    print()\n",
        "k = 20 \n",
        "\n",
        "print(' '.join([eng_tokenizer.index_word[i] for i in x[k] if i != 0]),\" ---> \",  ' '.join([pt_tokenizer.index_word[i] for i in y[k] if i != 0]))\n",
        "\n",
        "sentences = ' '.join([eng_tokenizer.index_word[i] for i in x[k] if i != 0])\n",
        "result = translate_sequence(input_tokens = tf.expand_dims(x[k],0), temperature=0.5, number_of_candidates = 2, loop= True)\n",
        "\n",
        "attention_plot, res= result[\"attention\"], result[\"pt_sentence\"]\n",
        "print(sentences,\" ---> \",res)\n",
        "\n",
        "attention_plot = attention_plot[:len(res.split(' ')), :len(sentences.split(' '))]\n",
        "plot_attention(attention_plot, sentences.split(' '), res.split(' '))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rqz6osIprOAL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "interpreter": {
      "hash": "dffee67a034c262da22ecd866efe5fa1174a6876e1e96ed512a9acb5833498b8"
    },
    "kernelspec": {
      "display_name": "Python 3.10.0 ('tf_cpu')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}