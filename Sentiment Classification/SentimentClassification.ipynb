{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: fasttext in /usr/local/lib/python3.7/dist-packages (0.9.2)\n",
      "Requirement already satisfied: livelossplot in /usr/local/lib/python3.7/dist-packages (0.5.5)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.21.6)\n",
      "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext) (2.10.0)\n",
      "Requirement already satisfied: bokeh in /usr/local/lib/python3.7/dist-packages (from livelossplot) (2.3.3)\n",
      "Requirement already satisfied: ipython==7.* in /usr/local/lib/python3.7/dist-packages (from livelossplot) (7.9.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from livelossplot) (3.2.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (0.7.5)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (2.6.1)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (4.4.2)\n",
      "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (0.18.1)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (2.0.10)\n",
      "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (4.8.0)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (5.1.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython==7.*->livelossplot) (0.8.3)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython==7.*->livelossplot) (1.15.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython==7.*->livelossplot) (0.2.5)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (6.0)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (7.1.2)\n",
      "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (21.3)\n",
      "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (5.1.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (4.1.1)\n",
      "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (2.11.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (2.8.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.9->bokeh->livelossplot) (2.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=16.8->bokeh->livelossplot) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot) (1.4.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython==7.*->livelossplot) (0.7.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip install emoji\n",
    "#!pip install fasttext\n",
    "#!pip install transformers\n",
    "#!pip install transformers\n",
    "#https://swatimeena989.medium.com/bert-text-classification-using-keras-903671e0207d\n",
    "#!pip install transformers\n",
    "\n",
    "%pip install fasttext livelossplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11163,
     "status": "ok",
     "timestamp": 1561582739246,
     "user": {
      "displayName": "yaniel barbosa",
      "photoUrl": "https://lh3.googleusercontent.com/-JlWhURV1zuQ/AAAAAAAAAAI/AAAAAAAAADQ/hOOCIumFYTE/s64/photo.jpg",
      "userId": "13057870575473273651"
     },
     "user_tz": -60
    },
    "id": "NvL8Lqk_DX60",
    "outputId": "461236cc-495f-4e04-fbb1-af7f2aa38c58"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 1532383034093507088\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 14444920832\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 1791653456720173815\n",
      "physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"]=\"true\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense,  Dropout, Input, LSTM, Embedding,SpatialDropout1D, BatchNormalization, Flatten, LayerNormalization\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import word_tokenize,pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from collections import defaultdict        \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pathlib import Path\n",
    "\n",
    "from livelossplot.inputs.tf_keras import PlotLossesCallback\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.data.experimental.enable_debug_mode()\n",
    "import pickle\n",
    "import itertools\n",
    "import fasttext.util\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only use the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 1677011594496342944\n",
       " xla_global_id: -1, name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 14444920832\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 6782327861466878517\n",
       " physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
       " xla_global_id: 416903419]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_MODE= True\n",
    "emb_dim=50\n",
    "batchsize=256\n",
    "\n",
    "sequence_size= 500\n",
    "READ_DATA = False\n",
    "TRAIN_WORD_SENTIMENT = False\n",
    "ETL_STOPWORDS= False\n",
    "FIT_MODEL = False\n",
    "FIT_ATTENTION_MODEL = True\n",
    "FIT_TRANSFORMER_MODEL = True\n",
    "ETL_SENTENCE_SIZE = False\n",
    "MAKE_PREDICTION = True\n",
    "\n",
    "TUNNING = True\n",
    "TUNNING_ATTENTION_V2 = True\n",
    "TUNNING_ATTENTION = True\n",
    "TUNNING_TRANSFORMER = True\n",
    "\n",
    "max_trials = 30\n",
    "threshold = 1\n",
    "classN=1\n",
    "tf.random.set_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Emoji_Dict ={\n",
    "    \":p\" : \"<>\" ,\n",
    "    \":)\" :\"<SMILE_SIMPLE>\",\n",
    "    \":(\":\"<SAD>\",\n",
    "    \":'(\":\"<CRY>\",\n",
    "    \":')\":\"<BLINK_EYE>\",\n",
    "    \":d\" : \"<BIG_SMILE>\", \n",
    "    \":-)\":\"<SMILE_HAPPY>\",\n",
    "    \";-)\":\"<SMILE_A_LOT>\",\n",
    "    \":<})\":\"<MOUSTACHE_SMILE>\",\n",
    "    \":-||\":\"<MAD>\",\n",
    "    \":-(\":\"<SAD>\", \n",
    "    \":'-(\":\"<CRY>\",\n",
    "    \":-))\":\"<HAPPY>\",\n",
    "    \":-*\": \"<KISS>\",\n",
    "    \":-P~\":\"<LICK>\", \n",
    "    \":-o\" :\"<SURPRISED>\",\n",
    "    \":-|\":\"<GRIM>\",\n",
    "    \":-/\":\"<PERPLEXED>\",\n",
    "    \"=:O\"  :\"<FRIGHTNED>\",\n",
    "    \"<3\": \"<LOVE>\",\n",
    "    \"*****\": \"<5_STARS>\",\n",
    "    \"****\": \"<4_STARS>\",\n",
    "    \"***\": \"<3_STARS>\",\n",
    "    \"**\": \"<2_STARS>\",\n",
    "    \"*\": \"<1_STARS>\",\n",
    "    \".org\" : \"<>\"\n",
    "    }\n",
    "\n",
    "SPECIAL_NUMBERS = {\n",
    "    \"1\" : \"<1_STARS>\",\n",
    "    \"2\": \"<2_STARS>\",\n",
    "    \"3\": \"<3_STARS>\",\n",
    "    \"4\": \"<4_STARS>\", \n",
    "    \"5\": \"<5_STARS>\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Functions to perform data cleaning and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def translate_sentiment(sentiment, op=4):\n",
    "    if op ==4:\n",
    "        if sentiment >0 and sentiment <=3 :\n",
    "            sentiment = 0\n",
    "        elif sentiment >3 and sentiment < 5 :\n",
    "            sentiment = 1\n",
    "        elif sentiment >=5 and sentiment <=8 :\n",
    "            sentiment = 2\n",
    "        elif sentiment >8 and sentiment <=10 :\n",
    "            sentiment = 3\n",
    "    else:\n",
    "        if sentiment <5 :\n",
    "            sentiment = 0\n",
    "        elif sentiment >=5:\n",
    "            sentiment = 1\n",
    "        \n",
    "    return sentiment\n",
    "\n",
    "from string import digits\n",
    "\n",
    "\n",
    "def process_dataset(review_list):\n",
    "    \n",
    "    tag_map = defaultdict(lambda : wn.NOUN)\n",
    "    tag_map['J'] = wn.ADJ\n",
    "    tag_map['V'] = wn.VERB\n",
    "    tag_map['N'] = wn.NOUN\n",
    "    tag_map['R'] = wn.ADV\n",
    "    lemma_function = WordNetLemmatizer()\n",
    "    \n",
    "    string_check= re.compile('[^0-9a-zA-Z.?!\\'<>]')\n",
    "    \n",
    "    processed_review = []\n",
    "    for index, review in enumerate(review_list):\n",
    "\n",
    "        if index %5000 == 0:\n",
    "            print(\"Process process : \",index,\"/\",len(review_list),\" sentences processed...\")\n",
    "\n",
    "        original = review\n",
    "    \n",
    "        if review.find(\".org\") != -1:\n",
    "            site = review.split(\".org\")[0].split()[-1]\n",
    "            review = review.replace(site+\".org\",\"<URL>\")\n",
    "        elif review.find(\".com\") != -1:\n",
    "            site = review.split(\".com\")[0].split()[-1]\n",
    "            review = review.replace(site+\".com\",\"<URL>\")\n",
    "        \n",
    "        line = (re.sub('\\.+', \" . \", review).replace(\"?\",\" . \").replace(\"!\",\" . \").replace(\"<br />\",\" \"))\n",
    "\n",
    "\n",
    "        for em in list(Emoji_Dict.keys()):\n",
    "            line = line.replace(em,Emoji_Dict[em])\n",
    "\n",
    "        stripped =[]\n",
    "        tmp = nltk.word_tokenize(line)\n",
    "        for w in tmp:\n",
    "            if  w.isdigit() and str(w) in list(SPECIAL_NUMBERS.keys()):\n",
    "                stripped.append(SPECIAL_NUMBERS[w])\n",
    "            elif  w.isdigit():\n",
    "                stripped.append(\"<NUMBER>\")\n",
    "            else:\n",
    "                stripped.append(w)\n",
    "\n",
    "        line = ' '.join(stripped)\n",
    "        line = re.sub(string_check, ' ', (line)\n",
    "                        .replace(\"-\",\" \")\n",
    "                        .replace(\"´\",\"'\")\n",
    "                        .replace(\"`\",\"'\")\n",
    "                        .replace(\"'nt\",\" not\")\n",
    "                        .replace(\"i'm\",\"i am\")\n",
    "                        .replace(\"i 'm\",\"i am\")\n",
    "                        .replace(\"'s\",\" \")\n",
    "                        .replace(\"don't\",\"do not\")\n",
    "                        .replace(\"can't\",\"can not\")\n",
    "                        .replace(\"n't\",\" not\")\n",
    "                        .replace(\"'re\",\" are\")\n",
    "                        .replace(\"'d\",\" would\")\n",
    "                        .replace(\"'ve\",\" have\")\n",
    "                        .replace(\"'ll\",\" will\")\n",
    "                        .replace(\"'till\",\" until\")\n",
    "                        )\n",
    "            \n",
    "        \n",
    "        tokens = word_tokenize(line)\n",
    "        lemmas = [lemma_function.lemmatize(token, tag_map[tag[0]]) for token, tag in pos_tag(tokens)]\n",
    "\n",
    "        line = \" \".join(lemmas).replace(\" >\",\">\").replace(\"< \",\"<\")\n",
    "\n",
    "        processed_review.append(line)\n",
    "            \n",
    "    return  processed_review\n",
    "\n",
    "    \n",
    "def read_dataset(folder):\n",
    "    \n",
    "    review_list = []\n",
    "    sentiment_list = []\n",
    "    index_list = []\n",
    "\n",
    "    for i , path in enumerate(Path(folder).rglob('*.txt')):\n",
    "\n",
    "        if i %5000 == 0:\n",
    "            print(i,\"files read...\")\n",
    "            \n",
    "        info = ((str(path).split('\\\\')[-1]).split('.')[0]).split('_')\n",
    "        \n",
    "        try:\n",
    "            if info[0] != '' and info[1] != '':\n",
    "                identifier, sentiment = int(info[0]), int(info[1])\n",
    "            else:\n",
    "                print(\"invalid path format : \",path)\n",
    "\n",
    "            f=open(path,'r', encoding=\"utf8\")\n",
    "\n",
    "            original = str(f.read().lower())\n",
    "            original = ' '.join(original.split('\\t'))\n",
    "\n",
    "            sentiment = translate_sentiment(sentiment, op = classN)\n",
    "\n",
    "            review_list.append(original)\n",
    "            sentiment_list.append(sentiment)\n",
    "            index_list.append(i)\n",
    "            \n",
    "            if i < 1 :\n",
    "                print(\"Original --->\",original)\n",
    "    \n",
    "            f.close()\n",
    "\n",
    "\n",
    "        except Exception as e : \n",
    "            print(\"invalid path format : \",path, \".\", e)\n",
    "\n",
    "\n",
    "    cols = ['review','sentiment', 'id']\n",
    "    df_tmp= pd.DataFrame(columns=cols)\n",
    "\n",
    "    print(len(review_list), len(sentiment_list), len(index_list))\n",
    "    df_tmp[\"review\"] = review_list\n",
    "    df_tmp[\"sentiment\"] =sentiment_list\n",
    "    df_tmp[\"id\"] =index_list\n",
    "\n",
    "    return  df_tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words( sentence,stop_words,ds_size, index): #https://stackoverflow.com/questions/45447848/check-for-words-from-list-and-remove-those-words-in-pandas-dataframe-column\n",
    "\n",
    "    try:\n",
    "        word_tokens = sentence.values[0].split()\n",
    "    except:\n",
    "        word_tokens = sentence.split()\n",
    "\n",
    "    if index % 5000 == 0:\n",
    "        print(ds_size,\"\\ \",index)\n",
    "    \n",
    "    sent_df = pd.DataFrame(word_tokens, columns=[\"words\"])\n",
    "    \n",
    "    return ' '.join(sent_df[~sent_df.words.isin(stop_words)].words.tolist())\n",
    "\n",
    "def get_max_sentence_size(seq):\n",
    "    maxWords=0\n",
    "    \n",
    "    for seq_x in seq:\n",
    "        if(maxWords<len(seq_x)):\n",
    "            maxWords = len(seq_x)\n",
    "    return maxWords\n",
    "\n",
    "def tokenize_data(tookizer_aux, df_aux):\n",
    "\n",
    "    if TRAIN_MODE == True :\n",
    "        sequences_aux = tookizer_aux.texts_to_sequences(df_aux.iloc[:, 0].tolist()) # All the corpus text integer index\n",
    "    else:\n",
    "        sequences_aux = tookizer_aux.texts_to_sequences(df_aux.review)\n",
    "        \n",
    "    word_index = tookizer_aux.word_index\n",
    "    vocab_len = len(word_index) + 1  \n",
    "    \n",
    "    return tookizer_aux, vocab_len,sequences_aux,word_index\n",
    "\n",
    "def check_word_frequency(tokenizer_temp):\n",
    "    word_frequency = {}\n",
    "    for i, (word, count) in enumerate(tokenizer_temp.word_counts.items()):\n",
    "        word_frequency[word]=count\n",
    "            \n",
    "    return word_frequency\n",
    "\n",
    "def check_sentece_size(df_aux,size_ = None):\n",
    "    \n",
    "    for i in range(0, len(df_aux.review.values)):\n",
    "        \n",
    "        if  size_ is not None and len(df_aux.at[i,\"review\"].split()) > size_:\n",
    "            df_aux.at[i,\"sentence_size\"] = -1\n",
    "        else:\n",
    "            df_aux.at[i,\"sentence_size\"] = len(df_aux.at[i,\"review\"].split())\n",
    "        \n",
    "    return df_aux\n",
    "\n",
    "def update_stop_words(word_frequency, freq,max_freq,stop_word):\n",
    " \n",
    "    it = 0\n",
    "    for word, count in word_frequency.items():\n",
    "        if count < freq:\n",
    "            if it < 10:\n",
    "                print(word)\n",
    "                it +=1\n",
    "            stop_word.append(word)\n",
    "        elif count >max_freq :\n",
    "            stop_word.append(word)\n",
    "            \n",
    "    return list(set(stop_word))\n",
    "\n",
    "\n",
    "def plot_words(d):\n",
    "    fig, ax = plt.subplots(figsize = (40, 30))\n",
    "\n",
    "    x = d.word_pos_log.values\n",
    "    y = d.word_neg_log.values\n",
    "\n",
    "    # Plot a dot for each pair of words\n",
    "    ax.scatter(x, y)  \n",
    "\n",
    "    # assign axis labels\n",
    "    plt.xlabel(\"Log Positive count\")\n",
    "    plt.ylabel(\"Log Negative count\")\n",
    "\n",
    "    # Add the word as the label at the same position as you added the points just before\n",
    "    for i in range(0, len(d)):\n",
    "        ax.annotate(d.words.tolist()[i], (x[i], y[i]), fontsize=12)\n",
    "\n",
    "    ax.plot([0, np.max(x)], [0, np.max(y)], color = 'red') # Plot the red line that divides the 2 areas.\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_embeddings_fb(ft,vocab, emb_dim,vocab_l):\n",
    "    embeddings_matrix = np.zeros((vocab_l,emb_dim))\n",
    "    for word in vocab:\n",
    "        embeddings_matrix[vocab[word],:] = [float(i) for i in ft.get_word_vector(word)]\n",
    "            \n",
    "    f.close()\n",
    "    \n",
    "    return embeddings_matrix\n",
    "\n",
    "\n",
    "def create_trainingdev_set(sequences_aux,df_aux,maxlen,devtest_set):\n",
    "    x = pad_sequences(sequences_aux, padding='post', maxlen=maxlen)\n",
    "    y = np.asarray(df_aux.sentiment.values.tolist()).astype('float32')\n",
    "    inputs_train = []\n",
    "    inputs_val = []\n",
    "    if devtest_set:\n",
    "\n",
    "        test_elements_index = np.random.choice(len(x), int(len(x)*0.50), replace=False) \n",
    "        print(\"devtest_elements_index length : \",len(test_elements_index))\n",
    "        \n",
    "        x_in = np.take(x, test_elements_index, 0)\n",
    "        y_in = np.take(y, test_elements_index, 0)\n",
    "        print(\"x_in shape\", x_in.shape)\n",
    "        print(\"y_in shape\", y_in.shape)\n",
    "\n",
    "        x_test= np.delete(x, test_elements_index, 0)\n",
    "        y_test= np.delete(y, test_elements_index, 0)\n",
    "        print(\"x_test shape\", x_test.shape)\n",
    "        print(\"y_test shape\", y_test.shape)\n",
    "        \n",
    "        inputs_train.append(x_in)\n",
    "        inputs_val.append(x_test)\n",
    "        \n",
    "        return inputs_train,y_in, inputs_val,y_test \n",
    "    \n",
    "    else:\n",
    "        print(\"x shape\", x.shape)\n",
    "        print(\"y shape\", y.shape)\n",
    "        \n",
    "        inputs_train.append(x)\n",
    "        \n",
    "        \n",
    "        return inputs_train,y  \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if READ_DATA == True :\n",
    "\n",
    "    df_noprocess =read_dataset(\"../../../aclImdb/train/\")#D:\\\\Artificial_Intelligence\\\\aclImdb\\\\train\\\\\n",
    "    review_list = process_dataset(df_noprocess.review.tolist())\n",
    "    df = df_noprocess.copy()\n",
    "    df[\"review\"] = review_list\n",
    "\n",
    "\n",
    "    df_test_noprocess =read_dataset(\"../../../aclImdb/test/\")#D:\\\\Artificial_Intelligence\\\\aclImdb\\\\test\\\\\n",
    "    review_list_test = process_dataset(df_test_noprocess.review.tolist())\n",
    "    df_test = df_test_noprocess.copy()\n",
    "    df_test[\"review\"] = review_list_test\n",
    "\n",
    "\n",
    "    print(\"Dataset lentgh : \",len(df))\n",
    "    print(\"Test Dataset lentgh : \",len(df_test))\n",
    "\n",
    "    print(df.review.values.tolist()[0])\n",
    "    tokenizer = Tokenizer(filters='!\"#$%&()*+,-/:;=?@[\\\\]^_´`«»{|}~\\t\\n\\'',oov_token='<oov>')# sequence_size = 500 ~= mean+ std sentence size\n",
    "    tokenizer.fit_on_texts(df.review.values.tolist())\n",
    "    tokenizer.fit_on_texts(df_test.review.values.tolist())\n",
    " \n",
    "        \n",
    "    with open('./checkpoints/tokenizer_init.pkl', 'wb') as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    " \n",
    "    df.to_pickle(\"./checkpoints/df_init.pkl\")  \n",
    "    df_test.to_pickle(\"./checkpoints/df_test_init.pkl\")  \n",
    "    \n",
    "    df_noprocess.to_pickle(\"./checkpoints/df_noprocess_init.pkl\")  \n",
    "    df_test_noprocess.to_pickle(\"./checkpoints/df_test_noprocess_init.pkl\")  \n",
    "    \n",
    "    \n",
    "else:\n",
    "    \n",
    "    df = pd.read_pickle(\"./checkpoints/df_init.pkl\")\n",
    "    df_test = pd.read_pickle(\"./checkpoints/df_test_init.pkl\")\n",
    "    \n",
    "    df_noprocess = pd.read_pickle(\"./checkpoints/df_noprocess_init.pkl\")\n",
    "    df_test_noprocess = pd.read_pickle(\"./checkpoints/df_test_noprocess_init.pkl\")\n",
    "    \n",
    "    with open('./checkpoints/tokenizer_init.pkl', 'rb') as f:\n",
    "        tokenizer = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().any(),df_test.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_list = stopwords.words('english') \n",
    "min_freq = 3\n",
    "\n",
    "if ETL_STOPWORDS == True :\n",
    "    word_frequency= check_word_frequency(tokenizer)\n",
    "    max_freq = np.max(list(word_frequency.values())) +1 \n",
    "    updated_stop_word = update_stop_words(word_frequency, min_freq,max_freq, stop_words_list.copy())\n",
    "\n",
    "    with open('checkpoints/word_frequency.pkl', 'wb') as f:\n",
    "        pickle.dump(word_frequency, f)\n",
    "\n",
    "    with open('checkpoints/updated_stop_word.pkl', 'wb') as f:\n",
    "        pickle.dump(updated_stop_word, f)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    with open('checkpoints/word_frequency.pkl', 'rb') as f:\n",
    "        word_frequency = pickle.load(f)\n",
    "        \n",
    "    with open('checkpoints/updated_stop_word.pkl', 'rb') as f:\n",
    "        updated_stop_word = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ETL_STOPWORDS == True :\n",
    "    tokenizer_train = Tokenizer(filters='!\"#$%&()*+,-/:;=?@[\\\\]^_´`«»{|}~\\t\\n\\'',oov_token='<oov>')# sequence_size = 500 ~= mean+ std sentence size\n",
    "    tokenizer_val = Tokenizer(filters='!\"#$%&()*+,-/:;=?@[\\\\]^_´`«»{|}~\\t\\n\\'',oov_token='<oov>')# sequence_size = 500 ~= mean+ std sentence size\n",
    "    tokenizer_total = Tokenizer(filters='!\"#$%&()*+,-/:;=?@[\\\\]^_´`«»{|}~\\t\\n\\'',oov_token='<oov>')# sequence_size = 500 ~= mean+ std sentence size\n",
    "\n",
    "    tokenizer_train.fit_on_texts(df.review.values.tolist())\n",
    "    tokenizer_val.fit_on_texts(df_test.review.values.tolist())\n",
    "\n",
    "    tokenizer_total.fit_on_texts(df.review.values.tolist())\n",
    "    tokenizer_total.fit_on_texts(df_test.review.values.tolist())\n",
    "\n",
    "    t = list(tokenizer_train.word_index.keys())\n",
    "    v = list(tokenizer_val.word_index.keys())\n",
    "    test_unique_words = list(set(v) - set(t))\n",
    "\n",
    "    with open('checkpoints/test_unique_words.pkl', 'wb') as f:\n",
    "        pickle.dump(test_unique_words, f)\n",
    "else :\n",
    "    with open('checkpoints/test_unique_words.pkl', 'rb') as f:\n",
    "        test_unique_words = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ETL_STOPWORDS: \n",
    "\n",
    "    df_tmp = df.reset_index()\n",
    "    df_tmp.columns = [\"counter\",\"review\", \"sentiment\", \"id\"]\n",
    "    df[\"review\"] = df_tmp.apply(lambda x: remove_stop_words(x.review, updated_stop_word,len(df), x.counter), axis = 1)\n",
    "\n",
    "    df_tmp_test = df_test.reset_index()\n",
    "    df_tmp_test.columns = [\"counter\",\"review\", \"sentiment\", \"id\"]\n",
    "    df_test[\"review\"] = df_tmp_test.apply(lambda x: remove_stop_words(x.review, updated_stop_word,len(df_test), x.counter), axis = 1)\n",
    "\n",
    "    df.to_pickle(\"./checkpoints/df_no_stopwords.pkl\")  \n",
    "    df_test.to_pickle(\"./checkpoints/df_test_no_stopwords.pkl\")  \n",
    "\n",
    "else : \n",
    "    df = pd.read_pickle(\"./checkpoints/df_no_stopwords.pkl\")\n",
    "    df_test = pd.read_pickle(\"./checkpoints/df_test_no_stopwords.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = check_sentece_size(df)\n",
    "df_test = check_sentece_size(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.boxplot(x=df['sentence_size'])\n",
    "df.sentence_size.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df_test['sentence_size'])\n",
    "df_test.sentence_size.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Q1_train = df.sentence_size.quantile(0.25)\n",
    "Q3_train = df.sentence_size.quantile(0.75)\n",
    "IQR_train = Q3_train - Q1_train\n",
    "IQR_train, (Q1_train - 1.5 * IQR_train),(Q3_train + 1.5 * IQR_train), len(df[ df.sentence_size <= (Q3_train + 1.5 * IQR_train)]), len(df[ df.sentence_size > (Q3_train + 1.5 * IQR_train)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_test = df_test.sentence_size.quantile(0.25)\n",
    "Q3_test = df_test.sentence_size.quantile(0.75)\n",
    "IQR_test = Q3_test - Q1_test\n",
    "IQR_test, (Q1_test - 1.5 * IQR_test),(Q3_test + 1.5 * IQR_test), len(df_test[ df_test.sentence_size <= (Q3_test + 1.5 * IQR_test)]), len(df_test[ df_test.sentence_size > (Q3_test + 1.5 * IQR_test)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IQR_total = np.minimum((Q3_train + 1.5 * IQR_train),(Q3_test + 1.5 * IQR_test))\n",
    "sequence_size = int(IQR_total)\n",
    "sequence_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ETL_SENTENCE_SIZE == True :\n",
    "    \n",
    "    df = check_sentece_size(df,sequence_size)\n",
    "    df_filtered=df.copy()\n",
    "    df_filtered = df[df['sentence_size'] > 0] \n",
    "    df_filtered = df_filtered.reset_index().drop([\"index\"], axis = 1)\n",
    "    \n",
    "    df_test = check_sentece_size(df_test,sequence_size)\n",
    "    df_test_filtered=df_test.copy()\n",
    "    df_test_filtered = df_test[df_test['sentence_size'] > 0] \n",
    "    df_test_filtered = df_test_filtered.reset_index().drop([\"index\"], axis = 1)\n",
    "    \n",
    "    \n",
    "    tokenizer = Tokenizer(filters='!\"#$%&()*+,-/:;=?@[\\\\]^_´`«»{|}~\\t\\n\\'',oov_token='<oov>')# sequence_size = 500 ~= mean+ std sentence size\n",
    "    tokenizer.fit_on_texts(df_filtered.review.values.tolist())\n",
    "    tokenizer.fit_on_texts(df_test_filtered.review.values.tolist())\n",
    "    word_frequency= check_word_frequency(tokenizer)\n",
    "    \n",
    "    with open('checkpoints/word_frequency.pkl', 'wb') as f:\n",
    "        pickle.dump(word_frequency, f)\n",
    "\n",
    "    with open('checkpoints/tokenizer.pkl', 'wb') as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "        \n",
    "    df.to_pickle(\"./checkpoints/df.pkl\")  \n",
    "    df_test.to_pickle(\"./checkpoints/df_test.pkl\")  \n",
    "    \n",
    "    df_filtered.to_pickle(\"./checkpoints/df_filtered.pkl\")  \n",
    "    df_test_filtered.to_pickle(\"./checkpoints/df_test_filtered.pkl\") \n",
    "    \n",
    "else:\n",
    "    df = pd.read_pickle(\"./checkpoints/df.pkl\")\n",
    "    df_test = pd.read_pickle(\"./checkpoints/df_test.pkl\")\n",
    "    \n",
    "    df_filtered = pd.read_pickle(\"./checkpoints/df_filtered.pkl\")\n",
    "    df_test_filtered = pd.read_pickle(\"./checkpoints/df_test_filtered.pkl\")\n",
    "    \n",
    "    with open('checkpoints/word_frequency.pkl', 'rb') as f:\n",
    "        word_frequency = pickle.load(f)\n",
    "        \n",
    "    with open('checkpoints/tokenizer.pkl', 'rb') as f:\n",
    "        tokenizer = pickle.load(f)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "pd.Series(df_filtered.sentence_size).hist()\n",
    "plt.title(\"Sentence Size for training dataset\")\n",
    "\n",
    "plt.figure()\n",
    "pd.Series(df_filtered.sentiment).hist()\n",
    "plt.title(\"Sentiment for training dataset\")\n",
    "\n",
    "plt.show()\n",
    "print(pd.Series(df_filtered.sentence_size).describe())\n",
    "\n",
    "plt.figure()\n",
    "pd.Series(df_test_filtered.sentence_size).hist()\n",
    "plt.title(\"Sentence Size for test dataset\")\n",
    "\n",
    "plt.figure()\n",
    "pd.Series(df_test_filtered.sentiment).hist()\n",
    "plt.title(\"Sentiment for test dataset\")\n",
    "\n",
    "plt.show()\n",
    "print(pd.Series(df_test_filtered.sentence_size).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_naive = df_filtered.copy()\n",
    "df_test_filtered_naive = df_test_filtered.copy()\n",
    "\n",
    "df_filtered_naive.review = df_filtered.review.apply(lambda x : x.replace('.', ''))\n",
    "df_test_filtered_naive.review = df_test_filtered.review.apply(lambda x : x.replace('.', ''))\n",
    "df_filtered_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sentiment_df = pd.DataFrame.from_dict(word_frequency, orient='index').reset_index()\n",
    "word_sentiment_df.columns = [\"words\",\"frequency\"]\n",
    "df_filtered_naive[\"review_list\"] = df_filtered_naive.review.apply(lambda x : tokenizer.sequences_to_texts(tokenizer.texts_to_sequences([x]))[0].split())\n",
    "df_test_filtered_naive[\"review_list\"] = df_test_filtered_naive.review.apply(lambda x : tokenizer.sequences_to_texts(tokenizer.texts_to_sequences([x]))[0].split())\n",
    "word_sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "if TRAIN_WORD_SENTIMENT == True :   \n",
    "    words_df = pd.DataFrame(columns = [\"sent\", \"words\"])\n",
    "\n",
    "    sent_list = []\n",
    "    words_list = []\n",
    "    for i, (s, l) in  enumerate(zip(df_filtered_naive.sentiment, df_filtered_naive.review_list)):\n",
    "\n",
    "        s = [s]*len(l)\n",
    "        sent_list.append(s)\n",
    "        words_list.append(l)\n",
    "        \n",
    "        if i %5000 == 0:\n",
    "            print(i)\n",
    "\n",
    "    \n",
    "    for i, (s, l) in  enumerate(zip(df_test_filtered_naive.sentiment, df_test_filtered_naive.review_list)):\n",
    "        s = [s]*len(l)\n",
    "        sent_list.append(s)\n",
    "        words_list.append(l)\n",
    "\n",
    "        if i %5000 == 0:\n",
    "            print(i)\n",
    "\n",
    "    sent_list = list(itertools.chain(*sent_list))\n",
    "    words_list = list(itertools.chain(*words_list))\n",
    "\n",
    "    words_df = pd.DataFrame(columns = [\"sent\", \"words\"])\n",
    "    words_df[\"sent\"] = sent_list\n",
    "    words_df[\"words\"] =words_list\n",
    "\n",
    "    tmp =words_df[words_df.sent < threshold].groupby([\"words\"]).agg({\"words\":\"count\"}).rename(columns={\"words\":\"negative_frequence\"}).reset_index()\n",
    "    word_sentiment_df = pd.merge(word_sentiment_df,tmp, how=\"left\", on =\"words\" )\n",
    "\n",
    "    tmp =words_df[words_df.sent >= threshold].groupby([\"words\"]).agg({\"words\":\"count\"}).rename(columns={\"words\":\"positive_frequence\"}).reset_index()\n",
    "    word_sentiment_df = pd.merge(word_sentiment_df,tmp, how=\"left\", on =\"words\" )\n",
    "    word_sentiment_df = word_sentiment_df.fillna(0)\n",
    "\n",
    "    total_pos_words_freq = np.sum(word_sentiment_df.positive_frequence)\n",
    "    total_neg_words_freq = np.sum(word_sentiment_df.negative_frequence)\n",
    "    word_sentiment_df[\"prob_pos\"] = word_sentiment_df.positive_frequence/total_pos_words_freq\n",
    "    word_sentiment_df[\"prob_neg\"] = word_sentiment_df.negative_frequence/total_neg_words_freq\n",
    "    \n",
    "\n",
    "    #Calculate smoothing \n",
    "    word_sentiment_df[\"word_pos_smoothing\"] = (word_sentiment_df.positive_frequence + 1)/(total_pos_words_freq + len(word_sentiment_df[word_sentiment_df.positive_frequence >0]))\n",
    "    word_sentiment_df[\"word_neg_smoothing\"] = (word_sentiment_df.negative_frequence + 1)/(total_neg_words_freq + len(word_sentiment_df[word_sentiment_df.negative_frequence >0]))\n",
    "    word_sentiment_df[\"ratio\"] = word_sentiment_df[\"word_pos_smoothing\"]/word_sentiment_df[\"word_neg_smoothing\"]\n",
    "\n",
    "    #Fixing numeric underflow\n",
    "    word_sentiment_df[\"loglikelihood\"] = word_sentiment_df.ratio.apply(lambda x : np.log(x))\n",
    "\n",
    "    total_positive_sent = len(df_filtered_naive[df_filtered_naive.sentiment >= threshold]) + len(df_test_filtered_naive[df_test_filtered_naive.sentiment >= threshold])\n",
    "    total_negative_sent = len(df_filtered_naive[df_filtered_naive.sentiment < threshold]) + len(df_test_filtered_naive[df_test_filtered_naive.sentiment < threshold])\n",
    "\n",
    "    word_sentiment_df[\"prior\"] = total_positive_sent/total_negative_sent\n",
    "    word_sentiment_df[\"logprior\"] = np.log(word_sentiment_df[\"prior\"]) \n",
    "    \n",
    "    word_sentiment_df[\"word_pos_log\"] = word_sentiment_df.positive_frequence.apply(lambda x : np.log(x + 1))\n",
    "    word_sentiment_df[\"word_neg_log\"] = word_sentiment_df.negative_frequence.apply(lambda x : np.log(x + 1 ))\n",
    "\n",
    "    word_sentiment_df.to_pickle(\"./checkpoints/word_sentiment_df.pkl\")  \n",
    "else:\n",
    "    word_sentiment_df = pd.read_pickle(\"./checkpoints/word_sentiment_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive sentiment analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_naive = df_filtered_naive.reset_index().drop([\"index\"], axis = 1)\n",
    "df_test_filtered_naive = df_test_filtered_naive.reset_index().drop([\"index\"], axis = 1)\n",
    "word_sentiment_df = word_sentiment_df.fillna(0) #word_sentiment_df.fillna(-1)\n",
    "word_sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = word_sentiment_df.sample(frac=0.01)    \n",
    "plot_words(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-/:;=?@[\\\\]^_´`«»{|}~\\t\\n\\'',oov_token='<oov>')# sequence_size = 500 ~= mean+ std sentence size\n",
    "tokenizer.fit_on_texts(df_filtered_naive.review.values.tolist())\n",
    "tokenizer.fit_on_texts(df_test_filtered_naive.review.values.tolist())\n",
    "word_frequency= check_word_frequency(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_predict(test_sent, word_sentiment_df):\n",
    "    # initialize probability to zero\n",
    "    p = 0\n",
    "\n",
    "    # add the logprior\n",
    "    p += word_sentiment_df[\"logprior\"].values[0]\n",
    "    p += word_sentiment_df[word_sentiment_df.words.isin(test_sent)].loglikelihood.sum()\n",
    "   \n",
    "    return p\n",
    "\n",
    "test_df_naive= df_test_filtered_naive.sample(frac=0.05)    \n",
    "test_df_naive[\"sent_filtered\"] = test_df_naive.sentiment.apply(lambda x: 1 if x>=threshold else 0 )\n",
    "\n",
    "test_df_naive[\"prediction\"] = test_df_naive.review_list.apply(lambda x : 1 if naive_bayes_predict(x,word_sentiment_df) >0 else 0)\n",
    "\n",
    "\n",
    "test_df_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tf.keras.metrics.Accuracy()\n",
    "m.update_state(test_df_naive.prediction.tolist(), test_df_naive.sent_filtered)\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_elements_index = np.random.choice(1000, 200, replace=False) \n",
    "print(\"devtest_elements_index length : \",(test_elements_index))\n",
    "\n",
    "test_df_naive_filtered = df_test_filtered_naive[df_test_filtered_naive.index.isin(test_elements_index)] \n",
    "df_test_noprocess_filtered = df_test_noprocess.copy()\n",
    "\n",
    "test_df_naive_filtered[\"review_list\"] = test_df_naive_filtered.review.apply(lambda x : x.split())\n",
    "test_df_naive_filtered[\"sent_filtered\"] = test_df_naive_filtered.sentiment.apply(lambda x: 1 if x>=threshold else 0 )\n",
    "test_df_naive_filtered = test_df_naive_filtered.reset_index().drop([\"index\"], axis = 1)\n",
    "test_df_naive_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_noprocess_filtered[\"sent_filtered\"] = df_test_noprocess_filtered.sentiment.apply(lambda x: 1 if x>=threshold else 0 )\n",
    "df_test_noprocess_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Truth Predicted Tweet')\n",
    "index = 0\n",
    "for i in range(len(test_df_naive_filtered)):\n",
    "    \n",
    "    y_hat = naive_bayes_predict(test_df_naive_filtered.review_list.values[i], word_sentiment_df)\n",
    "    \n",
    "    y = df_test_noprocess_filtered.sent_filtered.values[i]\n",
    "    x  = df_test_noprocess_filtered[(df_test_noprocess_filtered.id == test_df_naive_filtered.id.values[i])].review.tolist()\n",
    "    \n",
    "    if y != (np.sign(y_hat) > 0) and index <=10:\n",
    "\n",
    "        aux = test_df_naive_filtered.review.values[i]\n",
    "        print(f'ID : {test_df_naive_filtered.id.values[i]}  Y : {y}\\ny_hat:{int(np.sign(y_hat)> 0 )} :  \\nReal : {x} \\n\\nProcessed : {aux} \\n ------------------------\\n')\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_out=[]\n",
    "for i in range(len(df_test_filtered_naive)):\n",
    "    y_hat = naive_bayes_predict(df_test_filtered_naive.review.values[i].split(), word_sentiment_df)\n",
    "    y_out.append(int(np.sign(y_hat) > 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = np.array(y_out).reshape((len(y_out),1))\n",
    "\n",
    "y_test = df_test_filtered_naive.sentiment.values\n",
    "y_test = y_test.reshape((len(y_test),1))\n",
    "\n",
    "pr = tf.keras.metrics.AUC(curve=\"PR\")\n",
    "\n",
    "p = tf.keras.metrics.Precision()\n",
    "p.update_state(y_test,yhat)\n",
    "\n",
    "r = tf.keras.metrics.Recall()\n",
    "r.update_state(y_test,yhat)\n",
    "\n",
    "print(\"AUC\",pr(y_test, yhat).numpy(),\" | Precision : \",p.result().numpy(), \" | Recall : \",r.result().numpy())\n",
    "confusion_matrix(list(y_test),list(yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_naive_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.sentiment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = (df_filtered.dropna()).reset_index().drop([\"index\"], axis = 1)\n",
    "df_test_filtered = (df_test_filtered.dropna()).reset_index().drop([\"index\"], axis = 1)\n",
    "df_test_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = fasttext.load_model('../cc.en.300.bin')\n",
    "fasttext.util.reduce_model(ft, emb_dim)\n",
    "ft.get_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-/:;=?@[\\\\]^_´`«»{|}~\\t\\n\\'',oov_token='<oov>')# sequence_size = 500 ~= mean+ std sentence size\n",
    "tokenizer.fit_on_texts(df_filtered.review.values.tolist())\n",
    "tokenizer.fit_on_texts(df_test_filtered.review.values.tolist())\n",
    "_, vocab_len,sequences,word_index= tokenize_data(tokenizer, df_filtered)\n",
    "print(\"Train Vocab length : It should be the complete one : \",vocab_len)\n",
    "_, vocab_len_test,sequences_test,word_index_test= tokenize_data(tokenizer, df_test_filtered)\n",
    "print(\"Test Vocab length. It should be the complete one : \",vocab_len_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_matrix=get_embeddings_fb(ft,word_index,emb_dim,vocab_len)\n",
    "print(embeddings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = create_trainingdev_set(sequences,df_filtered,sequence_size, False)\n",
    "x_dev,y_dev,x_test,y_test = create_trainingdev_set(sequences_test,df_test_filtered,sequence_size,True)\n",
    "y_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training process \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gc import callbacks\n",
    "import keras_tuner as kt\n",
    "\n",
    "def get_model_dynamic(best_hps):\n",
    "    pr = tf.keras.metrics.AUC(curve=\"PR\")\n",
    "    p = tf.keras.metrics.Precision()\n",
    "    r = tf.keras.metrics.Recall()\n",
    "\n",
    "    inp = Input(batch_shape = (None,sequence_size,), name=\"X\")\n",
    "\n",
    "    ft = fasttext.load_model('../cc.en.300.bin')\n",
    "    fasttext.util.reduce_model(ft, best_hps[\"emb_dim\"])\n",
    "    embeddings_matrix_tmp=get_embeddings_fb(ft,word_index,best_hps[\"emb_dim\"],vocab_len)\n",
    "\n",
    "    X = Embedding(input_dim = vocab_len, output_dim = best_hps[\"emb_dim\"], input_length = sequence_size ,name =\"emb_layer\",  weights=[embeddings_matrix_tmp], trainable=False, mask_zero=True) (inp)# Use masking to handle the variable sequence lengths\n",
    "    #X = SpatialDropout1D(best_hps.get('SpatialDropout1D_1'))(X)\n",
    "    X = (LSTM(best_hps.get('units_0'),  dropout=best_hps.get('Dropout_0'), return_sequences=False))(X)\n",
    "\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(best_hps.get('Dropout_1'))(X)\n",
    "    X = Dense(best_hps.get('units_1'), activation='relu')(X)\n",
    "\n",
    "\n",
    "    Z = Dense(classN, activation='sigmoid')(X)\n",
    "\n",
    "    model =  Model(inputs = inp, outputs=Z )\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(learning_rate=best_hps.get('learning_rate')), metrics =[tf.keras.metrics.BinaryAccuracy(),pr,p,r,tf.keras.metrics.AUC()])\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_model_tunning(hp):\n",
    "\n",
    "    hp_units_0 = hp.Int('units_0', min_value=32, max_value=256, step=32)\n",
    "    hp_units_1 = hp.Int('units_1', min_value=32, max_value=256, step=32)\n",
    "    hp_drop_0 = hp.Choice('Dropout_0', values=[ 0.2, 0.3, 0.4, 0.5])\n",
    "    hp_drop_1 = hp.Choice('Dropout_1', values=[ 0.2, 0.3, 0.4, 0.5])\n",
    "    #hp_SpatialDropout1D_1 = hp.Choice('SpatialDropout1D_1', values=[0.1, 0.2, 0.3, 0.4, 0.5])\n",
    "    hp_emb_dim = hp.Choice('emb_dim', values=[ 50, 100, 200])\n",
    "\n",
    "\n",
    "    ft = fasttext.load_model('../cc.en.300.bin')\n",
    "    fasttext.util.reduce_model(ft, hp_emb_dim)\n",
    "    embeddings_matrix_tmp=get_embeddings_fb(ft,word_index,hp_emb_dim,vocab_len)\n",
    "\n",
    "\n",
    "    inp = Input(batch_shape = (None,sequence_size,), name=\"X\")\n",
    "    \n",
    "    X = Embedding(input_dim = vocab_len, output_dim = hp_emb_dim, input_length = sequence_size ,name =\"emb_layer\",  weights=[embeddings_matrix_tmp], trainable=False, mask_zero=True) (inp)# Use masking to handle the variable sequence lengths\n",
    "    #X = SpatialDropout1D(hp_SpatialDropout1D_1)(X)\n",
    "    X = (LSTM(hp_units_0, dropout=hp_drop_0, return_sequences=False))(X)\n",
    "\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(hp_drop_1)(X)\n",
    "    X = Dense(hp_units_1, activation='relu')(X)\n",
    "\n",
    "\n",
    "    Z = Dense(classN, activation='sigmoid')(X)\n",
    "    \n",
    "    model =  Model(inputs = inp, outputs=Z )\n",
    "    \n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[0.001, 0.005])\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), optimizer=Adam(learning_rate=hp_learning_rate), metrics =[tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.AUC()])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "        get_model_tunning,\n",
    "        objective=kt.Objective('val_auc', direction=\"max\"),# #val_binary_accuracy\n",
    "        max_trials = max_trials,\n",
    "        directory=r\"Hyperparam_tunning\",\n",
    "        project_name='keras_tunning'\n",
    "    )\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_auc', patience=30)\n",
    "if TUNNING:\n",
    "    tuner.search(x[0], y,  epochs=300, batch_size=batchsize, validation_data = (x_dev[0],y_dev), verbose =2, callbacks=[stop_early]) \n",
    "\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_hps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps[\"units_0\"],best_hps[\"units_1\"],best_hps[\"Dropout_0\"],best_hps[\"Dropout_0\"],best_hps[\"learning_rate\"]\n",
    "#(64, 64, 0.2, 0.2, 0.5, 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dynamic = get_model_dynamic(best_hps)\n",
    "if FIT_MODEL:\n",
    "    \n",
    "    model_dynamic.fit(x[0], y, \n",
    "              batch_size=batchsize, \n",
    "              epochs=300,\n",
    "              shuffle=True,\n",
    "              validation_data = (x_dev[0],y_dev),\n",
    "              callbacks=[PlotLossesCallback() ,stop_early]\n",
    "             )\n",
    "    \n",
    "    model_dynamic.save('./checkpoints_dynamic/model_dynamic')\n",
    "else:\n",
    "    model_dynamic = tf.keras.models.load_model('./checkpoints_dynamic/model_dynamic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat =model_dynamic.predict(x_test[0][0:100])\n",
    "yhat[yhat >=0.5] = 1\n",
    "yhat[yhat < 0.5] = 0 \n",
    "yhat\n",
    "\n",
    "y_test = y_test.reshape((len(y_test),1))\n",
    "y_test\n",
    "print(\"x 1º line : \"+str(x))\n",
    "print(\"y : \",y) \n",
    "print(\"y_test : \",y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ySMgYZbFDX7d"
   },
   "outputs": [],
   "source": [
    "\n",
    "yhat =model_dynamic.predict(x_test)\n",
    "yhat[yhat >=0.5] = 1\n",
    "yhat[yhat < 0.5] = 0 \n",
    "y_test = y_test.reshape((len(y_test),1))\n",
    "\n",
    "pr = tf.keras.metrics.AUC(curve=\"PR\")\n",
    "print()\n",
    "\n",
    "p = tf.keras.metrics.Precision()\n",
    "p.update_state(y_test,yhat)\n",
    "\n",
    "r = tf.keras.metrics.Recall()\n",
    "r.update_state(y_test,yhat)\n",
    "\n",
    "base_pr = pr(y_test, yhat).numpy()\n",
    "base_p = p.result().numpy()\n",
    "base_r = r.result().numpy()\n",
    "print(\"AUC\",base_pr,\" | Precision : \",base_p, \" | Recall : \",base_r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "confusion_matrix(list(y_test),list(yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_tn, base_fp, base_fn, base_tp = confusion_matrix(list(y_test),list(yhat)).ravel()\n",
    "base_tn, base_fp, base_fn, base_tp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15474,
     "status": "ok",
     "timestamp": 1561582743898,
     "user": {
      "displayName": "yaniel barbosa",
      "photoUrl": "https://lh3.googleusercontent.com/-JlWhURV1zuQ/AAAAAAAAAAI/AAAAAAAAADQ/hOOCIumFYTE/s64/photo.jpg",
      "userId": "13057870575473273651"
     },
     "user_tz": -60
    },
    "id": "mebJWLtxDX7l",
    "outputId": "53d69c67-1e6f-4f8f-8c09-817fc8ea8dd3"
   },
   "outputs": [],
   "source": [
    "def print_predictions(X, pred):\n",
    "    for i in range(len(X)):\n",
    "        xx = [tokenizer.index_word.get(ind) for ind in X[i] if tokenizer.index_word.get(ind) is not None]\n",
    "        print(' '.join(xx), \"Prediction :\", int(pred[i]),\" - Real :\",y_test[i][0],\"\\n\")\n",
    "        \n",
    "        if i==10:\n",
    "            break\n",
    "\n",
    "print_predictions(x_test[0], yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 32\n",
    "emb_dim = 300\n",
    "epochs = 100\n",
    "ft = fasttext.load_model('../cc.en.300.bin')\n",
    "fasttext.util.reduce_model(ft, emb_dim)\n",
    "ft.get_dimension()\n",
    "\n",
    "embeddings_matrix=get_embeddings_fb(ft,word_index,emb_dim,vocab_len)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x[0],y.reshape((len(y),1))))\n",
    "for i,o in dataset.take(3):\n",
    "    print(\"Input : \",i.numpy(), \" ->\",i.numpy().shape)\n",
    "    print(\"Output : \",o.numpy(), \" ->\",o.numpy().shape,\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "dataset = dataset.shuffle(buffer_size=10000).batch(batchsize, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "print(\"Batched dataset ... \\n\")\n",
    "for i,o in dataset.take(1):\n",
    "    print(\"Input : \",i)\n",
    "    print(\"Output : \",o,\"\\n\\n\")\n",
    "\n",
    "ds_len = len(list(dataset))\n",
    "val_size = int(0.07 * ds_len)\n",
    "val_dataset = dataset.take(val_size) \n",
    "train_dataset = dataset.skip(val_size)\n",
    "\n",
    "train_size = len(list(train_dataset))\n",
    "\n",
    "print(ds_len,\"-->\",train_size,\"--->\",val_size,\"/\",ds_len*batchsize)\n",
    "\n",
    "dataset = train_dataset\n",
    "ds_len = train_size\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test[0],y_test.reshape((len(y_test),1))))\n",
    "test_dataset = test_dataset.batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def plot_attention(attention, sentence, predicted_sentence, figsize  =(10, 10)):\n",
    "  sentence = sentence.split()\n",
    "  predicted_sentence = predicted_sentence.split() + ['<eos>']\n",
    "  fig = plt.figure(figsize=figsize)\n",
    "  ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "  attention = attention[:len(predicted_sentence), :len(sentence)]\n",
    "\n",
    "  ax.matshow(attention, cmap='viridis', vmin=0.0)\n",
    "\n",
    "  fontdict = {'fontsize': 14}\n",
    "\n",
    "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "  ax.set_xlabel('Input text')\n",
    "  ax.set_ylabel('Output text')\n",
    "  plt.suptitle('Attention weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SelfMaskedDotProductAttention(tf.keras.Model): \n",
    "    def __init__(self):\n",
    "        super(SelfMaskedDotProductAttention, self).__init__()\n",
    "\n",
    "\n",
    "    def call(self, q, k, v, padding_mask= None, look_ahead_mask= None):\n",
    "        \n",
    "        PRINT_SHAPE= False\n",
    "                \n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True) \n",
    "\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "        if PRINT_SHAPE : print(\"weight shape : \", scaled_attention_logits.shape,\" - [batch_size, Tq, Tv]\")\n",
    "        \n",
    "        if padding_mask is not None:\n",
    "            padding_mask = tf.expand_dims(padding_mask, 1)\n",
    "            scaled_attention_logits = tf.where(padding_mask, scaled_attention_logits, tf.experimental.numpy.full_like(scaled_attention_logits,  -1e9))\n",
    "\n",
    "        if look_ahead_mask is not None:\n",
    "            scaled_attention_logits += (look_ahead_mask * -1e9)\n",
    "\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1) \n",
    "        if PRINT_SHAPE : print(\"attention_weights / values : \",attention_weights.shape, v.shape)\n",
    "\n",
    "        context_vector = tf.matmul(attention_weights, v)\n",
    "\n",
    "        if PRINT_SHAPE : print(\"context_vector shape : \", context_vector.shape)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "\n",
    "class MaskedLossCustom(tf.keras.losses.Loss):\n",
    "  def __init__(self):\n",
    "    self.name = 'masked_loss_custom'\n",
    "    self.loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "  def __call__(self, y_true, y_pred):\n",
    "\n",
    "    # Calculate the loss for each item in the batch.\n",
    "    loss = self.loss(y_true, y_pred)\n",
    "\n",
    "    # Mask off the losses on padding.\n",
    "    mask = tf.cast(y_true != 0, tf.float32)\n",
    "    loss *= mask\n",
    "\n",
    "    # Return the total.\n",
    "    return tf.reduce_sum(loss)\n",
    "    \n",
    "auc = tf.keras.metrics.AUC(curve=\"PR\")\n",
    "p = tf.keras.metrics.Precision()\n",
    "r = tf.keras.metrics.Recall()\n",
    "\n",
    "\n",
    "\n",
    "class Sentiment_Attention_v2(tf.keras.Model):\n",
    "    def __init__(self, unit_lstm, unit_dropout_1, unit_dropout_2, unit_dense, vocab_len, emb_dim, embeddings_matrix, classes, input_length, causal_mask_enabled=True):\n",
    "        super(Sentiment_Attention_v2, self).__init__()\n",
    "\n",
    "        \n",
    "        self.embedding_layer = Embedding(input_dim = vocab_len, output_dim = emb_dim, input_length = input_length ,name =\"emb_layer\",  weights=[embeddings_matrix], trainable=False, mask_zero=True)# Use masking to handle the variable sequence lengths\n",
    "        self.lstm = LSTM(unit_lstm,  return_sequences=True, dropout=unit_dropout_1, return_state = True)#dropout=0.5,  1024\n",
    "        self.batch_norm = BatchNormalization()\n",
    "        self.drop = Dropout(unit_dropout_2)\n",
    "        self.d0 = Dense(unit_dense, activation='relu')\n",
    "        self.d = Dense(classes, activation='sigmoid')\n",
    "\n",
    "        self.dot_attention = SelfMaskedDotProductAttention()\n",
    "        self.causal_mask_enabled = causal_mask_enabled\n",
    "\n",
    "\n",
    "    def create_look_ahead_mask(self, size):\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "        return mask  \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        input_mask = (inputs != 0)\n",
    "        x = self.embedding_layer(inputs)\n",
    "\n",
    "        last_state = self.lstm.get_initial_state(x)\n",
    "        lstm_out, h_state, c_state = self.lstm(x, initial_state=last_state ) \n",
    "\n",
    "        if self.causal_mask_enabled == False :\n",
    "            look_ahead_mask = None\n",
    "        else:\n",
    "            look_ahead_mask = self.create_look_ahead_mask(tf.shape(inputs)[1])\n",
    "\n",
    "        context_vector, attention_weights = self.dot_attention(lstm_out, lstm_out, lstm_out, input_mask, look_ahead_mask)\n",
    "        x = tf.reduce_sum(context_vector, axis = 1)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.d0(x)\n",
    "        pred = self.d(x)\n",
    "                       \n",
    "        return pred\n",
    "\n",
    "    def predict_sentiment(self, inputs, input_mask = None, last_state= None):\n",
    "        \n",
    "        x = self.embedding_layer(inputs)\n",
    "        \n",
    "\n",
    "        if last_state is None:\n",
    "            last_state = self.lstm.get_initial_state(x)\n",
    "        \n",
    "        lstm_out, h_state, c_state = self.lstm(x, initial_state=last_state ) \n",
    "\n",
    "        if self.causal_mask_enabled == False :\n",
    "            look_ahead_mask = None\n",
    "        else:\n",
    "            look_ahead_mask = self.create_look_ahead_mask(tf.shape(inputs)[1])\n",
    "\n",
    "        context_vector, attention_weights = self.dot_attention(lstm_out, lstm_out, lstm_out, input_mask, look_ahead_mask)\n",
    "        x = tf.reduce_sum(context_vector, axis =1)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.d0(x)\n",
    "        pred = self.d(x)\n",
    "                       \n",
    "        return pred, (h_state, c_state), attention_weights, lstm_out, context_vector \n",
    "\n",
    "\n",
    "\n",
    "class Sentiment_Attention(tf.keras.Model):\n",
    "    def __init__(self, lstm_unit, dropout_0, dense_unit, vocab_len, emb_dim, embeddings_matrix, classes, input_length, causal_mask_enabled=True):\n",
    "        super(Sentiment_Attention, self).__init__()\n",
    "\n",
    "        \n",
    "        self.embedding_layer = Embedding(input_dim = vocab_len, output_dim = emb_dim, input_length = input_length ,name =\"emb_layer\",  weights=[embeddings_matrix], trainable=False, mask_zero=True)# Use masking to handle the variable sequence lengths\n",
    "        self.lstm = LSTM(lstm_unit,   return_sequences=True, return_state = True)#512\n",
    "        self.batch_norm = BatchNormalization()\n",
    "        self.drop = Dropout(dropout_0)\n",
    "        self.d0 = Dense(dense_unit, activation='relu')\n",
    "        self.d = Dense(classes, activation='sigmoid')\n",
    "\n",
    "        self.dot_attention = SelfMaskedDotProductAttention()\n",
    "        self.causal_mask_enabled = causal_mask_enabled\n",
    "\n",
    "\n",
    "    def create_look_ahead_mask(self, size):\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "        return mask  \n",
    "    \n",
    "    def call(self, inputs, input_mask = None, last_state= None):\n",
    "        \n",
    "        x = self.embedding_layer(inputs)\n",
    "        #x = self.spatial(x)\n",
    "\n",
    "        if last_state is None:\n",
    "            last_state = self.lstm.get_initial_state(x)\n",
    "        \n",
    "        lstm_out, h_state, c_state = self.lstm(x, initial_state=last_state ) \n",
    "\n",
    "        if self.causal_mask_enabled == False :\n",
    "            look_ahead_mask = None\n",
    "        else:\n",
    "            look_ahead_mask = self.create_look_ahead_mask(tf.shape(inputs)[1])\n",
    "\n",
    "        context_vector, attention_weights = self.dot_attention(lstm_out, lstm_out, lstm_out, input_mask, look_ahead_mask)\n",
    "        x = tf.reduce_sum(context_vector, axis = -1)\n",
    "        x = self.batch_norm(x)\n",
    "        #x = self.drop(x)\n",
    "        x = self.d0(x)\n",
    "        pred = self.d(x)\n",
    "                       \n",
    "        return pred, (h_state, c_state), attention_weights, lstm_out, context_vector \n",
    "\n",
    "class ModelClass(tf.keras.Model):\n",
    "    def __init__(self, lstm_unit, dropout_0, dense_unit, vocab_len, emb_dim, embeddings_matrix, classes, sequence_size, causal_mask_enabled=True ):\n",
    "        super(ModelClass, self).__init__()\n",
    "        self.vocab_size = vocab_len\n",
    "        self.sequence_size = sequence_size\n",
    "        self.sentiment_attention_model = Sentiment_Attention(lstm_unit, dropout_0, dense_unit, vocab_len, emb_dim, embeddings_matrix, classes, sequence_size, causal_mask_enabled)\n",
    "        self.use_tf_function = True\n",
    "\n",
    "    def call(self, inputs, last_state = None ):\n",
    "        input_mask = inputs != 0\n",
    "        pred, last_state , attention_weights, last_activation, last_attention  = self.sentiment_attention_model(inputs,  input_mask, last_state)\n",
    "\n",
    "        return pred, last_state , attention_weights, last_activation, last_attention\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        if self.use_tf_function:\n",
    "            return self._tf_train_step(inputs)\n",
    "        else:\n",
    "            return self._train_step(inputs)\n",
    "\n",
    "\n",
    "    def _train_step(self, inputs):\n",
    "        x, y = inputs  \n",
    "        target_mask = y != 0     \n",
    "        last_state= None\n",
    "        last_activation = None\n",
    "        last_attention = None\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = tf.constant(0.0)\n",
    "            acc = []\n",
    "            auc_list = []\n",
    "            r_list = []\n",
    "            p_list = []\n",
    "\n",
    "            input_mask = x != 0\n",
    "            pred, last_state , attention_weights, last_activation, last_attention  = self.sentiment_attention_model(x,  input_mask, last_state)\n",
    "\n",
    "            loss += self.loss(y, pred)\n",
    "            self.compiled_metrics.update_state(y, pred)\n",
    "            acc.append(self.metrics[0].result())\n",
    "\n",
    "            auc.update_state(y, pred)\n",
    "            auc_list.append(auc.result())\n",
    "\n",
    "            r.update_state(y, pred)\n",
    "            r_list.append(r.result())\n",
    "\n",
    "            p.update_state(y, pred)\n",
    "            p_list.append(p.result())\n",
    "\n",
    "        average_loss = loss \n",
    "        average_acc  = tf.reduce_mean(acc)\n",
    "        average_auc  = tf.reduce_mean(tf.convert_to_tensor(auc_list) )\n",
    "        average_r  = tf.reduce_mean(tf.convert_to_tensor(r_list) )\n",
    "        average_p  = tf.reduce_mean(tf.convert_to_tensor(p_list) )\n",
    "            \n",
    "        variables = self.trainable_variables \n",
    "        gradients = tape.gradient(average_loss, variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "        m = {m.name: average_acc   for m in self.metrics}\n",
    "        m[\"loss\"] = average_loss\n",
    "        m[\"auc\"] = average_auc\n",
    "        m[\"recall_avrg\"] = average_r\n",
    "        m[\"precision_avrg\"] = average_p\n",
    "    \n",
    "        return m\n",
    "    \n",
    "    @tf.function(input_signature=[[tf.TensorSpec(dtype=tf.int32, shape=[None, sequence_size]),\n",
    "                                tf.TensorSpec(dtype=tf.int32, shape=[None, 1])]])#\n",
    "                                \n",
    "    def _tf_train_step(self, inputs):\n",
    "        return self._train_step(inputs)\n",
    "\n",
    "\n",
    "    def test_step(self, inputs):\n",
    "        if self.use_tf_function:\n",
    "            return self._tf_test_step(inputs)\n",
    "        else:\n",
    "            return self._test_step(inputs)\n",
    "\n",
    "    \n",
    "    def _test_step(self, inputs):\n",
    "        x, y = inputs  \n",
    "        \n",
    "        input_mask = None\n",
    "\n",
    "        loss_val = tf.constant(0.0)\n",
    "        acc = []\n",
    "        auc_list = []\n",
    "        r_list = []\n",
    "        p_list = []\n",
    "\n",
    "        last_state= None\n",
    "        input_mask = x != 0\n",
    "        \n",
    "        pred, last_state , attention_weights, last_activation, last_attention  = self.sentiment_attention_model(x, input_mask, last_state)\n",
    "        \n",
    "        loss_val += self.loss(y, pred)\n",
    "\n",
    "        self.compiled_metrics.update_state(y, pred)\n",
    "        acc.append(self.metrics[0].result())\n",
    "\n",
    "        auc.update_state(y, pred)\n",
    "        auc_list.append(auc.result())\n",
    "\n",
    "        r.update_state(y, pred)\n",
    "        r_list.append(r.result())\n",
    "\n",
    "        p.update_state(y, pred)\n",
    "        p_list.append(p.result())\n",
    "\n",
    "\n",
    "        average_loss = loss_val\n",
    "        average_acc  = tf.reduce_mean(acc)\n",
    "        average_auc  = tf.reduce_mean(tf.convert_to_tensor(auc_list) )\n",
    "        average_r  = tf.reduce_mean(tf.convert_to_tensor(r_list) )\n",
    "        average_p  = tf.reduce_mean(tf.convert_to_tensor(p_list) )\n",
    "\n",
    "        m = {m.name: average_acc  for m in self.metrics}\n",
    "        m[\"loss\"] = average_loss\n",
    "        m[\"auc\"] = average_auc\n",
    "        m[\"recall_avrg\"] = average_r\n",
    "        m[\"precision_avrg\"] = average_p\n",
    "        \n",
    "        return m\n",
    "\n",
    "\n",
    "    @tf.function(input_signature=[[tf.TensorSpec(dtype=tf.int32, shape=[None, sequence_size]),\n",
    "                                tf.TensorSpec(dtype=tf.int32, shape=[None, 1])\n",
    "                                ]])\n",
    "    def _tf_test_step(self, inputs):\n",
    "        return self._test_step(inputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_layer = Embedding(input_dim=vocab_len, input_length = sequence_size, output_dim=emb_dim, weights=[embeddings_matrix] ,name =\"emb_layer\", trainable=False, mask_zero = True)\n",
    "attention_layer = SelfMaskedDotProductAttention()\n",
    "query = emb_layer(i)\n",
    "values = emb_layer(i)\n",
    "\n",
    "context_vector, attention_weights_nomask = attention_layer(query,values,values)\n",
    "context_vector, attention_weights = attention_layer(query,values,values, padding_mask=(i != 0))\n",
    "context_vector, attention_weightsv2 = attention_layer(query,values,values, padding_mask=(i != 0), look_ahead_mask= (1 - tf.linalg.band_part(tf.ones((i.shape[1], i.shape[1])), -1, 0)))\n",
    "\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.pcolormesh(attention_weights_nomask[0])#[ :mask_size,:mask_size]\n",
    "plt.title('Attention weights - Not mask')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.pcolormesh(attention_weights[0])#[ :mask_size,:mask_size]\n",
    "plt.title('Attention weights - Not causal')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.pcolormesh(attention_weightsv2[0])#[ :mask_size,:mask_size]\n",
    "plt.title('Attention weights 1')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.pcolormesh(i != 0)\n",
    "plt.title('Mask')\n",
    "\n",
    "sentence = ' '.join([tokenizer.index_word.get(ind.numpy()) for ind in i[0] if tokenizer.index_word.get(ind.numpy()) is not None])\n",
    "plot_attention(attention_weightsv2[0], sentence, sentence, figsize  =(30, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_attention_v2_tunning(hp):\n",
    "\n",
    "    hp_unit_lstm = hp.Int('unit_lstm', min_value=32, max_value=512, step=32)\n",
    "    hp_unit_dropout_1 = hp.Choice('unit_dropout_1', values=[ 0.1, 0.3, 0.5])\n",
    "    hp_unit_dropout_2 = hp.Choice('unit_dropout_2', values=[ 0.1, 0.3, 0.5])\n",
    "    hp_unit_dense = hp.Int('unit_dense', min_value=32, max_value=512, step=32)\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[0.001, 0.0005, 0.0001])\n",
    "    hp_emb_dim = hp.Choice('emb_dim', values=[ 100, 200,300])\n",
    "\n",
    "\n",
    "    ft = fasttext.load_model('../cc.en.300.bin')\n",
    "    fasttext.util.reduce_model(ft, hp_emb_dim)\n",
    "    embeddings_matrix_tmp=get_embeddings_fb(ft,word_index,hp_emb_dim,vocab_len)\n",
    "\n",
    "\n",
    "    model = Sentiment_Attention_v2(hp_unit_lstm, hp_unit_dropout_1, hp_unit_dropout_2, hp_unit_dense, vocab_len, hp_emb_dim, embeddings_matrix_tmp, classN, sequence_size, causal_mask_enabled=True)\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), optimizer=Adam(learning_rate=hp_learning_rate), metrics =[tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.AUC()])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "tuner_att_v2 = kt.BayesianOptimization(\n",
    "        get_attention_v2_tunning,\n",
    "        objective=kt.Objective('val_auc', direction=\"max\"),#'val_binary_accuracy',\n",
    "        max_trials = max_trials,\n",
    "        directory=r\"Hyperparam_att_v2_tunning\",\n",
    "        project_name='Hyperparam_att_v2_tunning'\n",
    "    )\n",
    "\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_auc', patience=30)\n",
    "\n",
    "if TUNNING_ATTENTION_V2:\n",
    "    tuner_att_v2.search(dataset,  epochs=100, batch_size=batchsize, validation_data = val_dataset, verbose =2, callbacks=[stop_early]) \n",
    "\n",
    "\n",
    "best_hps_att_v2=tuner_att_v2.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_hps_att_v2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = fasttext.load_model('../cc.en.300.bin')\n",
    "fasttext.util.reduce_model(ft, best_hps_att_v2[\"emb_dim\"])\n",
    "embeddings_matrix_tmp=get_embeddings_fb(ft,word_index,best_hps_att_v2[\"emb_dim\"],vocab_len)\n",
    "\n",
    "sentiment_MODEL_V2 = Sentiment_Attention_v2(best_hps_att_v2[\"unit_lstm\"], best_hps_att_v2[\"unit_dropout_1\"], best_hps_att_v2[\"unit_dropout_2\"], best_hps_att_v2[\"unit_dense\"], vocab_len, best_hps_att_v2[\"emb_dim\"], embeddings_matrix_tmp, classN, sequence_size, causal_mask_enabled=True)\n",
    "sentiment_MODEL_V2.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), optimizer=Adam(learning_rate=best_hps_att_v2[\"learning_rate\"]), metrics =[tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.AUC()])#Adam(learning_rate=best_hps.get('learning_rate'))\n",
    "\n",
    "i, o = x[0][0:2], y[0:2] \n",
    "i = i[:,0:100]\n",
    "\n",
    "mask_size = np.count_nonzero((i != 0))\n",
    "print(i.shape, o.shape, mask_size)\n",
    "pred = sentiment_MODEL_V2(i)\n",
    "pred, states, attention_weights, lstm_out, context_vector  = sentiment_MODEL_V2.predict_sentiment(i,input_mask = (i != 0))\n",
    "sentiment_MODEL_V2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps_att_v2[\"unit_lstm\"], best_hps_att_v2[\"unit_dropout_1\"], best_hps_att_v2[\"unit_dropout_2\"], best_hps_att_v2[\"unit_dense\"], vocab_len, best_hps_att_v2[\"emb_dim\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FIT_ATTENTION_MODEL:\n",
    "    history_attention = sentiment_MODEL_V2.fit( \n",
    "                dataset,\n",
    "                batch_size = batchsize, \n",
    "                epochs= epochs*2,#epochs,#400\n",
    "                shuffle=True,\n",
    "                callbacks=[PlotLossesCallback(), stop_early], \n",
    "                validation_data=val_dataset, \n",
    "                validation_steps=10)\n",
    "\n",
    "\n",
    "    sentiment_MODEL_V2.save_weights(\"./checkpoints_att/sentiment_attention_model_ckpt\")\n",
    "\n",
    "    with open('./checkpoints_att/history_attention.pickle', 'wb') as handle:\n",
    "        pickle.dump(history_attention.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"Loading saved data ...\")\n",
    "\n",
    "    sentiment_MODEL_V2.load_weights(\"./checkpoints_att/sentiment_attention_model_ckpt\")\n",
    "\n",
    "    with open('./checkpoints_att/history_attention.pickle', 'rb') as handle:\n",
    "        history_attention = pickle.load(handle)\n",
    "        plt.figure()\n",
    "        plt.plot(history_attention['binary_accuracy'])\n",
    "        plt.plot(history_attention['val_binary_accuracy'])\n",
    "        plt.title('Model accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(history_attention['loss'])\n",
    "        plt.plot(history_attention['val_loss'])\n",
    "        plt.title('Model training loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAKE_PREDICTION:\n",
    "    y_hat_list = []\n",
    "    counter= 0\n",
    "    for i, o in test_dataset.take(len(x_test[0])):\n",
    "        if counter%1000 ==0 :\n",
    "            print(counter)\n",
    "        yhat, last_state , attention_weights, last_activation, last_attention = sentiment_MODEL_V2.predict_sentiment(i)\n",
    "        y_hat_list.append(int(yhat[0].numpy()[0] >=0.5))\n",
    "        counter +=1\n",
    "        \n",
    "    y_test = y_test.reshape((len(y_test),1))\n",
    "    yhat = np.array(y_hat_list).reshape((len(y_test),1))\n",
    "\n",
    "    pr = tf.keras.metrics.AUC(curve=\"PR\")\n",
    "\n",
    "    p = tf.keras.metrics.Precision()\n",
    "    p.update_state(y_test,yhat)\n",
    "\n",
    "    r = tf.keras.metrics.Recall()\n",
    "    r.update_state(y_test,yhat)\n",
    "\n",
    "    print(\"With Attention : AUC\",pr(y_test, yhat).numpy(),\" | Precision : \",p.result().numpy(), \" | Recall : \",r.result().numpy())\n",
    "    print(\"Without Attention : AUC \",base_pr,\"  | Precision :  \",base_p,\"  | Recall :  \",base_r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAKE_PREDICTION:\n",
    "    sentence = ' '.join([tokenizer.index_word.get(ind.numpy()) for ind in i[0] if tokenizer.index_word.get(ind.numpy()) is not None])\n",
    "    plot_attention(attention_weightsv2[0], sentence, sentence, figsize  =(30, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_attention_tunning(hp):\n",
    "\n",
    "    hp_unit_lstm = hp.Int('lstm_unit', min_value=32, max_value=512, step=32)\n",
    "    hp_unit_dropout_0 = hp.Choice('dropout_0', values=[ 0.1, 0.3,  0.5])\n",
    "    hp_unit_dense = hp.Int('unit_dense', min_value=32, max_value=512, step=32)\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[0.001, 0.0005, 0.0001])\n",
    "    hp_emb_dim = hp.Choice('emb_dim', values=[ 100, 200,300])\n",
    "\n",
    "\n",
    "    ft = fasttext.load_model('../cc.en.300.bin')\n",
    "    fasttext.util.reduce_model(ft, hp_emb_dim)\n",
    "    embeddings_matrix_tmp=get_embeddings_fb(ft,word_index,hp_emb_dim,vocab_len)\n",
    "\n",
    "\n",
    "    model = ModelClass(hp_unit_lstm, hp_unit_dropout_0,  hp_unit_dense, vocab_len, hp_emb_dim, embeddings_matrix_tmp, classN, sequence_size, causal_mask_enabled=True)\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), optimizer=Adam(learning_rate=hp_learning_rate), metrics =[tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.AUC()])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "tuner_att = kt.BayesianOptimization(\n",
    "        get_attention_tunning,\n",
    "        objective=kt.Objective('val_auc', direction=\"max\"),#'val_binary_accuracy',\n",
    "        max_trials = max_trials,\n",
    "        directory=r\"Hyperparam_att_tunning\",\n",
    "        project_name='keras_tunning_att'\n",
    "    )\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_auc', patience=30)\n",
    "\n",
    "if TUNNING_ATTENTION:\n",
    "    tuner_att.search(dataset,  epochs=100, batch_size=batchsize, validation_data = val_dataset, verbose =0, callbacks=[stop_early]) #epochs\n",
    "\n",
    "\n",
    "best_hps_att=tuner_att.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_hps_att\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = fasttext.load_model('../cc.en.300.bin')\n",
    "fasttext.util.reduce_model(ft, best_hps_att[\"emb_dim\"])\n",
    "embeddings_matrix_tmp=get_embeddings_fb(ft,word_index,best_hps_att[\"emb_dim\"],vocab_len)\n",
    "\n",
    "sentiment_model = ModelClass(best_hps_att[\"lstm_unit\"], best_hps_att[\"dropout_0\"],best_hps_att[\"unit_dense\"], vocab_len, best_hps_att[\"emb_dim\"], embeddings_matrix_tmp, classN, sequence_size, causal_mask_enabled=False )\n",
    "sentiment_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), optimizer=Adam(learning_rate=best_hps_att[\"learning_rate\"]), metrics =[tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.AUC()])\n",
    "\n",
    "# if FIT_ATTENTION_MODEL:\n",
    "#     sentiment_model.use_tf_function = False\n",
    "#     for i, o in dataset.take(1):\n",
    "#         print(sentiment_model.train_step([i, o]))\n",
    "#         print(sentiment_model.test_step([i, o]))\n",
    "#     print()\n",
    "\n",
    "#     sentiment_model.use_tf_function = True\n",
    "#     for i, o in dataset.take(2):\n",
    "#         print(sentiment_model.train_step([i, o]))\n",
    "#         print(sentiment_model.test_step([i, o]))\n",
    "#     print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FIT_ATTENTION_MODEL:\n",
    "    history = sentiment_model.fit( \n",
    "                dataset,\n",
    "                batch_size = batchsize, \n",
    "                epochs=400,#epochs,#300,\n",
    "                shuffle=True,\n",
    "                callbacks=[PlotLossesCallback(), stop_early], \n",
    "                validation_data=val_dataset, \n",
    "                validation_steps=10)\n",
    "\n",
    "    sentiment_model.save_weights(\"./checkpoints_sent_att/sentiment_model_ckpt\")\n",
    "\n",
    "    with open('./checkpoints_sent_att/history.pickle', 'wb') as handle:\n",
    "        pickle.dump(history.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    print(\"Loading saved data ...\")\n",
    "\n",
    "    sentiment_model.load_weights(\"./checkpoints_sent_att/sentiment_model_ckpt\")\n",
    "\n",
    "    with open('./checkpoints_sent_att/history.pickle', 'rb') as handle:\n",
    "        history = pickle.load(handle)\n",
    "        plt.figure()\n",
    "        plt.plot(history['binary_accuracy'])\n",
    "        plt.plot(history['val_binary_accuracy'])\n",
    "        plt.title('Model accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(history['loss'])\n",
    "        plt.plot(history['val_loss'])\n",
    "        plt.title('Model training loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAKE_PREDICTION:\n",
    "    yhat, last_state , attention_weights, last_activation, last_attention =sentiment_model(x_test[0][0:100])\n",
    "    yhat = yhat.numpy()\n",
    "    yhat[yhat >=0.5] = 1\n",
    "    yhat[yhat < 0.5] = 0 \n",
    "    yhat\n",
    "\n",
    "    y_test = y_test.reshape((len(y_test),1))\n",
    "    y_test\n",
    "    print(\"x 1º line : \"+str(x))\n",
    "    print(\"y : \",y) \n",
    "    print(\"y_test : \",y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAKE_PREDICTION:\n",
    "    y_hat_list = []\n",
    "    counter= 0\n",
    "    for i, o in test_dataset.take(len(x_test[0])):\n",
    "        if counter%1000 ==0 :\n",
    "            print(counter)\n",
    "        yhat, last_state , attention_weights, last_activation, last_attention = sentiment_model(i)\n",
    "        y_hat_list.append(int(yhat[0].numpy()[0] >=0.5))\n",
    "        counter +=1\n",
    "\n",
    "    y_test = y_test.reshape((len(y_test),1))\n",
    "    yhat = np.array(y_hat_list).reshape((len(y_test),1))\n",
    "\n",
    "\n",
    "    pr = tf.keras.metrics.AUC(curve=\"PR\")\n",
    "\n",
    "    p = tf.keras.metrics.Precision()\n",
    "    p.update_state(y_test,yhat)\n",
    "\n",
    "    r = tf.keras.metrics.Recall()\n",
    "    r.update_state(y_test,yhat)\n",
    "\n",
    "    pr_att= pr(y_test, yhat).numpy()\n",
    "    p_att =p.result().numpy()\n",
    "    r_att = r.result().numpy()\n",
    "    print(\"With Attention : AUC\",pr_att,\" | Precision : \",p_att, \" | Recall : \",r_att)\n",
    "    print(\"Without Attention : AUC \",base_pr,\"  | Precision :  \",base_p,\"  | Recall :  \",base_r)\n",
    "\n",
    "    tn_att, fp_att, fn_att, tp_att = confusion_matrix(list(y_test),list(yhat)).ravel()\n",
    "    print(f\"With Attention : True Negative = {tn_att}, False Positive : {fp_att}, False Negative : {fn_att}, True Positive : {tp_att}\")\n",
    "    print(f\"Without Attention : True Negative = {base_tn}, False Positive : {base_fp}, False Negative : {base_fn}, True Positive : {base_tp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAKE_PREDICTION:\n",
    "    sentence = ' '.join([tokenizer.index_word.get(ind.numpy()) for ind in i[0] if tokenizer.index_word.get(ind.numpy()) is not None])\n",
    "    plot_attention(attention_weightsv2[0], sentence, sentence, figsize  =(30, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAKE_PREDICTION: print_predictions(x_test[0], yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment_model = tuner_att.get_best_models(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# yhat, last_state , attention_weights, last_activation, last_attention = sentiment_model.predict(x_test[0][0:100])\n",
    "# aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_models = tuner_att.get_best_models(40)\n",
    "# list_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for it, sentiment_model in enumerate(list_models):\n",
    "#     print(\"it --->\",it)\n",
    "#     y_hat_list = []\n",
    "#     counter= 0\n",
    "#     for i, o in test_dataset.take(len(x_test[0])):\n",
    "      \n",
    "#         yhat, last_state , attention_weights, last_activation, last_attention = sentiment_model(i)\n",
    "#         y_hat_list.append(int(yhat[0].numpy()[0] >=0.5))\n",
    "#         counter +=1\n",
    "\n",
    "#     y_test = y_test.reshape((len(y_test),1))\n",
    "#     yhat = np.array(y_hat_list).reshape((len(y_test),1))\n",
    "\n",
    "\n",
    "#     pr = tf.keras.metrics.AUC(curve=\"PR\")\n",
    "\n",
    "#     p = tf.keras.metrics.Precision()\n",
    "#     p.update_state(y_test,yhat)\n",
    "\n",
    "#     r = tf.keras.metrics.Recall()\n",
    "#     r.update_state(y_test,yhat)\n",
    "\n",
    "#     pr_att= pr(y_test, yhat).numpy()\n",
    "#     p_att =p.result().numpy()\n",
    "#     r_att = r.result().numpy()\n",
    "#     print(\"With Attention : AUC\",pr_att,\" | Precision : \",p_att, \" | Recall : \",r_att)\n",
    "#     print(\"Without Attention : AUC \",base_pr,\"  | Precision :  \",base_p,\"  | Recall :  \",base_r)\n",
    "\n",
    "#     tn_att, fp_att, fn_att, tp_att = confusion_matrix(list(y_test),list(yhat)).ravel()\n",
    "#     print(f\"With Attention : True Negative = {tn_att}, False Positive : {fp_att}, False Negative : {fn_att}, True Positive : {tp_att}\")\n",
    "#     print(f\"Without Attention : True Negative = {base_tn}, False Positive : {base_fp}, False Negative : {base_fn}, True Positive : {base_tp}\")\n",
    "\n",
    "#     if p_att> base_p and r_att > base_r:\n",
    "#         print(\"THIS ONE ----->\", it)\n",
    "    \n",
    "#     print(\"----------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aaaaaaaaaaaaaaaaaaaaaaaaaaaa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64#100\n",
    "emb_dim = 100\n",
    "num_heads= 2\n",
    "dropout_rate= 0.2\n",
    "num_layers= 1\n",
    "dff= 64\n",
    "lr = 0.0001\n",
    "\n",
    "ft = fasttext.load_model('../cc.en.300.bin')\n",
    "fasttext.util.reduce_model(ft, emb_dim)\n",
    "embeddings_matrix=get_embeddings_fb(ft,word_index,emb_dim,vocab_len)\n",
    "\n",
    "# x, y = create_trainingdev_set(sequences,df_filtered,sequence_size, False)\n",
    "# x_dev,y_dev,x_test,y_test = create_trainingdev_set(sequences_test,df_test_filtered,sequence_size,True)\n",
    "\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((x[0],y.reshape((len(y),1))))\n",
    "# val_dataset = tf.data.Dataset.from_tensor_slices((x_dev[0],y_dev.reshape((len(y_dev),1))))\n",
    "\n",
    "# for i,o in dataset.take(3):\n",
    "#     print(\"Input : \",i.numpy(), \" ->\",i.numpy().shape)\n",
    "#     print(\"Output : \",o.numpy(), \" ->\",o.numpy().shape,\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "# dataset = dataset.shuffle(buffer_size=10000).batch(batch_size, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "# val_dataset = val_dataset.shuffle(buffer_size=10000).batch(batch_size, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "print(\"Batched dataset ... \\n\")\n",
    "for i,o in dataset.take(1):\n",
    "    print(\"Input : \",i)\n",
    "    print(\"Output : \",o,\"\\n\\n\")\n",
    "\n",
    "ds_len = len(list(dataset))\n",
    "val_size = len(list(val_dataset)) # int(0.07 * ds_len)\n",
    "\n",
    "train_size = len(list(dataset))\n",
    "\n",
    "print(ds_len,\"-->\",train_size,\"--->\",val_size,\"/\",ds_len*batch_size)\n",
    "ds_len = train_size\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test[0],y_test.reshape((len(y_test),1))))\n",
    "test_dataset = test_dataset.batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, padding_mask= None, look_ahead_mask= None):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead)\n",
    "  but it must be broadcastable for addition.\n",
    "\n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable\n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "  if padding_mask is not None:\n",
    "    scaled_attention_logits += (padding_mask * -1e9)\n",
    "\n",
    "  if look_ahead_mask is not None:\n",
    "    scaled_attention_logits += (look_ahead_mask * -1e9)\n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights\n",
    "\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self,*, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.wq = Dense(d_model)\n",
    "    self.wk = Dense(d_model)\n",
    "    self.wv = Dense(d_model)\n",
    "\n",
    "    self.dense = Dense(d_model)\n",
    "\n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, v, k, q, padding_mask, look_ahead_mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "\n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention( q, k, v, padding_mask, look_ahead_mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention,(batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff): #Point wise feed forward network consists of two fully-connected layers with a ReLU activation in between.\n",
    "  return tf.keras.Sequential([\n",
    "      Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])\n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self,*, d_model, num_heads, dff, rate=0.1,epsilon=1e-6):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = LayerNormalization(epsilon=epsilon)\n",
    "    self.layernorm2 = LayerNormalization(epsilon=epsilon)\n",
    "\n",
    "    self.dropout1 = Dropout(rate)\n",
    "    self.dropout2 = Dropout(rate)\n",
    "\n",
    "  def call(self, x, training, padding_mask= None, look_ahead_mask = None):\n",
    "    attn_output, attn_weights_block1  = self.mha(x, x, x, padding_mask, look_ahead_mask)  # (batch_size, input_seq_len, d_model)\n",
    "    attn_output = self.dropout1(attn_output, training=training)\n",
    "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    return out2, attn_weights_block1\n",
    "\n",
    "\n",
    "sample_encoder_layer = DecoderLayer(d_model=80, num_heads=8, dff=batch_size)\n",
    "sample_encoder_layer_output,attn_weights_block1 = sample_encoder_layer(tf.random.uniform((batch_size, 43, 80)), False, None)\n",
    "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "  pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "n, d = 2048, vocab_len\n",
    "pos_encoding = positional_encoding(n, d)\n",
    "print(pos_encoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self,*, num_layers, d_model, embeddings_matrix, window_size, num_heads, dff, vocab_len, rate=0.1, epsilon=1e-6):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.emb_layer = Embedding(input_dim=vocab_len, input_length = window_size, output_dim=d_model, weights=[embeddings_matrix] ,name =\"emb_layer2\", trainable=False)#, mask_zero = True\n",
    "    self.pos_encoding = positional_encoding(window_size, self.d_model)\n",
    "\n",
    "    self.dec_layers = [DecoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate, epsilon=epsilon) for _ in range(num_layers)]\n",
    "\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    self.final_layer = Dense(1, activation=\"sigmoid\")\n",
    "    \n",
    "  def call(self, x, training, padding_mask =None, look_ahead_mask= None):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    attention_weights = {}\n",
    "\n",
    "    x = self.emb_layer(x)  # (batch_size, input_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "    x = self.dropout(x, training=training)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x, block1 = self.dec_layers[i](x, training, padding_mask, look_ahead_mask)\n",
    "      attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "\n",
    "    x = self.final_layer(x)  # (batch_size, tar_seq_len, 1)\n",
    "    return x ,attention_weights # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "sample_decoder = Decoder(num_layers=2, d_model=emb_dim, embeddings_matrix = embeddings_matrix, window_size= sequence_size, num_heads=10,\n",
    "                         dff=2048, vocab_len=vocab_len)\n",
    "\n",
    "for i, o in dataset.take(1):\n",
    "  print(i.shape, o.shape)\n",
    "  output, attn = sample_decoder(i, training=True, padding_mask =None, look_ahead_mask= None)\n",
    "  \n",
    "output.shape, attn['decoder_layer2_block1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "\n",
    "learning_rate = CustomSchedule(emb_dim)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,epsilon=1e-9)\n",
    "\n",
    "temp_learning_rate_schedule = CustomSchedule(100)\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.xlabel('Train Step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,*, num_layers, d_model,embeddings_matrix,window_size, num_heads, dff,  vocab_len, rate=0.1, use_tf_function= True, epsilon=1e-6):\n",
    "        super().__init__()\n",
    "\n",
    "        self.decoder = Decoder(num_layers=num_layers, d_model=d_model, embeddings_matrix = embeddings_matrix,\n",
    "                            window_size= window_size, num_heads=num_heads, dff=dff, vocab_len=vocab_len, rate=rate, epsilon=epsilon)\n",
    "\n",
    "        \n",
    "        self.use_tf_function = use_tf_function\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        # Keras models prefer if you pass all your inputs in the first argument\n",
    "        inp = inputs\n",
    "\n",
    "        padding_mask, look_ahead_mask = self.create_masks(inp)\n",
    "        \n",
    "        # dec_output.shape == (batch_size, inp_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(inp, training, padding_mask, look_ahead_mask)\n",
    "\n",
    "        return dec_output\n",
    "\n",
    "    def predict_on(self, inputs, training=False):\n",
    "        # Keras models prefer if you pass all your inputs in the first argument\n",
    "        inp = inputs\n",
    "\n",
    "        padding_mask, look_ahead_mask = self.create_masks(inp)\n",
    "        \n",
    "        # dec_output.shape == (batch_size, inp_seq_len, d_model)\n",
    "        look_ahead_mask = None\n",
    "        dec_output, attention_weights = self.decoder(inp, training, padding_mask, look_ahead_mask)\n",
    "\n",
    "        return dec_output, attention_weights\n",
    "\n",
    "    def create_masks(self, inp):\n",
    "        # Encoder padding mask (Used in the 2nd attention block in the decoder too.)\n",
    "        \n",
    "        padding_mask = self.create_padding_mask(inp)\n",
    "        look_ahead_mask = self.create_look_ahead_mask(tf.shape(inp)[1])\n",
    "\n",
    "        return padding_mask, look_ahead_mask\n",
    "\n",
    "    def create_padding_mask(self,seq):\n",
    "        seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "        return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "    def create_look_ahead_mask(self, size):\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "        return mask  # (seq_len, seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_transformer_tunning(hp):\n",
    "\n",
    "    hp_dff = hp.Int('dff', min_value=32, max_value=128, step=32)\n",
    "    hp_unit_dropout_rate = hp.Choice('dropout_rate', values=[ 0.1, 0.2, 0.3, 0.5])\n",
    "    hp_num_heads = hp.Choice('num_heads', values=[2, 5])\n",
    "    hp_num_layers= hp.Int('num_layers', min_value=4, max_value=6, step=1)\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[0.001,  0.0001])\n",
    "    hp_emb_dim = hp.Choice('emb_dim', values=[ 100, 150])\n",
    "\n",
    "\n",
    "    ft = fasttext.load_model('../cc.en.300.bin')\n",
    "    fasttext.util.reduce_model(ft, hp_emb_dim)\n",
    "    embeddings_matrix=get_embeddings_fb(ft,word_index,hp_emb_dim,vocab_len)\n",
    "\n",
    "\n",
    "    transformer = Transformer(\n",
    "        num_layers=hp_num_layers,\n",
    "        d_model=hp_emb_dim,\n",
    "        embeddings_matrix = embeddings_matrix,\n",
    "        window_size = sequence_size,\n",
    "        num_heads=hp_num_heads,\n",
    "        dff=hp_dff,\n",
    "        vocab_len=vocab_len,\n",
    "        rate=hp_unit_dropout_rate)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(hp_learning_rate, beta_1=0.9, beta_2=0.98,epsilon=1e-9)\n",
    "    transformer.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), optimizer=optimizer, metrics=[ tf.metrics.BinaryAccuracy(),tf.keras.metrics.AUC()])#,\"accuracy\"  tf.keras.losses.BinaryCrossentropy(from_logits=False), tf.metrics.BinaryAccuracy()\n",
    "\n",
    "    \n",
    "    return transformer\n",
    "\n",
    "tuner_transformer = kt.BayesianOptimization(\n",
    "        get_transformer_tunning,\n",
    "        objective=kt.Objective('val_auc', direction=\"max\"),#'val_binary_accuracy',\n",
    "        max_trials = max_trials,\n",
    "        directory=r\"Hyperparam_transformer_tunning\",\n",
    "        project_name='Hyperparam_transformer_tunning',\n",
    "    )\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_auc', patience=30)\n",
    "\n",
    "if TUNNING_TRANSFORMER:\n",
    "    \n",
    "    tuner_transformer.search(dataset,  epochs=100, batch_size=batchsize, shuffle= True,validation_data = val_dataset, verbose =2, callbacks=[stop_early]) #epochs\n",
    "\n",
    "\n",
    "best_hps_transformer=tuner_transformer.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_hps_transformer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps_transformer[\"learning_rate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = fasttext.load_model('../cc.en.300.bin')\n",
    "fasttext.util.reduce_model(ft, best_hps_transformer[\"emb_dim\"])\n",
    "embeddings_matrix=get_embeddings_fb(ft,word_index,best_hps_transformer[\"emb_dim\"])\n",
    "\n",
    "transformer = Transformer(\n",
    "    num_layers=best_hps_transformer[\"num_layers\"],\n",
    "    d_model=best_hps_transformer[\"emb_dim\"],\n",
    "    embeddings_matrix = embeddings_matrix,\n",
    "    window_size = sequence_size,\n",
    "    num_heads=best_hps_transformer[\"num_heads\"],\n",
    "    dff=best_hps_transformer[\"dff\"],\n",
    "    vocab_len=vocab_len,\n",
    "    rate=best_hps_transformer[\"dropout_rate\"])\n",
    "\n",
    "for i, o in dataset.take(1):\n",
    "    print(i.shape, o.shape)\n",
    "    fn_out = transformer(i, training=False)\n",
    "    print\n",
    "fn_out.shape  \n",
    "\n",
    "#transformer.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), optimizer=optimizer, metrics=[tf.metrics.BinaryAccuracy()])\n",
    "transformer.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), optimizer=optimizer, metrics=[tf.metrics.BinaryAccuracy(),tf.keras.metrics.AUC()])#,\"accuracy\"  tf.keras.losses.BinaryCrossentropy(from_logits=False), tf.metrics.BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FIT_TRANSFORMER_MODEL:\n",
    "    history_transformer = transformer.fit(dataset, epochs=epochs, shuffle= True,batch_size = batchsize,#6000\n",
    "                            steps_per_epoch = train_size, callbacks=[PlotLossesCallback(),stop_early], validation_data=val_dataset)#, validation_steps=10\n",
    "    transformer.save_weights(\"./checkpoints_transformer/transformer_ckpt\")\n",
    "\n",
    "    with open('./checkpoints_transformer/history_transformer.pickle', 'wb') as handle:\n",
    "        pickle.dump(history_transformer.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    print(\"Loading saved data ...\")\n",
    "    transformer = Transformer( num_layers=num_layers, d_model=emb_dim, embeddings_matrix = embeddings_matrix, window_size = sequence_size, num_heads=num_heads, dff=dff, vocab_len=vocab_len, rate=dropout_rate)\n",
    "    transformer.load_weights(\"./checkpoints_transformer/transformer_ckpt\")\n",
    "\n",
    "    with open('./checkpoints_transformer/history_transformer.pickle', 'rb') as handle:\n",
    "        history_transformer = pickle.load(handle)\n",
    "        plt.figure()\n",
    "        plt.plot(history_transformer['binary_accuracy'])\n",
    "        plt.plot(history_transformer['val_binary_accuracy'])\n",
    "        plt.title('Model accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(history_transformer['loss'])\n",
    "        plt.plot(history_transformer['val_loss'])\n",
    "        plt.title('Model training loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_hat_list = []\n",
    "counter= 0\n",
    "yhat=None\n",
    "for i, o in test_dataset.take(len(x_test[0])):\n",
    "    if counter%1000 ==0 :\n",
    "        print(counter)\n",
    "    yhat, attention_weights = transformer.predict_on(i)\n",
    "    y_hat_list.append(int(yhat[0].numpy()[0] >=0.5))\n",
    "    counter +=1\n",
    "\n",
    "y_test = y_test.reshape((len(y_test),1))\n",
    "yhat = np.array(y_hat_list).reshape((len(y_test),1))\n",
    "\n",
    "pr = tf.keras.metrics.AUC(curve=\"PR\")\n",
    "\n",
    "p = tf.keras.metrics.Precision()\n",
    "p.update_state(y_test,yhat)\n",
    "\n",
    "r = tf.keras.metrics.Recall()\n",
    "r.update_state(y_test,yhat)\n",
    "\n",
    "print(\"With Transformers : AUC\",pr(y_test, yhat).numpy(),\" | Precision : \",p.result().numpy(), \" | Recall : \",r.result().numpy())\n",
    "print(\"With Attention : AUC\",pr_att,\" | Precision : \",p_att, \" | Recall : \",r_att)\n",
    "print(\"Without Attention : AUC \",base_pr,\"  | Precision :  \",base_p,\"  | Recall :  \",base_r)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(list(y_test),list(yhat)).ravel()\n",
    "print(f\"With Transformer : True Negative = {tn}, False Positive : {fp}, False Negative : {fn}, True Positive : {tp}\")\n",
    "print(f\"With Attention : True Negative = {tn_att}, False Positive : {fp_att}, False Negative : {fn_att}, True Positive : {tp_att}\")\n",
    "print(f\"Without Attention : True Negative = {base_tn}, False Positive : {base_fp}, False Negative : {base_fn}, True Positive : {base_tp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SentimentClassification.ipynb",
   "provenance": [
    {
     "file_id": "1DUXYgOxu2qAOP0B7VnjYFK0z4wXoDMsa",
     "timestamp": 1560792666082
    },
    {
     "file_id": "1MBv0UvhVnj2zvRWBQLik9hC9Fgm-pyR-",
     "timestamp": 1560067092399
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "dffee67a034c262da22ecd866efe5fa1174a6876e1e96ed512a9acb5833498b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
