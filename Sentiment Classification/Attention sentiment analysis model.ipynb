{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11163,
     "status": "ok",
     "timestamp": 1561582739246,
     "user": {
      "displayName": "yaniel barbosa",
      "photoUrl": "https://lh3.googleusercontent.com/-JlWhURV1zuQ/AAAAAAAAAAI/AAAAAAAAADQ/hOOCIumFYTE/s64/photo.jpg",
      "userId": "13057870575473273651"
     },
     "user_tz": -60
    },
    "id": "NvL8Lqk_DX60",
    "outputId": "461236cc-495f-4e04-fbb1-af7f2aa38c58"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Dense,  Dropout, Input, LSTM, Embedding,SpatialDropout1D, BatchNormalization, Flatten, LayerNormalization\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow_text as text\n",
    "\n",
    "import pandas_tfrecords as pdtfr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import keras_tuner as kt\n",
    "import seaborn as sns\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "np.random.seed(0)\n",
    "stop_words_list = stopwords.words('english') \n",
    "\n",
    "#tf.config.run_functions_eagerly(True)\n",
    "#import tensorflow_data_validation as tfdv\n",
    "#tf.debugging.set_log_device_placement(True)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras_nlp\n",
    "from transformers import  RobertaTokenizer, TFRobertaModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, info = tfds.load('imdb_reviews', with_info=True, \n",
    "                          as_supervised=True)\n",
    "\n",
    "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, o in train_dataset.take(1):\n",
    "    print(i.shape, o.shape)\n",
    "    print(i, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tfdataframe_to_pddataframe(tf_df):\n",
    "    data_list = []\n",
    "    for i, (features, label) in enumerate(tf_df):\n",
    "        \n",
    "        if i % 5000 == 0 : print(f\"{i}/{len(tf_df)}\")\n",
    "        \n",
    "        data_list.append((features.numpy().decode('utf-8'), label.numpy()))\n",
    "\n",
    "    # Create a Pandas DataFrame\n",
    "    return pd.DataFrame(data_list, columns = [ \"review\", \"sentiment\"])\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "train_dataset = convert_tfdataframe_to_pddataframe(train_dataset)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pandas DataFrame\n",
    "test_dataset = convert_tfdataframe_to_pddataframe(test_dataset)\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate a StatsOptions class and define the feature_allowlist property\n",
    "# stats_options = tfdv.StatsOptions(\n",
    "#     feature_allowlist=train_dataset.columns.tolist(),\n",
    "#     enable_semantic_domain_stats= True # Because we are processing text\n",
    "#     )\n",
    "# stats_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_stats = tfdv.generate_statistics_from_dataframe(train_dataset, stats_options) \n",
    "\n",
    "# # get the number of features used to compute statistics\n",
    "# print(f\"Number of features used: {len(train_stats.datasets[0].features)}\")\n",
    "\n",
    "# # check the number of examples used\n",
    "# print(f\"Number of examples used: {train_stats.datasets[0].num_examples}\")\n",
    "\n",
    "# # check the column names of the first and last feature\n",
    "# print(f\"First feature: {train_stats.datasets[0].features[0].path.step[0]}\")\n",
    "# print(f\"Last feature: {train_stats.datasets[0].features[-1].path.step[0]}\")\n",
    "# tfdv.visualize_statistics(train_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "\n",
    "ETL_STOPWORDS= False\n",
    "PREPARE_TRAINIG_DATA= False\n",
    "TRAIN_MODE= True\n",
    "\n",
    "TUNNING = True\n",
    "INFERENCE_MODE = True\n",
    "\n",
    "threshold = 1\n",
    "classN=1\n",
    "min_freq = 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_CHECKPOINTS_DIR = './checkpoints'\n",
    "tfrecord_filename = \"train_tmp.tfrecord\"\n",
    "train_tmp_record_path = f'{DATA_CHECKPOINTS_DIR}/{tfrecord_filename}'\n",
    "\n",
    "!mkdir -p {DATA_CHECKPOINTS_DIR}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Functions to perform data cleaning and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_word_frequency(train_df):\n",
    "    \n",
    "    print(\"Dataset lentgh : \",len(train_df))\n",
    "\n",
    "    tokenizer = Tokenizer(filters='!\"#$%&()*+,-/:;=?@[\\\\]^_´`«»{|}~\\t\\n\\'',oov_token='<oov>')# sequence_size = 500 ~= mean+ std sentence size\n",
    "    tokenizer.fit_on_texts(train_df.review.values.tolist())\n",
    "\n",
    "    word_frequency = {}\n",
    "    for i, (word, count) in enumerate(tokenizer.word_counts.items()):\n",
    "        word_frequency[word]=count\n",
    "            \n",
    "    return word_frequency\n",
    "\n",
    "\n",
    "def update_stop_words(word_frequency, freq,max_freq,stop_word):\n",
    " \n",
    "    it = 0\n",
    "    for word, count in word_frequency.items():\n",
    "        if count < freq:\n",
    "            if it < 10:\n",
    "                print(word)\n",
    "                it +=1\n",
    "            stop_word.append(word)\n",
    "        elif count >max_freq :\n",
    "            stop_word.append(word)\n",
    "            \n",
    "    return list(set(stop_word))\n",
    "\n",
    "\n",
    "word_frequency= check_word_frequency(train_dataset)\n",
    "max_freq = np.max(list(word_frequency.values())) +1 \n",
    "# Update stop words list with the less frequent \n",
    "updated_stop_word = update_stop_words(word_frequency, min_freq, max_freq, stop_words_list.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(updated_stop_word)/len(word_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def limit_words_tf(review, sentiment):\n",
    "    # Split the string into tokens\n",
    "    tokens = tf.strings.split(review)\n",
    "\n",
    "    # Limit the number of tokens to max_words\n",
    "    limited_tokens = tokens[:sequence_size]\n",
    "    \n",
    "    # Join the tokens back into a string\n",
    "    limited_review = tf.strings.reduce_join(limited_tokens, separator=' ')\n",
    "        \n",
    "    return limited_review, sentiment\n",
    "\n",
    "\n",
    "\n",
    "def save_record(dataset, record_path):\n",
    "\n",
    "    with tf.io.TFRecordWriter(record_path) as writer:\n",
    "        for index, row in dataset.iterrows():\n",
    "            input_ids, attention_mask, sentiment = row[\"input_ids\"], row[\"attention_mask\"], row[\"sentiment\"]\n",
    "            # Serialize example\n",
    "            feature = {\n",
    "                'input_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=input_ids)),  # List feature\n",
    "                'attention_mask': tf.train.Feature(float_list=tf.train.FloatList(value=attention_mask)), # List feature\n",
    "                'sentiment': tf.train.Feature(int64_list=tf.train.Int64List(value=[sentiment])) # Single value feature\n",
    "            }\n",
    "            example_proto = tf.train.Example(features=tf.train.Features(feature=feature)).SerializeToString()\n",
    "            \n",
    "            writer.write(example_proto)\n",
    "\n",
    "def clean_reviews(review, sentiment):\n",
    "    \n",
    "    lowercase = tf.strings.lower(review)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    \n",
    "    tokens = tf.strings.split(stripped_html)\n",
    "    # Filter out words to remove\n",
    "    words_to_remove_tensor = tf.constant(updated_stop_word)\n",
    "    mask = tf.logical_not(tf.reduce_any(tf.equal(tokens, words_to_remove_tensor[:, tf.newaxis]), axis=0))\n",
    "    \n",
    "    # Filter out words to remove\n",
    "    filtered_tokens = tf.boolean_mask(tokens, mask)\n",
    "    \n",
    "    # Join the remaining tokens back into a string\n",
    "    processed_data = tf.strings.reduce_join(filtered_tokens, separator=' ')\n",
    "    \n",
    "    sentence_size = len(tf.strings.split(processed_data))\n",
    "    return (processed_data, sentiment , sentence_size)\n",
    "    \n",
    "def process_stop_words(td_dataset):\n",
    "\n",
    "    # Convert to tf.dataset\n",
    "    td_dataset = tf.data.Dataset.from_tensor_slices((td_dataset['review'], td_dataset['sentiment']))\n",
    "\n",
    "    for i, o in td_dataset.take(1):\n",
    "        print(i.shape, o.shape)\n",
    "        print(\"Pre processed ds : \",i, o )\n",
    "   \n",
    "    \n",
    "    td_dataset = td_dataset.map(clean_reviews, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    for i, o, l in td_dataset.take(3):\n",
    "        print(\"Post processed ds : \",l, o, i )\n",
    "        \n",
    "    data_list_element = []\n",
    "    total_len = len(td_dataset)\n",
    "\n",
    "    print(f\"Starting Extraction in batches {total_len} ...\")\n",
    "    for i, (features, label, size) in enumerate(td_dataset.take(total_len)):\n",
    "        if i % 1000 == 0 : print(f\"{i}/{total_len}\")\n",
    "        \n",
    "        data_list_element.append((features.numpy(), label.numpy(), size.numpy()))\n",
    "    \n",
    "    print(\"Creating dataframe....\")\n",
    "    return pd.DataFrame(data_list_element, columns = [ \"review\", \"sentiment\", \"size\"])\n",
    "\n",
    "if ETL_STOPWORDS:\n",
    "    train_ds = process_stop_words(train_dataset)\n",
    "    train_ds.to_pickle(f\"{DATA_CHECKPOINTS_DIR}/train_ds.pkl\")  \n",
    "    \n",
    "    display(train_ds)\n",
    "    test_ds  = process_stop_words(test_dataset)\n",
    "    test_ds.to_pickle(f\"{DATA_CHECKPOINTS_DIR}/test_ds.pkl\") \n",
    "    display(test_ds) \n",
    "else:\n",
    "    train_ds = pd.read_pickle(f\"{DATA_CHECKPOINTS_DIR}/train_ds.pkl\")  \n",
    "    test_ds = pd.read_pickle(f\"{DATA_CHECKPOINTS_DIR}/test_ds.pkl\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.review = train_ds.review.apply(lambda x : x.decode(\"utf-8\"))\n",
    "test_ds.review = test_ds.review.apply(lambda x : x.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ETL_STOPWORDS:\n",
    "\n",
    "    with open(f'{DATA_CHECKPOINTS_DIR}/word_frequency.pkl', 'wb') as f:\n",
    "        pickle.dump(word_frequency, f)\n",
    "\n",
    "    with open(f'{DATA_CHECKPOINTS_DIR}/updated_stop_word.pkl', 'wb') as f:\n",
    "        pickle.dump(updated_stop_word, f)\n",
    "\n",
    "else:\n",
    "    \n",
    "    with open(f'{DATA_CHECKPOINTS_DIR}/word_frequency.pkl', 'rb') as f:\n",
    "        word_frequency = pickle.load(f)\n",
    "        \n",
    "    with open(f'{DATA_CHECKPOINTS_DIR}/updated_stop_word.pkl', 'rb') as f:\n",
    "        updated_stop_word = pickle.load(f)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check sentence size\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[\"size\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[train_ds[\"size\"] > 800].review.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=test_ds['size'])\n",
    "test_ds[\"size\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the outliers to fix the sentence size . Every sample with size considered as outlier will be trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Q1_train = train_ds[\"size\"].quantile(0.25)\n",
    "Q3_train = train_ds[\"size\"].quantile(0.75)\n",
    "IQR_train = Q3_train - Q1_train\n",
    "print(\"Train Q1_train : \",Q1_train)\n",
    "print(\"Train Q3_train : \",Q3_train)\n",
    "print(\"Train IQR : \",IQR_train)\n",
    "print(\"Final training dataset size : \",(Q1_train - 1.5 * IQR_train),(Q3_train + 1.5 * IQR_train), len(train_ds[ train_ds[\"size\"] <= (Q3_train + 1.5 * IQR_train)]))\n",
    "print(\"Training data to be trimed : \",len(train_ds[ train_ds[\"size\"] > (Q3_train + 1.5 * IQR_train)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_test = test_ds[\"size\"].quantile(0.25)\n",
    "Q3_test = test_ds[\"size\"].quantile(0.75)\n",
    "IQR_test = Q3_test - Q1_test\n",
    "\n",
    "print(\"Test IQR : \",IQR_test)\n",
    "print(\"Final testing dataset size : \",(Q1_test - 1.5 * IQR_test),(Q3_test + 1.5 * IQR_test), len(test_ds[ test_ds[\"size\"] <= (Q3_test + 1.5 * IQR_test)]))\n",
    "print(\"Testing data to be trimed : \",len(test_ds[ test_ds[\"size\"] > (Q3_test + 1.5 * IQR_test)]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the sentence cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IQR_total = np.maximum((Q3_train + 1.5 * IQR_train),(Q3_test + 1.5 * IQR_test))\n",
    "sequence_size = int(IQR_total)\n",
    "sequence_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = test_ds[test_ds['size'] > 0] \n",
    "train_ds = train_ds[train_ds['size'] > 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def limit_words(review):\n",
    "    # Split the string into tokens\n",
    "    tokens = review.split()\n",
    "\n",
    "    # Limit the number of tokens to max_words\n",
    "    limited_tokens = tokens[:sequence_size]\n",
    "    \n",
    "    # Join the tokens back into a string\n",
    "    limited_review = ' '.join(limited_tokens)\n",
    "        \n",
    "    return limited_review\n",
    "\n",
    "print(\"Cleaning training dataset - sentences size processing \")\n",
    "train_ds[\"cleaned_review\"] = train_ds.review.apply(lambda x : limit_words(x)) \n",
    "train_ds[\"cleaned_size\"] = train_ds.cleaned_review.apply(lambda x : len(x.split())) \n",
    "\n",
    "print(\"Cleaning testing dataset - sentences size processing \")\n",
    "test_ds[\"cleaned_review\"] = test_ds.review.apply(lambda x : limit_words(x)) \n",
    "test_ds[\"cleaned_size\"] = test_ds.cleaned_review.apply(lambda x : len(x.split())) \n",
    "\n",
    "num_labels = len(train_ds.sentiment.unique())  # number of labels for your classification task\n",
    "\n",
    "display(train_ds)\n",
    "display(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before thinking on trimming, we must check if any important sentiment information will not be deleted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize and prepare embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "pd.Series(train_ds.size).hist(bins=10)\n",
    "plt.title(\"Sentence Size for training dataset\")\n",
    "\n",
    "plt.figure()\n",
    "pd.Series(train_ds.sentiment).hist()\n",
    "plt.title(\"Sentiment for training dataset\")\n",
    "\n",
    "plt.show()\n",
    "print(pd.Series(train_ds.size).describe())\n",
    "\n",
    "plt.figure()\n",
    "pd.Series(test_ds.size).hist()\n",
    "plt.title(\"Sentence Size for test dataset\")\n",
    "\n",
    "plt.figure()\n",
    "pd.Series(test_ds.sentiment).hist()\n",
    "plt.title(\"Sentiment for test dataset\")\n",
    "\n",
    "plt.show()\n",
    "print(pd.Series(test_ds.size).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds[[\"cleaned_review\",\"sentiment\"]]\n",
    "test_ds = test_ds[[\"cleaned_review\",\"sentiment\"]]\n",
    "train_ds.sentiment = train_ds.sentiment.astype(\"int8\")\n",
    "test_ds.sentiment = test_ds.sentiment.astype(\"int8\")\n",
    "\n",
    "display(train_ds)\n",
    "display(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_ds, val_ds = train_test_split(train_ds,test_size=0.2,train_size=0.8)\n",
    "display(train_ds)\n",
    "display(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the data to the disk to clean memory after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfds = tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            train_ds.cleaned_review.tolist()\n",
    "        )\n",
    "    ).batch(batch_size).shuffle(buffer_size=len(train_ds)).prefetch(tf.data.AUTOTUNE)\n",
    "for x in train_tfds.take(1):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_tfds = tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            val_ds.cleaned_review.tolist(), val_ds.sentiment.tolist()\n",
    "        )\n",
    "    ).batch(batch_size).shuffle(buffer_size=len(val_ds)).prefetch(tf.data.AUTOTUNE)\n",
    "for x in dev_tfds.take(1):\n",
    "    print(x)\n",
    "\n",
    "test_tfds = tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            test_ds.cleaned_review.tolist(), test_ds.sentiment.tolist()\n",
    "        )\n",
    "    ).batch(batch_size).shuffle(buffer_size=len(test_ds)).prefetch(tf.data.AUTOTUNE)\n",
    "for x in test_tfds.take(1):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_frequency.keys())\n",
    "vocab_size, sequence_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train tokenizer vocabulary on training data only. Words that occur only on the test data will be unknown, and this is the expected behaviour when considering real word environment.\n",
    "\n",
    "vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
    "    train_tfds,\n",
    "    vocabulary_size=vocab_size,\n",
    "    lowercase=True,\n",
    "    reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"],\n",
    ")\n",
    "\n",
    "#WordPieceTokenizer is an efficient implementation of the WordPiece algorithm used by BERT and other models. \n",
    "tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=vocab,\n",
    "    sequence_length=sequence_size,\n",
    "    lowercase=True,\n",
    ")\n",
    "\n",
    "train_tfds = tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            train_ds.cleaned_review.tolist(), train_ds.sentiment.tolist()\n",
    "        )\n",
    "    ).batch(batch_size).shuffle(buffer_size=len(train_ds)).prefetch(tf.data.AUTOTUNE)\n",
    "for x in train_tfds.take(1):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packer adds a start token\n",
    "start_packer = keras_nlp.layers.StartEndPacker(\n",
    "    sequence_length=sequence_size,\n",
    "    start_value=tokenizer.token_to_id(\"[BOS]\"),\n",
    "    end_value = tokenizer.token_to_id(\"[EOS]\"),\n",
    ")\n",
    "\n",
    "def preprocess(inputs,output):\n",
    "    tokenized_inputs = tokenizer(inputs)\n",
    "    features = start_packer(tokenized_inputs)\n",
    "    return features, output\n",
    "\n",
    "train_tfds= train_tfds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).prefetch(\n",
    "    tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "dev_tfds= dev_tfds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).prefetch(\n",
    "    tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "\n",
    "test_tfds= test_tfds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).prefetch(\n",
    "    tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "print(\"Training set ...\")\n",
    "for i, o in train_tfds.take(1):\n",
    "    print(\"input : \",i[0])\n",
    "    print(\"output : \",o)\n",
    "    print(\"Detokenized input: \",tokenizer.detokenize(i[0]))\n",
    "\n",
    "\n",
    "print(\"Dev set ...\")\n",
    "for i, o in dev_tfds.take(1):\n",
    "    print(\"input : \",i[0])\n",
    "    print(\"output : \",o)\n",
    "    print(\"Detokenized input: \",tokenizer.detokenize(i[0]))\n",
    "\n",
    "\n",
    "print(\"Testing set ...\")\n",
    "for i, o in test_tfds.take(1):\n",
    "    print(\"input : \",i[0])\n",
    "    print(\"output : \",o)\n",
    "    print(\"Detokenized input: \",tokenizer.detokenize(i[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention model training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training process \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSentimentModel(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_classes = 1,\n",
    "        vocab_len = vocab_size, \n",
    "        window_size = sequence_size, \n",
    "        embedding_dim = 100,  \n",
    "        num_att_layers = 1, \n",
    "        n_attention_head = 2, \n",
    "        feed_forward_dim = [128],\n",
    "        dropout_rate = [0.0],\n",
    "        num_dense_layers = 1,\n",
    "        dense_units_list = [32]):\n",
    "        super(AttentionSentimentModel, self).__init__()\n",
    "        \n",
    "        self.num_att_layers = num_att_layers\n",
    "        self.n_attention_head =n_attention_head\n",
    "        self.feed_forward_dim = feed_forward_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        \n",
    "\n",
    "        self.embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "            vocabulary_size=vocab_len,\n",
    "            sequence_length=window_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            mask_zero=True,\n",
    "        )\n",
    "        \n",
    "        self.decoder_layer = [keras_nlp.layers.TransformerDecoder(num_heads=self.n_attention_head, intermediate_dim=self.feed_forward_dim[i],dropout = dropout_rate[i]) for i in range(self.num_att_layers)]\n",
    "        \n",
    "        self.num_dense_layers = num_dense_layers\n",
    "        self.dense_layers = [Dense(dense_units_list[i], activation='relu') for i in range(self.num_dense_layers)]\n",
    "        self.output_dense = Dense(n_classes, activation='sigmoid')\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.embedding_layer(inputs)\n",
    "        for i in range(self.num_att_layers ):\n",
    "            x = self.decoder_layer[i](x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        for i in range(self.num_dense_layers):\n",
    "            x = self.dense_layers[i](x)\n",
    "\n",
    "        classes  = self.output_dense(x)\n",
    "\n",
    "        return classes\n",
    "\n",
    "# test_sentiment_model =AttentionSentimentModel(n_classes = 1)\n",
    "# test_sentiment_model.compile(optimizer=Adam(learning_rate=0.0001), loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), metrics =[tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.AUC()])\n",
    "# for i, o in train_tfds.take(1):\n",
    "#     test_sentiment_model(i)\n",
    "# test_sentiment_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_sentiment_model.fit(train_tfds,  epochs=5, batch_size=batch_size, validation_data = dev_tfds, verbose =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=f\"logs/attention_tunning/\")\n",
    "\n",
    "def get_model_tunning(hp):\n",
    "\n",
    "    pr = tf.keras.metrics.AUC(curve=\"PR\")\n",
    "    p = tf.keras.metrics.Precision()\n",
    "    r = tf.keras.metrics.Recall()\n",
    "\n",
    "    n_dense_layers = hp.Int('max_n_dense', 1, 2)\n",
    "    dense_units_list =  [hp.Int('dense_units_'+str(i), 32, 128) for i in range(n_dense_layers)]\n",
    "\n",
    "    num_att_layers = hp.Int('num_att_layers', 1, 3)\n",
    "    n_attention_head = hp.Int('n_attention_head', 1, 3)\n",
    "    feed_forward_dim =  [hp.Int('feed_forward_dim_'+str(i), 32, 128) for i in range(num_att_layers)]\n",
    "    dropout_units_list =  [hp.Float('dropout_rate_'+str(i), 0.0, 0.5, step = 0.1) for i in range(num_att_layers)]\n",
    "    \n",
    "    hp_learning_rate = hp.Float('learning_rate', 0.000001, 0.001)\n",
    "\n",
    "\n",
    "    embedding_dim = hp.Int('embedding_dim', 50, 200, step = 50)\n",
    "\n",
    "    model =AttentionSentimentModel(\n",
    "        n_classes = 1, \n",
    "        vocab_len = vocab_size, \n",
    "        window_size = sequence_size, \n",
    "        embedding_dim = embedding_dim,  \n",
    "\n",
    "        num_att_layers = num_att_layers, \n",
    "        n_attention_head = n_attention_head, \n",
    "        feed_forward_dim = feed_forward_dim,\n",
    "        dropout_rate = dropout_units_list,\n",
    "        num_dense_layers = n_dense_layers,\n",
    "        dense_units_list = dense_units_list\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=hp_learning_rate), loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), metrics =[tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.AUC(), pr, p, r])\n",
    "    for i, o in train_tfds.take(1):\n",
    "        model(i)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "        get_model_tunning,\n",
    "        objective= kt.Objective('val_auc', direction=\"max\"),\n",
    "        max_trials = 30,\n",
    "        directory=r\"Hyperparam_tunning\",\n",
    "        project_name='keras_attention_tunning',\n",
    "    )\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50)\n",
    "if TUNNING:\n",
    "    tuner.search(train_tfds,  epochs=100, batch_size=batch_size, validation_data = dev_tfds, verbose =1, callbacks=[stop_early, tensorboard_callback]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, best_hps in enumerate(tuner.get_best_hyperparameters(num_trials=3)):\n",
    "    print(f\"Best Hyperparameters: {best_hps.__dict__}\")\n",
    "    text_generator_model = tuner.hypermodel.build(best_hps)  # Build the model with best hyperparameters\n",
    "    \n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=f\"logs/attention_training/model{i}\")\n",
    "    text_generator_model.fit(train_tfds,  epochs=300, batch_size=batch_size, validation_data = test_tfds, verbose =1, callbacks=[stop_early, tensorboard_callback]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaaaaaaaaaaaaaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dynamic = get_model_dynamic(best_hps)\n",
    "\n",
    "if FIT_MODEL:\n",
    "    \n",
    "    history = model_dynamic.fit(x[0], y, \n",
    "              batch_size=batch_size, \n",
    "              epochs=300,\n",
    "              shuffle=True,\n",
    "              validation_data = (x_dev[0],y_dev),\n",
    "              callbacks=[PlotLossesCallback() ,stop_early]\n",
    "             )\n",
    "    \n",
    "    model_dynamic.save(f'{DATA_CHECKPOINTS_DIR}_dynamic/model_dynamic')\n",
    "\n",
    "    with open(f'{DATA_CHECKPOINTS_DIR}_dynamic/history.pickle', 'wb') as handle:\n",
    "        pickle.dump(history.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "else:\n",
    "    model_dynamic = tf.keras.models.load_model(f'{DATA_CHECKPOINTS_DIR}_dynamic/model_dynamic')\n",
    "\n",
    "    with open(f'{DATA_CHECKPOINTS_DIR}_dynamic/history.pickle', 'rb') as handle:\n",
    "        history = pickle.load(handle)\n",
    "        plt.figure()\n",
    "        plt.plot(history['binary_accuracy'])\n",
    "        plt.plot(history['val_binary_accuracy'])\n",
    "        plt.title('Model accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(history['loss'])\n",
    "        plt.plot(history['val_loss'])\n",
    "        plt.title('Model training loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat =model_dynamic.predict(x_test[0][0:100])\n",
    "yhat[yhat >=0.5] = 1\n",
    "yhat[yhat < 0.5] = 0 \n",
    "yhat\n",
    "\n",
    "y_test = y_test.reshape((len(y_test),1))\n",
    "y_test\n",
    "print(\"x 1º line : \"+str(x))\n",
    "print(\"y : \",y) \n",
    "print(\"y_test : \",y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ySMgYZbFDX7d"
   },
   "outputs": [],
   "source": [
    "\n",
    "yhat =model_dynamic.predict(x_test)\n",
    "yhat[yhat >=0.5] = 1\n",
    "yhat[yhat < 0.5] = 0 \n",
    "y_test = y_test.reshape((len(y_test),1))\n",
    "\n",
    "pr = tf.keras.metrics.AUC(curve=\"PR\")\n",
    "print()\n",
    "\n",
    "p = tf.keras.metrics.Precision()\n",
    "p.update_state(y_test,yhat)\n",
    "\n",
    "r = tf.keras.metrics.Recall()\n",
    "r.update_state(y_test,yhat)\n",
    "\n",
    "base_pr = pr(y_test, yhat).numpy()\n",
    "base_p = p.result().numpy()\n",
    "base_r = r.result().numpy()\n",
    "print(\"AUC\",base_pr,\" | Precision : \",base_p, \" | Recall : \",base_r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(list(y_test),list(yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_tn, base_fp, base_fn, base_tp = confusion_matrix(list(y_test),list(yhat)).ravel()\n",
    "base_tn, base_fp, base_fn, base_tp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15474,
     "status": "ok",
     "timestamp": 1561582743898,
     "user": {
      "displayName": "yaniel barbosa",
      "photoUrl": "https://lh3.googleusercontent.com/-JlWhURV1zuQ/AAAAAAAAAAI/AAAAAAAAADQ/hOOCIumFYTE/s64/photo.jpg",
      "userId": "13057870575473273651"
     },
     "user_tz": -60
    },
    "id": "mebJWLtxDX7l",
    "outputId": "53d69c67-1e6f-4f8f-8c09-817fc8ea8dd3"
   },
   "outputs": [],
   "source": [
    "def print_predictions(X, pred):\n",
    "    for i in range(len(X)):\n",
    "        xx = [tokenizer.index_word.get(ind) for ind in X[i] if tokenizer.index_word.get(ind) is not None]\n",
    "        print(' '.join(xx), \"Prediction :\", int(pred[i]),\" - Real :\",y_test[i][0],\"\\n\")\n",
    "        \n",
    "        if i==10:\n",
    "            break\n",
    "\n",
    "print_predictions(x_test[0], yhat)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SentimentClassification.ipynb",
   "provenance": [
    {
     "file_id": "1DUXYgOxu2qAOP0B7VnjYFK0z4wXoDMsa",
     "timestamp": 1560792666082
    },
    {
     "file_id": "1MBv0UvhVnj2zvRWBQLik9hC9Fgm-pyR-",
     "timestamp": 1560067092399
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tf_cpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "dffee67a034c262da22ecd866efe5fa1174a6876e1e96ed512a9acb5833498b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
