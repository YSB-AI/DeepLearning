{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8048,
     "status": "ok",
     "timestamp": 1595880019461,
     "user": {
      "displayName": "yaniel barbosa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjeEwoPVbOEW9Pwf_Z5Q-LyxKX0GvQrJ-S-pfKh=s64",
      "userId": "13057870575473273651"
     },
     "user_tz": -60
    },
    "id": "eR2oejU7JhSE",
    "outputId": "b803b1cb-a567-4b08-84de-09862d595617"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-20 09:24:28.262343: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-20 09:24:28.321132: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-20 09:24:29.354621: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/n/anaconda3/envs/AI_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-20 09:24:31.097300: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-20 09:24:31.193854: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-20 09:24:31.197826: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re #regex\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Dense, LayerNormalization, Dropout,  LSTM, Dense, Embedding, Bidirectional,BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import io\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "import random\n",
    "\n",
    "from rouge import Rouge #https://towardsdatascience.com/introduction-to-text-summarization-with-rouge-scores-84140c64b471\n",
    "\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras_tuner as kt\n",
    "from transformers import GPT2TokenizerFast, GPT2Config, GPT2Model\n",
    "\n",
    "print(\"Num GPUs Available: \", (tf.config.experimental.list_physical_devices()))\n",
    "\n",
    "ROUGE = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras-nlp\n",
    "import keras_nlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8007,
     "status": "ok",
     "timestamp": 1595880019463,
     "user": {
      "displayName": "yaniel barbosa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjeEwoPVbOEW9Pwf_Z5Q-LyxKX0GvQrJ-S-pfKh=s64",
      "userId": "13057870575473273651"
     },
     "user_tz": -60
    },
    "id": "CBl11TrADX6-"
   },
   "outputs": [],
   "source": [
    "buffer_size = 10000\n",
    "batch_size= 16\n",
    "window_size= 50 \n",
    "epochs= 150\n",
    "embedding_dim = 200\n",
    "\n",
    "TRAIN_AND_SAVE = True\n",
    "TRAIN_AND_SAVE_TRANFORMER = True\n",
    "\n",
    "TRAIN_TEXT= True\n",
    "SEED=42\n",
    "TRAIN_TOKEN = True\n",
    "TUNNING = True\n",
    "GENERATE_TEXT= True\n",
    "\n",
    "\n",
    "translationTable = str.maketrans(\"áéíóúàèìòùâêîôûãõç\", \"aeiouaeiouaeiouaoc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow.data as tf_data\n",
    "# import tensorflow.strings as tf_strings\n",
    "\n",
    "# keras.utils.get_file(\n",
    "#     origin=\"https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip\",\n",
    "#     extract=True,\n",
    "# )\n",
    "# dir = os.path.expanduser(\"~/.keras/datasets/simplebooks/\")\n",
    "\n",
    "# # Load simplebooks-92 train set and filter out short lines.\n",
    "# raw_train_ds = (\n",
    "#     tf_data.TextLineDataset(dir + \"simplebooks-92-raw/train.txt\")\n",
    "#     .filter(lambda x: tf_strings.length(x) > 512)\n",
    "#     .batch(batch_size)\n",
    "#     .shuffle(buffer_size=256)\n",
    "# )\n",
    "# for x in raw_train_ds.take(1):\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_CHECKPOINTS_DIR = './checkpoints'\n",
    "tfrecord_filename = \"train_tmp.tfrecord\"\n",
    "train_tmp_record_path = f'{DATA_CHECKPOINTS_DIR}/{tfrecord_filename}'\n",
    "\n",
    "!mkdir -p {DATA_CHECKPOINTS_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(path_to_file ='./nietzsche.txt'):\n",
    "    path_to_file = \"./eca_de_queiros_os_maias.txt\"\n",
    "    \n",
    "    with io.open(path_to_file, encoding='utf-8') as f:\n",
    "        text = f.read().lower()\n",
    "\n",
    "    print('corpus length:', len(text))\n",
    "    text = text.translate(translationTable)\n",
    "\n",
    "    string_check= re.compile('[^a-zA-Z.?!,:\\'<>]')\n",
    "\n",
    "    text = re.sub(string_check, ' ', (text)\n",
    "                        .replace(\"-\",\" \")\n",
    "                        .replace(\"´\",\"'\")\n",
    "                        .replace(\"`\",\"'\")\n",
    "                        .replace(\",\",\" , \")\n",
    "                        .replace(\"s.\",\" sao \")\n",
    "                        .replace(\"d.\",\"don\")\n",
    "                        .replace(\"v.\",\"v\")\n",
    "                        .replace(\"sr.\", \"senhor\")\n",
    "                        .replace(\"sra.\", \"senhora\")\n",
    "                        .replace(\"exmo.\", \"exmo\")\n",
    "                        .replace(\"exma.\", \"exma\")\n",
    "                        .replace(\"x.\", \"x\")\n",
    "    )\n",
    "\n",
    "    text = re.sub(' +', ' ',text)\n",
    "    text = (re.sub('\\.+', \" . \\n\", text).replace(\"<br />\",\" \"))\n",
    "\n",
    "    lines_list = list()\n",
    "    lines_list = text.split(\"\\n\")\n",
    "\n",
    "    cols = ['sentences']\n",
    "    df_tmp= pd.DataFrame(columns=cols)\n",
    "    df_tmp[\"sentences\"] = lines_list\n",
    "\n",
    "    return df_tmp\n",
    "complete_text= prepare_data()\n",
    "complete_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= prepare_data()\n",
    "\n",
    "complete_text = ' '.join(df.sentences.values.tolist())\n",
    "complete_unique_words_list = list(set(complete_text.split(' ')))\n",
    "vocab_len = len(complete_unique_words_list)\n",
    "display(df)\n",
    "display(vocab_len)\n",
    "display(complete_unique_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sent_length\"] = df.sentences.apply(lambda x: len(x.split()))\n",
    "\n",
    "plt.figure()\n",
    "pd.Series(df.sent_length).hist()\n",
    "plt.title(\"Sentence Size for training dataset\")\n",
    "df[df.sent_length>window_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.sent_length>=5]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_list=['data scientist tasks', 'data scientist jobs','data scientist job','data scientist, and','data engineer tasks','data engineer jobs']\n",
    "\n",
    "def Minimum_Bayes_Risk(list_of_candidates, field = \"rouge-l\", candidate_reference = None):\n",
    "    candidate_score = {}\n",
    "    max_score = 0\n",
    "    best_score_candidate= ''\n",
    "    best_candidate_index = -1\n",
    "    if candidate_reference == None :\n",
    "        for candidate_ref in list_of_candidates:\n",
    "            \n",
    "            rest_of_candidates = copy.deepcopy(list_of_candidates)\n",
    "            if candidate_ref in rest_of_candidates : rest_of_candidates.remove(candidate_ref)\n",
    "            score = 0\n",
    "\n",
    "            for candidate in rest_of_candidates:\n",
    "                score += ROUGE.get_scores(candidate, candidate_ref)[0][field][\"f\"]\n",
    "            \n",
    "            score = score/len(rest_of_candidates)\n",
    "\n",
    "            candidate_score[candidate_ref] = score\n",
    "\n",
    "            if score >= max_score:\n",
    "                best_score_candidate =candidate_ref\n",
    "                best_candidate_index = list_of_candidates.index(candidate_ref)\n",
    "                max_score = score\n",
    "\n",
    "    else:\n",
    "            \n",
    "            for candidate in list_of_candidates:\n",
    "                score = 0\n",
    "                score = ROUGE.get_scores(candidate, candidate_reference)[0][field][\"f\"]\n",
    "                if score >= max_score:\n",
    "                    best_score_candidate =candidate_ref\n",
    "\n",
    "\n",
    "    return candidate_score, best_score_candidate, best_candidate_index\n",
    "\n",
    "candidate_score, best_score_candidate, best_candidate_index = Minimum_Bayes_Risk(candidate_list)\n",
    "candidate_score, best_score_candidate , best_candidate_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting vocabs and tokenizing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_val_test = train_test_split(df,test_size=0.2,train_size=0.8)\n",
    "\n",
    "display(df_train)\n",
    "display(df_val_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "def decode_text(row):\n",
    "    cleaned_review = row[\"cleaned_review\"]\n",
    "\n",
    "    out = tokenizer(\n",
    "        cleaned_review,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return out[\"last_hidden_state\"]\n",
    "\n",
    "train_ds[[\"inputs\"]] = train_ds.apply(encode_text, axis=1, result_type='expand')\n",
    "val_ds[[\"inputs\"]] = val_ds.apply(encode_text, axis=1, result_type='expand')\n",
    "test_ds[[\"inputs\"]] = test_ds.apply(encode_text, axis=1, result_type='expand')\n",
    "\n",
    "display(train_ds)\n",
    "display(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_test, df_val = train_test_split(df_val_test, test_size = 0.25,train_size =0.75)\n",
    "display(df_test)\n",
    "display(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfds = tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            df_train.sentences.tolist()\n",
    "        )\n",
    "    ).batch(batch_size).shuffle(buffer_size=len(df_train)).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "decoder = GPT2DoubleHeadsModel.from_pretrained(\"openai-community/gpt2\")\n",
    "for x in train_tfds.take(1):\n",
    "    print(x)\n",
    "    decoder(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_len = vocab_len, \n",
    "        window_size = window_size, \n",
    "        embedding_dim = embedding_dim,  \n",
    "        n_layers = 1, \n",
    "        n_attention_head = 2, \n",
    "        feed_forward_dim = [128],\n",
    "        att_dropout_rate = [0.0],\n",
    "        n_dense_layers = 1, \n",
    "        dense_layers = [32], \n",
    "        dropout_rate = []\n",
    "        ):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.num_layers = n_layers\n",
    "        self.n_attention_head =n_attention_head\n",
    "        self.feed_forward_dim = feed_forward_dim\n",
    "        self.att_dropout_rate = att_dropout_rate\n",
    "\n",
    "        self.decoder = GPT2DoubleHeadsModel.from_pretrained(\"openai-community/gpt2\")\n",
    "        \n",
    "        self.decoder_layer = [keras_nlp.layers.TransformerDecoder(num_heads=self.n_attention_head, intermediate_dim=self.feed_forward_dim[i],dropout = att_dropout_rate[i]) for i in range(self.num_layers)]\n",
    "        \n",
    "        self.n_dense_layers = n_dense_layers\n",
    "\n",
    "        if self.n_dense_layers >0:\n",
    "            self.dense_layers = [Dense(dense_layers[i], activation='relu') for i in range(self.n_dense_layers)]\n",
    "\n",
    "            if self.n_dense_layers > 1:\n",
    "                self.dropout_layers = [Dropout(dropout_rate[i]) for i in range(self.n_dense_layers-1)]\n",
    "        \n",
    "        self.output_dense = Dense(vocab_len)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.decoder_layer(inputs)\n",
    "\n",
    "        for i in range(self.num_layers ):\n",
    "            x = self.decoder_layer[i](x)\n",
    "        \n",
    "        if self.n_dense_layers > 0:\n",
    "            for i in range(self.n_dense_layers):\n",
    "                x = self.dense_layers[i](x)\n",
    "\n",
    "                if i < (self.n_dense_layers - 1) and  self.n_dense_layers > 1:\n",
    "                    x = self.dropout_layers[i](x)\n",
    "\n",
    "        outputs_logits  = self.output_dense(x)\n",
    "\n",
    "        return outputs_logits\n",
    "\n",
    "\n",
    "# generate_text_model = TextGenerator(\n",
    "#     vocab_len = vocab_len,\n",
    "#     window_size = window_size, \n",
    "#     embedding_dim = embedding_dim,  \n",
    "#     n_layers = 1, \n",
    "#     n_attention_head = 2, \n",
    "#     feed_forward_dim = [128],\n",
    "#     dropout_rate = [0.0]\n",
    "#     )\n",
    "\n",
    "# loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
    "\n",
    "# generate_text_model.compile(\n",
    "#     loss=loss_fn, \n",
    "#     optimizer=Adam(learning_rate=0.0005), \n",
    "#     metrics=[perplexity])\n",
    "\n",
    "# for i, o in train_tfds.take(1):\n",
    "#     print(i.shape, o.shape)\n",
    "#     print(generate_text_model(i).shape)\n",
    "\n",
    "# generate_text_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# generate_text_model.fit(train_tfds, validation_data= dev_tfds, epochs=epochs, batch_size=32, verbose =1, callbacks=[stop_early, tensorboard_callback, text_generation_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tokens = start_packer(tokenizer([\"\"]))\n",
    "prompt_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next(prompt, cache, index):\n",
    "    logits = generate_text_model(prompt)[:, index - 1, :]\n",
    "    # Ignore hidden states for now; only needed for contrastive search.\n",
    "    hidden_states = None\n",
    "    return logits, hidden_states, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and tunning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopKTextGenerator(keras.callbacks.Callback):\n",
    "    \"\"\"A callback to generate text from a trained model using top-k.\"\"\"\n",
    "\n",
    "    def __init__(self, k):\n",
    "        self.top_k_sampler = keras_nlp.samplers.TopKSampler(k)\n",
    "        #self.beam_search_sampler = keras_nlp.samplers.BeamSampler(num_beams=10)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        output_tokens = self.top_k_sampler(\n",
    "            next=next,\n",
    "            prompt=prompt_tokens,\n",
    "            index=1,\n",
    "        )\n",
    "        txt = tokenizer.detokenize(output_tokens)\n",
    "        print(f\"Top-K search generated text: \\n{txt}\\n\")\n",
    "        \n",
    "text_generation_callback = TopKTextGenerator(k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"_v1\"\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=f\"logs/attention_training{version}/\")\n",
    "\n",
    "def get_model_tunning(hp):\n",
    "\n",
    "\n",
    "    n_att_layers = hp.Int('n_att_layers', 1, 3)\n",
    "    n_attention_head= hp.Int('n_attention_head', 1, 2)\n",
    "    feed_forward_dim_list =  [hp.Int('att_units_'+str(i), 32, 128) for i in range(n_att_layers)]\n",
    "    att_dropout_rate=  [hp.Float('att_dropout_rate_'+str(i), 0.0, 0.5, step = 0.1) for i in range(n_att_layers)]\n",
    "\n",
    "    n_dense_layers = hp.Int('n_dense_layers', 0, 2)\n",
    "    dropout_rate = []\n",
    "    dense_layers_list = []\n",
    "    if n_dense_layers> 0:\n",
    "        dense_layers_list =  [hp.Int('dense_layers_'+str(i), 32, 128) for i in range(n_dense_layers)]\n",
    "\n",
    "        if n_dense_layers > 1:\n",
    "            dropout_rate=  [hp.Float('dropout_rate_'+str(i), 0.0, 0.5, step = 0.1) for i in range(n_dense_layers-1)]\n",
    "    \n",
    "    hp_learning_rate = hp.Float('learning_rate', 0.000001, 0.001)\n",
    "    embedding_dim_list = 50#hp.Int('embedding_dim', 50,  200, step = 50)\n",
    "    \n",
    "    model = TextGenerator(\n",
    "        vocab_len = vocab_len,\n",
    "        window_size = window_size, \n",
    "        embedding_dim = embedding_dim_list,  \n",
    "        n_layers = n_att_layers, \n",
    "        n_attention_head = n_attention_head, \n",
    "        feed_forward_dim = feed_forward_dim_list,\n",
    "        att_dropout_rate = att_dropout_rate,\n",
    "        n_dense_layers = n_dense_layers, \n",
    "        dense_layers = dense_layers_list, \n",
    "        dropout_rate = dropout_rate\n",
    "    )\n",
    "\n",
    "    loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
    "\n",
    "    model.compile(\n",
    "        loss=loss_fn, \n",
    "        optimizer=Adam(learning_rate=hp_learning_rate), \n",
    "        metrics=[perplexity],\n",
    "        )\n",
    "\n",
    "    for i, o in train_tfds.take(1):\n",
    "        print(i.shape, o.shape)\n",
    "        print(model(i).shape)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "        get_model_tunning,\n",
    "        objective= kt.Objective('val_perplexity', direction=\"min\"), #kt.Objective('val_auc', direction=\"max\"),# #val_binary_accuracy\n",
    "        max_trials = 60,\n",
    "        directory=r\"Hyperparam_tunning\",\n",
    "        project_name=f'keras_att_tunning{version}',\n",
    "    )\n",
    "\n",
    "if TUNNING:\n",
    "    tuner.search(train_tfds,  epochs=200, batch_size=batch_size, validation_data = dev_tfds, verbose =1, callbacks=[stop_early, tensorboard_callback]) \n",
    "\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_hps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, trials in enumerate(tuner.oracle.get_best_trials(num_trials=50)):\n",
    "    print(f\"[{i}] Trial id :{trials.trial_id} | Score :{trials.score} -->\",trials.hyperparameters.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, trials in enumerate(tuner.oracle.get_best_trials(num_trials=60)):\n",
    "    print(f\"[{i}] Trial id :{trials.trial_id} | Score :{trials.score} -->\",trials.hyperparameters.values)\n",
    "    if i == 34:\n",
    "        print(f\"[{i}] Trial id :{trials.trial_id} | Score :{trials.score} -->\",trials.hyperparameters.values)\n",
    "\n",
    "        n_att_layers = 1#trials.hyperparameters.values['n_att_layers']\n",
    "        n_attention_head= trials.hyperparameters.values['n_attention_head']\n",
    "\n",
    "\n",
    "        feed_forward_dim_list = []\n",
    "        att_dropout_rate = []\n",
    "        if n_att_layers > 0 :\n",
    "            feed_forward_dim_list =  [32 for i in range(n_att_layers)] #trials.hyperparameters.values['att_units_'+str(i)] \n",
    "            att_dropout_rate=  [0.4 for i in range(n_att_layers)] #trials.hyperparameters.values['att_dropout_rate_'+str(i)]\n",
    "        \n",
    "        hp_learning_rate = 0.000005 #trials.hyperparameters.values['learning_rate']\n",
    "        embedding_dim_list = 100\n",
    "        \n",
    "\n",
    "        n_dense_layers = trials.hyperparameters.values['n_dense_layers']\n",
    "        dropout_rate = []\n",
    "        dense_layers_list = []\n",
    "        if n_dense_layers> 0:\n",
    "            dense_layers_list =  [trials.hyperparameters.values['dense_layers_'+str(i)] for i in range(n_dense_layers)]\n",
    "\n",
    "            if n_dense_layers > 1:\n",
    "                dropout_rate=  [trials.hyperparameters.values['dropout_rate_'+str(i)] for i in range(n_dense_layers-1)]\n",
    "\n",
    "\n",
    "        generate_text_model = TextGenerator(\n",
    "            vocab_len = vocab_len,\n",
    "            window_size = window_size, \n",
    "            embedding_dim = embedding_dim_list,  \n",
    "            n_layers = n_att_layers, \n",
    "            n_attention_head = n_attention_head, \n",
    "            feed_forward_dim = feed_forward_dim_list,\n",
    "            att_dropout_rate = att_dropout_rate,\n",
    "            n_dense_layers = n_dense_layers, \n",
    "            dense_layers = dense_layers_list, \n",
    "            dropout_rate = dropout_rate\n",
    "        )\n",
    "\n",
    "        loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
    "\n",
    "        generate_text_model.compile(\n",
    "            loss=loss_fn, \n",
    "            optimizer=Adam(learning_rate=hp_learning_rate), \n",
    "            metrics=[perplexity],\n",
    "            )\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=f\"logs/attention_training{version}/model{i}_v2\")\n",
    "        for i, o in train_tfds.take(1):\n",
    "            print(i.shape, o.shape)\n",
    "            print(generate_text_model(i).shape)\n",
    "\n",
    "        \n",
    "        generate_text_model.fit(train_tfds,  epochs=500, batch_size=batch_size, validation_data = dev_tfds, verbose =1, callbacks=[tensorboard_callback,text_generation_callback]) \n",
    "        break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, best_hps in enumerate(tuner.get_best_hyperparameters(num_trials=40)):\n",
    "#     if i == 34:\n",
    "#         print(f\"Best Hyperparameters: {best_hps.__dict__}\")\n",
    "#         generate_text_model = tuner.hypermodel.build(best_hps)  # Build the model with best hyperparameters\n",
    "        \n",
    "#         tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=f\"logs/attention_training{version}/model{i}\")\n",
    "#         generate_text_model.fit(train_tfds,  epochs=250,\n",
    "#          batch_size=batch_size, validation_data = dev_tfds, verbose =1, callbacks=[tensorboard_callback, text_generation_callback]) \n",
    "\n",
    "\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference using different techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy sampling approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = keras_nlp.samplers.GreedySampler()aaaaaaaaaaaa\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,  # Start sampling immediately after the [BOS] token.\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "print(f\"Greedy search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Search approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = keras_nlp.samplers.BeamSampler(num_beams=10)\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "print(f\"Beam search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = keras_nlp.samplers.RandomSampler()\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "print(f\"Random search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-K search approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = keras_nlp.samplers.TopKSampler(k=10)\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "print(f\"Top-K search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-P search approach\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = keras_nlp.samplers.TopPSampler(p=0.5)\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "print(f\"Top-P search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, o in dataset.take(1):aaaaaaaaaaaaaaaaaa\n",
    "    print(i)\n",
    "    \n",
    "emb_layer = Embedding(input_dim=vocab_len, input_length = window_size, output_dim=emb_dim, weights=[embeddings_matrix] ,name =\"emb_layer\", trainable=False, mask_zero = True)\n",
    "\n",
    "attention_layer = SelfMaskedDotProductAttention()\n",
    "attention_layer_causal1 = SelfMaskedDotProductAttention()\n",
    "#attention_layer_causal2 = SelfMaskedDotProductAttention(causal_mask_enabled=2)\n",
    "\n",
    "emb_layer = Embedding(input_dim=vocab_len, input_length = window_size, output_dim=emb_dim, weights=[embeddings_matrix] ,name =\"emb_layer\", trainable=False, mask_zero = True)\n",
    "query = emb_layer(i)\n",
    "values = emb_layer(i)\n",
    "\n",
    "context_vector, attention_weights = attention_layer(query,values,values, (i != 0))\n",
    "context_vector1, attention_weights1 = attention_layer_causal1(query,values,values, (i != 0))\n",
    "context_vector1, attention_weights_causal = attention_layer_causal1(query,values,values, (i != 0), (1 - tf.linalg.band_part(tf.ones((i.shape[1], i.shape[1])), -1, 0)))\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.subplot(3, 2, 1)\n",
    "plt.pcolormesh(attention_weights[:, 0, :])\n",
    "plt.title('Attention weights - Not causal')\n",
    "\n",
    "plt.subplot(3, 2, 2)\n",
    "plt.pcolormesh(i != 0)\n",
    "plt.title('Mask')\n",
    "\n",
    "plt.subplot(3, 2, 3)\n",
    "plt.pcolormesh(attention_weights1[:, 0, :])\n",
    "plt.title('Attention weights 1')\n",
    "\n",
    "plt.subplot(3, 2, 4)\n",
    "plt.pcolormesh(attention_weights[0])\n",
    "plt.title('Attention weights')\n",
    "\n",
    "plt.subplot(3, 2, 5)\n",
    "plt.pcolormesh(attention_weights1[0])\n",
    "plt.title('Attention weights 1')\n",
    "\n",
    "plt.subplot(3, 2, 5)\n",
    "plt.pcolormesh(attention_weights_causal[0])\n",
    "plt.title('Attention weights causal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blstm_enable = False\n",
    "\n",
    "if TRAIN_AND_SAVE:\n",
    "    model = ModelClass(vocab_len, model_type=1, causal_mask_enabled= 0, blstm_enable=blstm_enable, use_attention=False)\n",
    "    model.compile(loss=MaskedLossCustom(), optimizer=Adam(learning_rate=0.0005), metrics=[tf.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "    model_causal1 = ModelClass(vocab_len, model_type=1, causal_mask_enabled= 1, blstm_enable=blstm_enable, loop= False)\n",
    "    model_causal1.compile(loss=MaskedLossCustom(), optimizer=Adam(learning_rate=0.0005), metrics=[tf.metrics.SparseCategoricalAccuracy()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = vocab_len\n",
    "token_mask_ids = np.array([tokenizer.word_index[x] for x in  [ '<oov>', '[EOS]' ]])[:, None]#\n",
    "token_mask = np.zeros([vocab_size], dtype=np.bool)\n",
    "token_mask[np.array(token_mask_ids)] = True\n",
    "\n",
    "print(\"token_mask\", token_mask)\n",
    "\n",
    "\n",
    "def generate_sentence(model_inst, input_text, max_length=10, temperature=0.5, number_of_candidates = 20, ):\n",
    "    \n",
    "    \n",
    "    attention_per_candidate = []\n",
    "    result_text_per_candidate = []\n",
    "    input_text_per_candidate = []\n",
    "    output_text_per_candidate = []\n",
    "\n",
    "    for c in range(number_of_candidates):\n",
    "\n",
    "        last_state= None\n",
    "        last_activation = None\n",
    "        input_mask = None\n",
    "        last_attention = None\n",
    "\n",
    "        result_tokens = []\n",
    "    \n",
    "        attention = []\n",
    "\n",
    "        x = input_tokens\n",
    "        input_text= []\n",
    "        output_text = []\n",
    "        result_tokens = [w.numpy() for w in  x[0] if w != 0 ]\n",
    "        for k in range(max_length):\n",
    "\n",
    "            input_text.append(' '.join([tokenizer.index_word[w.numpy()] for w in  x[0] if w != 0 ]))\n",
    "            x = pad_sequences(x, padding = \"post\", maxlen = window_size)#\n",
    "\n",
    "            input_mask = x!= 0\n",
    "            t = np.count_nonzero(input_mask)\n",
    "\n",
    "            if no_state: last_state= None\n",
    "\n",
    "            predictions, last_state , attention_weights, last_activation, last_attention  =model_inst(x, last_attention, last_state,  last_activation)\n",
    "\n",
    "            if k > 0 and attention_weights is not None:\n",
    "                attention.append(attention_weights[0])\n",
    "\n",
    "            predicted_logits  = predictions[:,t-1,:]/temperature\n",
    "            predicted_logits = tf.where(token_mask, -np.inf, predicted_logits)\n",
    "            predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "            new_tokens = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "            if loop:\n",
    "            \n",
    "                x = new_tokens.numpy()\n",
    "            else : \n",
    "\n",
    "                x = list(x[0])\n",
    "                x = [i for i in x if i != 0]\n",
    "                x.append(new_tokens.numpy()[0])\n",
    "                \n",
    "\n",
    "            output_text.append(' '.join([tokenizer.index_word[w] for w in  x if w != 0 ][1:]))\n",
    "            x = tf.reshape(tf.convert_to_tensor(x), (len(input_tokens),len(x)))\n",
    "            result_tokens.append(new_tokens.numpy()[0])\n",
    "                \n",
    "            if tokenizer.word_index['[EOS]'] == new_tokens.numpy()[0]:\n",
    "                break\n",
    "\n",
    "\n",
    "        result_text =   ' '.join([tokenizer.index_word[w] for w in  result_tokens if w != 0 ])\n",
    "        \n",
    "        attention_per_candidate.append(attention)\n",
    "        result_text_per_candidate.append(result_text)\n",
    "        input_text_per_candidate.append(input_text)\n",
    "        output_text_per_candidate.append(output_text)\n",
    "\n",
    "    candidate_score, best_score_candidate, best_candidate_index = Minimum_Bayes_Risk(result_text_per_candidate)\n",
    "\n",
    "    attention_stack = attention_per_candidate[best_candidate_index]\n",
    "    result_text = result_text_per_candidate[best_candidate_index]\n",
    "    input_text = input_text_per_candidate[best_candidate_index]\n",
    "    output_text = output_text_per_candidate[best_candidate_index]\n",
    "    \n",
    "    return {'generated_text': result_text, 'attention': attention_stack, \"input_text\" : input_text, \"output_text\" : output_text, \"candidates\": result_text_per_candidate}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_AND_SAVE:\n",
    "    input_sent = \"[BOS] eu quero \"\n",
    "    inp_seq = np.array([tokenizer.word_index[x] for x in input_sent.split()])\n",
    "    print(\"inp_seq -->\",inp_seq)\n",
    "\n",
    "    result = generate_sentence(model_causal1, input_tokens = tf.expand_dims(inp_seq,0),max_length= 5)\n",
    "    print(\"RESULT : \",result[\"generated_text\"])\n",
    "    plot_weights(result, stop_time = 3, loop=True)\n",
    "\n",
    "    result = generate_sentence(model_causal1, input_tokens = tf.expand_dims(inp_seq,0),max_length= 5, loop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_AND_SAVE:\n",
    "    model.use_tf_function = False\n",
    "    for i, o in dataset.take(2):\n",
    "        print(model.train_step([i, o]))\n",
    "        print(model.test_step([i, o]))\n",
    "    print()\n",
    "\n",
    "    model_causal1.use_tf_function = False\n",
    "    for i, o in dataset.take(2):\n",
    "        print(model_causal1.train_step([i, o]))\n",
    "        print(model_causal1.test_step([i, o]))\n",
    "    print()\n",
    "\n",
    "    print(\"\\n---------------------\\n\")\n",
    "\n",
    "    model.use_tf_function = True\n",
    "    for i, o in dataset.take(2):\n",
    "        print(model.train_step([i, o]))\n",
    "        print(model.test_step([i, o]))\n",
    "    print()\n",
    "\n",
    "    model_causal1.use_tf_function = True\n",
    "    for i, o in dataset.take(2):\n",
    "        print(model_causal1.train_step([i, o]))\n",
    "        print(model_causal1.test_step([i, o]))\n",
    "\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_AND_SAVE:\n",
    "    \n",
    "    history = model.fit(dataset, epochs=epochs, shuffle= True,#6000\n",
    "                        steps_per_epoch = train_size, callbacks=[PlotLossesCallback()], validation_data=val_dataset, validation_steps=10)\n",
    "    model.save_weights(\"./checkpoints/model_ckpt\")\n",
    "\n",
    "    with open('./checkpoints/history.pickle', 'wb') as handle:\n",
    "        pickle.dump(history.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "else :\n",
    "    print(\"Loading saved data ...\")\n",
    "    model = ModelClass(vocab_len, model_type=1, causal_mask_enabled= 0, blstm_enable=blstm_enable, use_attention=False)\n",
    "    model.load_weights(\"./checkpoints/model_ckpt\")\n",
    "\n",
    "    with open('./checkpoints/history.pickle', 'rb') as handle:\n",
    "        history = pickle.load(handle)\n",
    "        plt.figure()\n",
    "        plt.plot(history['sparse_categorical_accuracy'])\n",
    "        plt.plot(history['val_sparse_categorical_accuracy'])\n",
    "        plt.title('Model accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(history['loss'])\n",
    "        plt.plot(history['val_loss'])\n",
    "        plt.title('Model training loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(history['perplexity'])\n",
    "        plt.plot(history['val_perplexity'])\n",
    "        plt.title('Model perplexity ')\n",
    "        plt.ylabel('Perplexity')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_AND_SAVE:\n",
    "    \n",
    "    history_causal1 = model_causal1.fit(dataset, epochs=epochs+100, shuffle= True, #6000\n",
    "                        steps_per_epoch = train_size, callbacks=[PlotLossesCallback()], validation_data=val_dataset, validation_steps=10)\n",
    "    model_causal1.save_weights(\"./checkpoints/model_causal1_ckpt\")\n",
    "\n",
    "    with open('./checkpoints/history_causal1.pickle', 'wb') as handle:\n",
    "        pickle.dump(history_causal1.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "else :\n",
    "    print(\"Loading saved data ...\")\n",
    "    model_causal1 = ModelClass(vocab_len, model_type=1, causal_mask_enabled= 1, blstm_enable=blstm_enable, loop= False)\n",
    "\n",
    "    model_causal1.load_weights(\"./checkpoints/model_causal1_ckpt\")\n",
    "\n",
    "    with open('./checkpoints/history_causal1.pickle', 'rb') as handle:\n",
    "        history_causal1 = pickle.load(handle)\n",
    "        plt.figure()\n",
    "        plt.plot(history_causal1['sparse_categorical_accuracy'])\n",
    "        plt.plot(history_causal1['val_sparse_categorical_accuracy'])\n",
    "        plt.title('Model accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(history_causal1['loss'])\n",
    "        plt.plot(history_causal1['val_loss'])\n",
    "        plt.title('Model training loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(history_causal1['perplexity'])\n",
    "        plt.plot(history_causal1['val_perplexity'])\n",
    "        plt.title('Model perplexity ')\n",
    "        plt.ylabel('Perplexity')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, padding_mask= None, look_ahead_mask= None):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead)\n",
    "  but it must be broadcastable for addition.\n",
    "\n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable\n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "  if padding_mask is not None:\n",
    "    scaled_attention_logits += (padding_mask * -1e9)\n",
    "\n",
    "  if look_ahead_mask is not None:\n",
    "    scaled_attention_logits += (look_ahead_mask * -1e9)\n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights\n",
    "\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self,*, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.wq = Dense(d_model)\n",
    "    self.wk = Dense(d_model)\n",
    "    self.wv = Dense(d_model)\n",
    "\n",
    "    self.dense = Dense(d_model)\n",
    "\n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, v, k, q, padding_mask, look_ahead_mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "\n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention( q, k, v, padding_mask, look_ahead_mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention,(batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "n_vectors = 150\n",
    "d_model = 256\n",
    "\n",
    "query = tf.random.uniform((batch_size, n_vectors, d_model), dtype=tf.float32)\n",
    "key = tf.random.uniform((batch_size, n_vectors, d_model), dtype=tf.float32)\n",
    "value = tf.random.uniform((batch_size, n_vectors, d_model), dtype=tf.float32)\n",
    "query.shape, key.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dot product attention\n",
    "x,_ = scaled_dot_product_attention(query, key, key, None)\n",
    "print(f\"Output from dot product attention: {x.shape}\")\n",
    "\n",
    "att = SelfMaskedDotProductAttention()\n",
    "x1, _ = att(query, key,key)\n",
    "#x = tf.concat(x, -1)\n",
    "print(f\"Output from dot product attention: {x1.shape}\")\n",
    "np.where((x-x1) > 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multi-head attention\n",
    "mh_layer = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "x, _ = mh_layer(query, key, value, None, None)\n",
    "print(f\"Output from multi-head attention: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff): #Point wise feed forward network consists of two fully-connected layers with a ReLU activation in between.\n",
    "  return tf.keras.Sequential([\n",
    "      Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])\n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self,*, d_model, num_heads, dff, rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    self.dropout1 = Dropout(rate)\n",
    "    self.dropout2 = Dropout(rate)\n",
    "\n",
    "  def call(self, x, training, padding_mask= None, look_ahead_mask = None):\n",
    "    attn_output, attn_weights_block1  = self.mha(x, x, x, padding_mask, look_ahead_mask)  # (batch_size, input_seq_len, d_model)\n",
    "    attn_output = self.dropout1(attn_output, training=training)\n",
    "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    return out2, attn_weights_block1\n",
    "\n",
    "\n",
    "sample_encoder_layer = DecoderLayer(d_model=512, num_heads=8, dff=batch_size)\n",
    "sample_encoder_layer_output,attn_weights_block1 = sample_encoder_layer(tf.random.uniform((64, 43, 512)), False, None)\n",
    "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "  pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "n, d = 2048, vocab_len\n",
    "pos_encoding = positional_encoding(n, d)\n",
    "print(pos_encoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self,*, num_layers, d_model, embeddings_matrix, window_size, num_heads, dff, vocab_len,\n",
    "               rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.emb_layer = Embedding(input_dim=vocab_len, input_length = window_size, output_dim=d_model, weights=[embeddings_matrix] ,name =\"emb_layer2\", trainable=False)#, mask_zero = True\n",
    "    self.pos_encoding = positional_encoding(window_size, self.d_model)\n",
    "\n",
    "    self.dec_layers = [DecoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate) for _ in range(num_layers)]\n",
    "\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "  def call(self, x, training, padding_mask =None, look_ahead_mask= None):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    attention_weights = {}\n",
    "\n",
    "    x = self.emb_layer(x)  # (batch_size, input_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "    x = self.dropout(x, training=training)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x, block1 = self.dec_layers[i](x, training, padding_mask, look_ahead_mask)\n",
    "      attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "\n",
    "\n",
    "    return x ,attention_weights # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "sample_decoder = Decoder(num_layers=2, d_model=emb_dim, embeddings_matrix = embeddings_matrix, window_size= window_size, num_heads=10,\n",
    "                         dff=2048, vocab_len=vocab_len)\n",
    "\n",
    "for i, o in dataset.take(1):\n",
    "  print(i.shape, o.shape)\n",
    "  output, attn = sample_decoder(i, training=True, padding_mask =None, look_ahead_mask= None)\n",
    "  \n",
    "output.shape, attn['decoder_layer2_block1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(tf.keras.losses.Loss):\n",
    "  def __init__(self):\n",
    "    self.name = 'masked_loss_transformer_custom'\n",
    "    self.loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "  def __call__(self, real, pred):\n",
    "      \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = self.loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "  real = tf.cast(real, dtype=tf.int32)\n",
    "  accuracies = tf.equal(real, tf.cast(tf.argmax(pred, axis=2), dtype=tf.int32))\n",
    "\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  accuracies = tf.math.logical_and(mask, accuracies)\n",
    "\n",
    "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "  mask = tf.cast(mask, dtype=tf.float32)\n",
    "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)\n",
    "  \n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.xlabel('Train Step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,*, num_layers, d_model,embeddings_matrix,window_size, num_heads, dff,  vocab_len,rate=0.1, use_tf_function= False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.decoder = Decoder(num_layers=num_layers, d_model=d_model, embeddings_matrix = embeddings_matrix,\n",
    "                            window_size= window_size, num_heads=num_heads, dff=dff, vocab_len=vocab_len, rate=rate)\n",
    "\n",
    "        self.final_layer = Dense(vocab_len)\n",
    "        self.use_tf_function = use_tf_function\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        # Keras models prefer if you pass all your inputs in the first argument\n",
    "        inp = inputs\n",
    "\n",
    "        padding_mask, look_ahead_mask = self.create_masks(inp)\n",
    "        \n",
    "        # dec_output.shape == (batch_size, inp_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(inp, training, padding_mask, look_ahead_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, vocab_len)\n",
    "\n",
    "        return final_output, attention_weights\n",
    "\n",
    "    def create_masks(self, inp):\n",
    "        # Encoder padding mask (Used in the 2nd attention block in the decoder too.)\n",
    "        \n",
    "        padding_mask = self.create_padding_mask(inp)\n",
    "\n",
    "        # Used in the 1st attention block in the decoder.\n",
    "        # It is used to pad and mask future tokens in the input received by\n",
    "        # the decoder.\n",
    "        look_ahead_mask = self.create_look_ahead_mask(tf.shape(inp)[1])\n",
    "\n",
    "        return padding_mask, look_ahead_mask\n",
    "\n",
    "    def create_padding_mask(self,seq):\n",
    "        seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "        # add extra dimensions to add the padding\n",
    "        # to the attention logits.\n",
    "        return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "    def create_look_ahead_mask(self, size):\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "        return mask  # (seq_len, seq_len)\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        if self.use_tf_function:\n",
    "            return self._tf_train_step(inputs)\n",
    "        else:\n",
    "            return self._train_step(inputs)\n",
    "\n",
    "    @tf.function(input_signature=[[tf.TensorSpec(dtype=tf.int32, shape=[None, window_size]),\n",
    "                                tf.TensorSpec(dtype=tf.int32, shape=[None, window_size])]])\n",
    "                                \n",
    "    def _tf_train_step(self, inputs):\n",
    "        return self._train_step(inputs)\n",
    "\n",
    "\n",
    "\n",
    "    def test_step(self, inputs):\n",
    "        if self.use_tf_function:\n",
    "            return self._tf_test_step(inputs)\n",
    "        else:\n",
    "            return self._test_step(inputs)\n",
    "\n",
    "\n",
    "    @tf.function(input_signature=[[tf.TensorSpec(dtype=tf.int32, shape=[None, window_size]),\n",
    "                                tf.TensorSpec(dtype=tf.int32, shape=[None, window_size])]])\n",
    "    def _tf_test_step(self, inputs):\n",
    "        return self._test_step(inputs)\n",
    "\n",
    "\n",
    "    def _train_step(self, inputs):\n",
    "        inp, tar = inputs\n",
    "        target_mask = tar != 0     \n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            acc = []\n",
    "            loss = tf.constant(0.0)\n",
    "\n",
    "            predictions, _ = self.call(inp,training = True)\n",
    "            loss += self.loss(tar, predictions)\n",
    "            self.compiled_metrics.update_state(tar, predictions)\n",
    "\n",
    "            acc.append(self.metrics[0].result())\n",
    "\n",
    "            average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))  \n",
    "            average_acc  = tf.reduce_mean(tf.convert_to_tensor(acc) )\n",
    "            average_perplexity  = tf.reduce_mean(tf.convert_to_tensor(perplexity(tar, predictions) ))\n",
    "            average_cc  = tf.reduce_mean(tf.convert_to_tensor(accuracy_function(tf.cast(tar, tf.int32), predictions) ))\n",
    "\n",
    "        variables = self.trainable_variables \n",
    "        gradients = tape.gradient(average_loss, variables)\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "        m = {m.name: average_acc  for m in self.metrics} #.update({'loss': batch_loss})m.result() \n",
    "        m[\"loss\"] = average_loss\n",
    "        m[\"perplexity\"] = average_perplexity\n",
    "        m[\"custom_acc\"] = average_cc\n",
    "\n",
    "        return m\n",
    "\n",
    "    def _test_step(self, inputs):\n",
    "        inp, tar = inputs\n",
    "        target_mask = tar != 0     \n",
    "\n",
    "        acc = []\n",
    "        loss = tf.constant(0.0)\n",
    "\n",
    "        predictions, _ = self.call(inp,training = True)\n",
    "        loss += self.loss(tar, predictions)\n",
    "        self.compiled_metrics.update_state(tar, predictions)\n",
    "\n",
    "        acc.append(self.metrics[0].result())\n",
    "\n",
    "        average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))  \n",
    "        average_acc  = tf.reduce_mean(tf.convert_to_tensor(acc) )\n",
    "        average_perplexity  = tf.reduce_mean(tf.convert_to_tensor(perplexity(tar, predictions) ))\n",
    "        average_cc  = tf.reduce_mean(tf.convert_to_tensor(accuracy_function(tf.cast(tar, tf.int32), predictions) ))\n",
    "\n",
    "        m = {m.name: average_acc  for m in self.metrics} #.update({'loss': batch_loss})m.result() \n",
    "        m[\"loss\"] = average_loss\n",
    "        m[\"perplexity\"] = average_perplexity\n",
    "        m[\"custom_acc\"] = average_cc\n",
    "\n",
    "        return m\n",
    "\n",
    "\n",
    "transformer = Transformer(\n",
    "    num_layers=4,\n",
    "    d_model=emb_dim,\n",
    "    embeddings_matrix = embeddings_matrix,\n",
    "    window_size = window_size,\n",
    "    num_heads=10,\n",
    "    dff=256,\n",
    "    vocab_len=vocab_len,\n",
    "    rate=0.1)\n",
    "\n",
    "for i, o in dataset.take(1):\n",
    "    print(i.shape, o.shape)\n",
    "    fn_out, _ = transformer(i, training=False)\n",
    "\n",
    "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "transformer.compile(loss=CustomLoss(), optimizer=optimizer, metrics=[tf.metrics.SparseCategoricalAccuracy()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_sentence_for_transformer(transformer_inst, input_tokens, max_length=10, temperature=1, number_of_candidates = 20, loop= False):\n",
    "    \n",
    "    attention_per_candidate = []\n",
    "    result_text_per_candidate = []\n",
    "    input_text_per_candidate = []\n",
    "    output_text_per_candidate = []\n",
    "\n",
    "    for c in range(number_of_candidates):\n",
    "\n",
    "        input_mask = None\n",
    "        result_tokens = []\n",
    "        attention = []\n",
    "\n",
    "        x = input_tokens\n",
    "        input_text= []\n",
    "        output_text = []\n",
    "        result_tokens = [w.numpy() for w in  x[0] if w != 0 ]\n",
    "        for k in range(max_length):\n",
    "\n",
    "            input_text.append(' '.join([tokenizer.index_word[w.numpy()] for w in  x[0] if w != 0 ]))\n",
    "            x = pad_sequences(x, padding = \"post\", maxlen = window_size)#window_size\n",
    "\n",
    "            input_mask = x!= 0\n",
    "            t = np.count_nonzero(input_mask)\n",
    "\n",
    "            predictions, attention_weights  =transformer_inst(x, training=False)\n",
    "\n",
    "            if k > 0 and attention_weights is not None:\n",
    "                attention.append(attention_weights)\n",
    "\n",
    "            predicted_logits  = predictions[:,t-1,:]/temperature\n",
    "            predicted_logits = tf.where(token_mask, -np.inf, predicted_logits)\n",
    "            predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "            new_tokens = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "            if loop:\n",
    "                x = new_tokens.numpy()\n",
    "            else : \n",
    "\n",
    "                x = list(x[0])\n",
    "                x = [i for i in x if i != 0]\n",
    "                x.append(new_tokens.numpy()[0])\n",
    "                \n",
    "\n",
    "            output_text.append(' '.join([tokenizer.index_word[w] for w in  x if w != 0 ][1:]))\n",
    "            x = tf.reshape(tf.convert_to_tensor(x), (len(input_tokens),len(x)))\n",
    "            result_tokens.append(new_tokens.numpy()[0])\n",
    "                \n",
    "            if tokenizer.word_index['[EOS]'] == new_tokens.numpy()[0]:\n",
    "                break\n",
    "\n",
    "        result_text =   ' '.join([tokenizer.index_word[w] for w in  result_tokens if w != 0 ])\n",
    "        \n",
    "        attention_per_candidate.append(attention)\n",
    "        result_text_per_candidate.append(result_text)\n",
    "        input_text_per_candidate.append(input_text)\n",
    "        output_text_per_candidate.append(output_text)\n",
    "        \n",
    "\n",
    "\n",
    "    candidate_score, best_score_candidate, best_candidate_index = Minimum_Bayes_Risk(result_text_per_candidate)\n",
    "\n",
    "    attention_stack = attention_per_candidate[best_candidate_index]\n",
    "    result_text = result_text_per_candidate[best_candidate_index]\n",
    "    input_text = input_text_per_candidate[best_candidate_index]\n",
    "    output_text = output_text_per_candidate[best_candidate_index]\n",
    "    \n",
    "    return {'generated_text': result_text, 'attention': attention_stack, \"input_text\" : input_text, \"output_text\" : output_text, \"candidates\": result_text_per_candidate}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_AND_SAVE_TRANFORMER:\n",
    "    transformer.use_tf_function = False\n",
    "    for i, o in dataset.take(2):\n",
    "        print(transformer.train_step([i, o]))\n",
    "        print(transformer.test_step([i, o]))\n",
    "    print()\n",
    "\n",
    "\n",
    "    transformer.use_tf_function = True\n",
    "    for i, o in dataset.take(2):\n",
    "        print(transformer.train_step([i, o]))\n",
    "        print(transformer.test_step([i, o]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_AND_SAVE_TRANFORMER:\n",
    "    history_transformer = transformer.fit(dataset, epochs=epochs+50, shuffle= True,#6000\n",
    "                            steps_per_epoch = train_size, callbacks=[PlotLossesCallback()], validation_data=val_dataset, validation_steps=10)\n",
    "    transformer.save_weights(\"./checkpoints/transformer_ckpt\")\n",
    "\n",
    "    with open('./checkpoints/history_transformer.pickle', 'wb') as handle:\n",
    "        pickle.dump(history_transformer.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    print(\"Loading saved data ...\")\n",
    "    transformer = Transformer( num_layers=4, d_model=emb_dim, embeddings_matrix = embeddings_matrix, window_size = window_size, num_heads=10, dff=256, vocab_len=vocab_len, rate=0.1)\n",
    "    transformer.load_weights(\"./checkpoints/transformer_ckpt\")\n",
    "\n",
    "    with open('./checkpoints/history_transformer.pickle', 'rb') as handle:\n",
    "        history_transformer = pickle.load(handle)\n",
    "        plt.figure()\n",
    "        plt.plot(history_transformer['sparse_categorical_accuracy'])\n",
    "        plt.plot(history_transformer['val_sparse_categorical_accuracy'])\n",
    "        plt.title('Model accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(history_transformer['loss'])\n",
    "        plt.plot(history_transformer['val_loss'])\n",
    "        plt.title('Model training loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(history_transformer['perplexity'])\n",
    "        plt.plot(history_transformer['val_perplexity'])\n",
    "        plt.title('Model perplexity ')\n",
    "        plt.ylabel('Perplexity')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(history_transformer['custom_acc'])\n",
    "        plt.plot(history_transformer['val_custom_acc'])\n",
    "        plt.title('Model Custom acc')\n",
    "        plt.ylabel('Custom acc')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "        plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, o in val_dataset.take(1):\n",
    "    print(i[0])\n",
    "    print(' '.join([tokenizer.index_word[x.numpy()] for x in i[0] if x.numpy() != 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sent = \"[BOS] no inverno , as familias necessitadas\"\n",
    "\n",
    "#tokenizer.texts_to_sequences(\"[BOS] o rato roeu a roupa do rei de roma \")\n",
    "#inp_seq = np.array([tokenizer.word_index[x] for x in input_sent.split()])\n",
    "\n",
    "inp_seq = np.array([ w[0] for w in  tokenizer.texts_to_sequences(input_sent.split(\" \")) if len(w)>0])\n",
    "temp = 0.9\n",
    "candidates_n = 50\n",
    "max_len = 30\n",
    "\n",
    "inp_seq#, tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE_TEXT:\n",
    "    result1 = generate_sentence(model, input_tokens = tf.expand_dims(inp_seq,0), max_length= max_len, temperature= temp , number_of_candidates=candidates_n, no_state = False)\n",
    "    result2 = generate_sentence(model, input_tokens = tf.expand_dims(inp_seq,0), max_length= max_len, temperature= temp , number_of_candidates=candidates_n , loop= True)\n",
    "\n",
    "    print(\"RESULT WITH STATE : \",result1[\"generated_text\"],\"\\n\")\n",
    "    print(\"RESULT WITH STATE AND LOOP : \",result2[\"generated_text\"],\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention with causal mask on upper triangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE_TEXT:\n",
    "    result = generate_sentence(model_causal1, input_tokens = tf.expand_dims(inp_seq,0), max_length= max_len, temperature= temp , number_of_candidates=candidates_n, no_state = True)\n",
    "    result2 = generate_sentence(model_causal1, input_tokens = tf.expand_dims(inp_seq,0), max_length= max_len, temperature= temp , number_of_candidates=candidates_n , loop= True)\n",
    "\n",
    "    plot_weights(result, stop_time = 3, loop = False)\n",
    "    print(\"RESULT NO STATE : \",result[\"generated_text\"],\"\\n\")\n",
    "    print(\"RESULT WITH STATE AND LOOP : \",result2[\"generated_text\"],\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer with causal mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE_TEXT:\n",
    "    result = generate_sentence_for_transformer(transformer, input_tokens = tf.expand_dims(inp_seq,0), max_length= max_len, temperature= temp , number_of_candidates=candidates_n, )\n",
    "    result1 = generate_sentence_for_transformer(transformer, input_tokens = tf.expand_dims(inp_seq,0), max_length= max_len, temperature= temp , number_of_candidates=candidates_n , loop= True)\n",
    "    print(\"RESULT  : \",result[\"generated_text\"],\"\\n\")\n",
    "    print(\"RESULT WITH  LOOP : \",result1[\"generated_text\"],\"\\n\")\n",
    "\n",
    "    t = -1\n",
    "    plt.figure()\n",
    "    plot_attention(result['attention'][t][\"decoder_layer4_block1\"][0][-1],  result[\"output_text\"][t],  result[\"output_text\"][t])#input_text\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "TextGenerator-In progress.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "dffee67a034c262da22ecd866efe5fa1174a6876e1e96ed512a9acb5833498b8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('tf_cpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
